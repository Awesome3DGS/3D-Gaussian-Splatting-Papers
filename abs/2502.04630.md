### High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting

Capturing and reconstructing high-speed dynamic 3D scenes has numerous applications in computer graphics, vision, and interdisciplinary fields such as robotics, aerodynamics, and evolutionary biology. However, achieving this using a single imaging modality remains challenging. For instance, traditional RGB cameras suffer from low frame rates, limited exposure times, and narrow baselines. To address this, we propose a novel sensor fusion approach using Gaussian splatting, which combines RGB, depth, and event cameras to capture and reconstruct deforming scenes at high speeds. The key insight of our method lies in leveraging the complementary strengths of these imaging modalities: RGB cameras capture detailed color information, event cameras record rapid scene changes with microsecond resolution, and depth cameras provide 3D scene geometry. To unify the underlying scene representation across these modalities, we represent the scene using deformable 3D Gaussians. To handle rapid scene movements, we jointly optimize the 3D Gaussian parameters and their temporal deformation fields by integrating data from all three sensor modalities. This fusion enables efficient, high-quality imaging of fast and complex scenes, even under challenging conditions such as low light, narrow baselines, or rapid motion. Experiments on synthetic and real datasets captured with our prototype sensor fusion setup demonstrate that our method significantly outperforms state-of-the-art techniques, achieving noticeable improvements in both rendering fidelity and structural accuracy.

捕捉和重建高速动态3D场景在计算机图形学、视觉以及机器人学、空气动力学和进化生物学等跨学科领域中具有广泛的应用。然而，使用单一成像方式实现这一目标仍然具有挑战性。例如，传统RGB相机受限于低帧率、有限的曝光时间和狭窄的基线。为了解决这个问题，我们提出了一种新颖的传感器融合方法，利用高斯溅射技术，将RGB、深度和事件相机结合起来，用于捕捉和重建高速变形场景。我们方法的关键在于利用这些成像方式的互补优势：RGB相机捕捉详细的颜色信息，事件相机以微秒级分辨率记录快速场景变化，深度相机提供3D场景几何信息。为了统一这些模式下的场景表示，我们使用可变形的3D高斯表示场景。为了处理快速场景运动，我们通过整合来自三种传感器模式的数据，联合优化3D高斯参数及其时间变形场。这种融合方法使得在低光、狭窄基线或快速运动等挑战性条件下，仍能高效且高质量地成像快速复杂的场景。我们在使用原型传感器融合系统捕获的合成和真实数据集上的实验结果表明，我们的方法显著超越了现有的最先进技术，在渲染保真度和结构准确性上都取得了显著提升。
