### Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting

We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively.

我们提出了 Observer Actor（ObAct），一个用于主动视觉模仿学习的新颖框架，其中观察者会主动移动到最佳视角为执行者提供观察信息。我们在配备腕部摄像头的双臂机器人系统上研究了 ObAct。在测试阶段，ObAct 会动态分配观察者与执行者的角色：观察者手臂通过三张图像构建三维高斯溅射（3DGS）表示，随后在虚拟空间中探索最优相机位姿并移动至该位置；执行者手臂则基于观察者提供的视角执行策略。该机制提高了策略观察中物体与机械夹爪的清晰度与可见性。因此，我们能够在更接近于无遮挡训练分布的观察下训练双臂通用策略，从而提升策略的鲁棒性。我们在两个现有模仿学习方法上验证了该框架——轨迹迁移与行为克隆，实验表明 ObAct 相较静态相机方案取得了显著提升：在无遮挡场景下，轨迹迁移提升 145%，有遮挡时提升 233%；行为克隆分别提升 75% 和 143%。
