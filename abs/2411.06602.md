### Adaptive and Temporally Consistent Gaussian Surfels for Multi-view Dynamic Reconstruction

3D Gaussian Splatting has recently achieved notable success in novel view synthesis for dynamic scenes and geometry reconstruction in static scenes. Building on these advancements, early methods have been developed for dynamic surface reconstruction by globally optimizing entire sequences. However, reconstructing dynamic scenes with significant topology changes, emerging or disappearing objects, and rapid movements remains a substantial challenge, particularly for long sequences. To address these issues, we propose AT-GS, a novel method for reconstructing high-quality dynamic surfaces from multi-view videos through per-frame incremental optimization. To avoid local minima across frames, we introduce a unified and adaptive gradient-aware densification strategy that integrates the strengths of conventional cloning and splitting techniques. Additionally, we reduce temporal jittering in dynamic surfaces by ensuring consistency in curvature maps across consecutive frames. Our method achieves superior accuracy and temporal coherence in dynamic surface reconstruction, delivering high-fidelity space-time novel view synthesis, even in complex and challenging scenes. Extensive experiments on diverse multi-view video datasets demonstrate the effectiveness of our approach, showing clear advantages over baseline methods.

3D Gaussian Splatting (3DGS) 近年来在动态场景的新视图合成和静态场景的几何重建方面取得了显著成功。在此基础上，早期方法通过对整个序列进行全局优化，实现了动态表面的重建。然而，对于具有显著拓扑变化、新物体出现或消失以及快速运动的动态场景的重建，特别是在长序列中，仍然存在巨大挑战。
为了解决这些问题，我们提出了 AT-GS，一种通过逐帧增量优化从多视图视频中重建高质量动态表面的新方法。为避免帧间的局部极小值，我们引入了一种统一且自适应的梯度感知致密化策略，结合了传统克隆与分裂技术的优势。此外，通过确保连续帧间曲率图的一致性，我们有效减少了动态表面中的时间抖动。
我们的方法在动态表面重建中实现了更高的准确性和时间一致性，即使在复杂且具有挑战性的场景中，也能提供高保真的时空新视图合成。在广泛的多视图视频数据集上进行的实验表明，我们的方法相较基线方法展现了显著优势，有效验证了其性能和适用性。
