### UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting

Despite significant advancements in dynamic neural rendering, existing methods fail to address the unique challenges posed by UAV-captured scenarios, particularly those involving monocular camera setups, top-down perspective, and multiple small, moving humans, which are not adequately represented in existing datasets. In this work, we introduce UAV4D, a framework for enabling photorealistic rendering for dynamic real-world scenes captured by UAVs. Specifically, we address the challenge of reconstructing dynamic scenes with multiple moving pedestrians from monocular video data without the need for additional sensors. We use a combination of a 3D foundation model and a human mesh reconstruction model to reconstruct both the scene background and humans. We propose a novel approach to resolve the scene scale ambiguity and place both humans and the scene in world coordinates by identifying human-scene contact points. Additionally, we exploit the SMPL model and background mesh to initialize Gaussian splats, enabling holistic scene rendering. We evaluated our method on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and Okutama-Action, each with distinct characteristics and 10~50 humans. Our results demonstrate the benefits of our approach over existing methods in novel view synthesis, achieving a 1.5 dB PSNR improvement and superior visual sharpness.

尽管动态神经渲染取得了显著进展，现有方法仍未能有效应对无人机（UAV）拍摄场景所面临的独特挑战，尤其是在单目摄像头设置、俯视视角以及包含多个小型移动人物的场景中，这类情况在现有数据集中尚未得到充分覆盖。为此，本文提出了 UAV4D 框架，用于实现对无人机拍摄的真实动态场景的照片级真实感渲染。
具体而言，我们致力于从单目视频数据中重建包含多名行人动态活动的场景，无需额外传感器辅助。我们结合使用三维基础模型与人体网格重建模型，分别对场景背景和人物进行重建。为了解决场景尺度歧义问题，我们提出了一种新方法，通过识别人物与场景的接触点，将人物与场景共同定位于世界坐标系中。
此外，我们利用 SMPL 模型与背景网格初始化高斯泼洒，实现对整体现实场景的渲染。我们在三个具有代表性的复杂无人机拍摄数据集——VisDrone、Manipal-UAV 和 Okutama-Action 上对方法进行了评估，这些数据集中均包含 10 至 50 名不等的人物，且各具特性。
实验结果表明，与现有方法相比，我们的方案在新视角合成任务中表现出更强优势，PSNR 提升达 1.5 dB，同时在视觉清晰度上也展现出更优质的效果。
