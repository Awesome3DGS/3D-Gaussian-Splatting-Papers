### Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians

3D generation has made significant progress, however, it still largely remains at the object-level. Feedforward 3D scene-level generation has been rarely explored due to the lack of models capable of scaling-up latent representation learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data in a bounded canonical space, scene-level generations with 3D scenes represented by 3D Gaussian Splatting (3DGS) are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level variational autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent embedding, which effectively captures both semantic and spatial information of the inputs. Beyond model design, we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our method on the recent scene-level 3D dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as our applications to demonstrate its ability to facilitate downstream generation tasks.

三维生成技术已取得显著进展，但仍主要停留在物体级别。由于缺乏能够在三维场景级数据上扩展潜在表示学习的模型，前向式的三维场景级生成鲜有探索。与在有界标准空间中利用标注完善的三维数据训练的物体级生成模型不同，基于三维高斯泼溅（3DGS）表示的场景级生成是无界的，并且在不同场景间存在尺度不一致问题，这使得面向生成任务的统一潜在表示学习极具挑战性。本文提出了 Can3Tok，这是首个能够将大量高斯基元编码为低维潜在嵌入的三维场景级变分自编码器（VAE），能够有效捕获输入的语义和空间信息。除了模型设计，我们还提出了一套通用的三维场景数据处理流程，以解决尺度不一致的问题。我们在最新的场景级三维数据集 DL3DV-10K 上验证了该方法，结果发现，只有 Can3Tok 能够成功泛化到新的三维场景，而对比方法在训练中即使面对几百个场景输入也无法收敛，并且在推理时表现出零泛化能力。最后，我们展示了图像到 3DGS 和文本到 3DGS 的生成应用，以证明其在下游生成任务中的促进作用。
