### Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images

3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis performance. While conventional methods require per-scene optimization, more recently several feed-forward methods have been proposed to generate pixel-aligned Gaussian representations with a learnable network, which are generalizable to different scenes. However, these methods simply combine pixel-aligned Gaussians from multiple views as scene representations, thereby leading to artifacts and extra memory cost without fully capturing the relations of Gaussians from different images. In this paper, we propose Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian representations. Specifically, we construct Gaussian Graphs to model the relations of Gaussian groups from different views. To support message passing at Gaussian level, we reformulate the basic graph operations over Gaussian representations, enabling each Gaussian to benefit from its connected Gaussian groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling layer to aggregate various Gaussian groups for efficient representations. We conduct experiments on the large-scale RealEstate10K and ACID datasets to demonstrate the efficiency and generalization of our method. Compared to the state-of-the-art methods, our model uses fewer Gaussians and achieves better image quality with higher rendering speed.

3D 高斯散点 (3DGS) 在新视角合成任务中展现出了卓越的性能。传统方法通常需要针对每个场景进行优化，而最近出现了一些前馈 (feed-forward) 方法，通过可学习的网络直接生成像素对齐的高斯表示，从而具备跨场景的泛化能力。然而，这些方法仅通过简单地将多个视角的像素对齐高斯点组合成场景表示，未能充分捕捉不同图像间的高斯点关系，导致伪影 (artifacts) 增多，同时带来额外的内存开销。
为了解决这一问题，我们提出了高斯图网络 (Gaussian Graph Network, GGN)，用于生成高效且具备泛化能力的高斯表示。具体而言，我们构建高斯图 (Gaussian Graphs)，用于建模来自不同视角的高斯点组之间的关系。为了支持高斯级别的信息传递 (message passing)，我们重新定义了基本的图操作，使得高斯点能够通过高斯特征融合 (Gaussian feature fusion) 从其关联的高斯组中受益。此外，我们设计了一种高斯池化层 (Gaussian pooling layer)，用于聚合多个高斯点组，从而生成更加紧凑、高效的表示。
我们在大规模 RealEstate10K 和 ACID 数据集上进行了实验，验证了我们方法的高效性和泛化能力。与现有最先进的方法相比，我们的模型在使用更少高斯点的情况下，实现了更高质量的图像渲染，同时显著提升渲染速度。
