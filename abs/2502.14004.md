### Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction

Recent advancements in implicit 3D reconstruction methods, e.g., neural rendering fields and Gaussian splatting, have primarily focused on novel view synthesis of static or dynamic objects with continuous motion states. However, these approaches struggle to efficiently model a human-interactive object with n movable parts, requiring 2^n separate models to represent all discrete states. To overcome this limitation, we propose Inter3D, a new benchmark and approach for novel state synthesis of human-interactive objects. We introduce a self-collected dataset featuring commonly encountered interactive objects and a new evaluation pipeline, where only individual part states are observed during training, while part combination states remain unseen. We also propose a strong baseline approach that leverages Space Discrepancy Tensors to efficiently modelling all states of an object. To alleviate the impractical constraints on camera trajectories across training states, we propose a Mutual State Regularization mechanism to enhance the spatial density consistency of movable parts. In addition, we explore two occupancy grid sampling strategies to facilitate training efficiency. We conduct extensive experiments on the proposed benchmark, showcasing the challenges of the task and the superiority of our approach.

近年来，隐式三维重建（Implicit 3D Reconstruction） 方法（如 神经渲染场（Neural Rendering Fields） 和 高斯溅射（Gaussian Splatting））的进展主要集中在 静态或动态对象的 新视角合成，其中对象的运动状态是 连续的。然而，这些方法在建模 具有 n 个可移动部件的人机交互对象 时存在效率问题，需要 2^n 个独立模型 才能表示所有离散状态，从而导致计算和存储成本急剧上升。
为了解决这一局限性，我们提出 Inter3D，一个针对 人机交互对象的新状态合成（Novel State Synthesis） 的 基准（Benchmark） 和 新方法。我们构建了一个 自采集数据集，涵盖常见的交互式对象，并引入了 新的评估流程，其中 训练阶段仅观测单个部件的状态，而部件组合状态在训练中是不可见的。此外，我们提出了一种 强基线方法，利用 空间差异张量（Space Discrepancy Tensors） 高效建模对象的所有状态。
为缓解 训练过程中不同状态下相机轨迹的一致性约束，我们设计了一种 互相状态正则化（Mutual State Regularization） 机制，以增强可移动部件的空间密度一致性。此外，我们探索了 两种占据网格（Occupancy Grid）采样策略 以提升训练效率。我们在 Inter3D 基准 上进行了广泛实验，展示了该任务的挑战性以及我们方法的优越性。
