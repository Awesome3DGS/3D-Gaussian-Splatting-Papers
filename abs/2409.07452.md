### Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models

Despite having tremendous progress in image-to-3D generation, existing methods still struggle to produce multi-view consistent images with high-resolution textures in detail, especially in the paradigm of 2D diffusion that lacks 3D awareness. In this work, we present High-resolution Image-to-3D model (Hi3D), a new video diffusion based paradigm that redefines a single image to multi-view images as 3D-aware sequential image generation (i.e., orbital video generation). This methodology delves into the underlying temporal consistency knowledge in video diffusion model that generalizes well to geometry consistency across multiple views in 3D generation. Technically, Hi3D first empowers the pre-trained video diffusion model with 3D-aware prior (camera pose condition), yielding multi-view images with low-resolution texture details. A 3D-aware video-to-video refiner is learnt to further scale up the multi-view images with high-resolution texture details. Such high-resolution multi-view images are further augmented with novel views through 3D Gaussian Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D reconstruction. Extensive experiments on both novel view synthesis and single view reconstruction demonstrate that our Hi3D manages to produce superior multi-view consistency images with highly-detailed textures.

尽管图像到3D生成领域取得了巨大进展，现有方法在生成具有高分辨率细节纹理的多视角一致图像时仍然面临挑战，尤其是在缺乏3D感知的2D扩散范式下。为了解决这一问题，我们提出了高分辨率图像到3D模型（Hi3D），这是一种基于视频扩散的新范式，将单张图像生成多视角图像重新定义为3D感知的序列图像生成（即轨道视频生成）。这种方法深入探讨了视频扩散模型中的时间一致性知识，这种知识可以很好地推广到3D生成中的几何一致性。
在技术上，Hi3D 首先为预训练的视频扩散模型赋予了3D感知先验（相机位姿条件），从而生成具有低分辨率纹理细节的多视角图像。接着，学习一个3D感知的视频到视频的精化器，用于进一步提升多视角图像的分辨率和纹理细节。这些高分辨率的多视角图像通过3D高斯散点进行视图扩展，最终用于通过3D重建获取高保真的网格。我们在新视图合成和单视图重建上的广泛实验表明，Hi3D能够生成具有高度细节纹理的多视角一致图像。

