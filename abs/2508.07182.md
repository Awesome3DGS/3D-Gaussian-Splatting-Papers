### 3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction

This paper addresses the challenge of novel-view synthesis and motion reconstruction of dynamic scenes from monocular video, which is critical for many robotic applications. Although Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering static scenes, extending them to reconstruct dynamic scenes remains challenging. In this work, we introduce a novel approach that combines 3DGS with a motion trajectory field, enabling precise handling of complex object motions and achieving physically plausible motion trajectories. By decoupling dynamic objects from static background, our method compactly optimizes the motion trajectory field. The approach incorporates time-invariant motion coefficients and shared motion trajectory bases to capture intricate motion patterns while minimizing optimization complexity. Extensive experiments demonstrate that our approach achieves state-of-the-art results in both novel-view synthesis and motion trajectory recovery from monocular video, advancing the capabilities of dynamic scene reconstruction.

本文针对单目视频中动态场景的新视角合成与运动重建问题展开研究，这在众多机器人应用中至关重要。尽管神经辐射场（NeRF）和三维高斯溅射（3DGS）在静态场景渲染方面已取得显著成功，但将其扩展到动态场景的重建仍然具有挑战性。在本研究中，我们提出了一种新方法，将3DGS与运动轨迹场相结合，从而能够精确处理复杂的物体运动，并实现物理合理的运动轨迹。通过将动态物体与静态背景解耦，我们的方法能够紧凑地优化运动轨迹场。该方法引入了时间不变的运动系数和共享的运动轨迹基，以捕捉复杂的运动模式，同时降低优化复杂度。大量实验结果表明，该方法在单目视频的新视角合成与运动轨迹恢复方面均达到了最新的先进水平，推动了动态场景重建能力的发展。
