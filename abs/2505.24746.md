### Tackling View-Dependent Semantics in 3D Language Gaussian Splatting

Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D scene reconstruction from RGB images. Many studies extend this paradigm for language-driven open-vocabulary scene understanding. However, most of them simply project 2D semantic features onto 3D Gaussians and overlook a fundamental gap between 2D and 3D understanding: a 3D object may exhibit various semantics from different viewpoints--a phenomenon we term view-dependent semantics. To address this challenge, we propose LaGa (Language Gaussians), which establishes cross-view semantic connections by decomposing the 3D scene into objects. Then, it constructs view-aggregated semantic representations by clustering semantic descriptors and reweighting them based on multi-view semantics. Extensive experiments demonstrate that LaGa effectively captures key information from view-dependent semantics, enabling a more comprehensive understanding of 3D scenes. Notably, under the same settings, LaGa achieves a significant improvement of +18.7% mIoU over the previous SOTA on the LERF-OVS dataset.

近年来，3D Gaussian Splatting（3D-GS） 在从 RGB 图像进行高质量三维场景重建方面取得了显著进展。许多研究进一步将该范式扩展至语言驱动的开放词汇三维场景理解。然而，大多数方法仅仅是将二维语义特征投影到三维高斯上，忽略了二维与三维语义理解之间的一个基本鸿沟：一个三维物体在不同视角下可能呈现出不同的语义——我们称之为视角相关语义（view-dependent semantics）。
为应对这一挑战，我们提出了 LaGa（Language Gaussians）。该方法通过将三维场景分解为不同对象，建立跨视角的语义关联；随后，它对语义描述符进行聚类，并基于多视角语义对其进行重加权，从而构建出视角聚合语义表示（view-aggregated semantic representations）。
大量实验表明，LaGa 能够有效捕捉视角相关语义中的关键信息，实现对三维场景更全面的理解。值得注意的是，在相同实验设置下，LaGa 在 LERF-OVS 数据集上将此前 SOTA 的 mIoU 提升了 18.7 个百分点，展现出显著优势。
