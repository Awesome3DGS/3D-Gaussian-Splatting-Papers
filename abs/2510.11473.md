### VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment

3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis.

三维高斯泼溅（3D Gaussian Splatting）近年来作为一种高效的高质量实时新视角合成方案逐渐受到关注。然而，其在精确表面重建方面的能力尚未被充分挖掘。由于高斯基元的离散性与非结构化特性，单纯依赖图像渲染损失进行监督往往会导致几何结构不准确以及多视角对齐不一致。为此，本文提出了一种通过视角对齐（View Alignment, VA）提升三维高斯几何表达能力的新方法。具体而言，我们在渲染损失中引入边缘感知图像线索，以提升表面边界的清晰度；同时，为实现跨视角的几何一致性，我们提出了一种感知可见性的光度对齐损失，该损失建模遮挡关系，鼓励高斯基元间准确的空间结构。此外，为减弱光照变化带来的歧义，我们引入基于法向的约束以优化高斯的空间朝向，从而提升局部表面估计的精度。我们还利用深度图像特征嵌入进行跨视角一致性约束，增强所学几何在不同视角和光照条件下的鲁棒性。我们在多个标准基准数据集上进行大量实验，结果表明本方法在表面重建与新视角合成任务中均达到了当前最优水平。
