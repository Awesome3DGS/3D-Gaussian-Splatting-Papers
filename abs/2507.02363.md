### LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling

Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes.

由于现实世界中存在复杂且高度动态的运动，从多视图输入合成任意视角的动态视频是一项具有挑战性的任务。基于神经辐射场或三维高斯投影的现有方法在建模细粒度运动方面存在局限性，从而极大地限制了其应用。本文提出了 LocalDyGS，该方法由两个部分组成，以同时适应大规模和细粒度运动场景：1）我们将复杂的动态场景分解为由种子定义的精简局部空间，通过捕捉每个局部空间内的运动，实现全局建模；2）我们将局部空间运动建模中的静态特征与动态特征解耦，时间步共享的静态特征用于捕捉静态信息，而动态残差场则提供特定时间的特征。二者结合并解码生成时序高斯（Temporal Gaussians），用于建模各局部空间内的运动。因此，我们提出了一种新颖的动态场景重建框架，以更真实地建模高度动态的现实场景。我们的方法不仅在多个细粒度数据集上与当前最先进（SOTA）方法相比表现出竞争力，还首次尝试对更大规模、更复杂的高度动态场景进行建模。
