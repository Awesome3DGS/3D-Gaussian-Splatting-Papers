### MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation

Object compositing offers significant promise for augmented reality (AR) and embodied intelligence applications. Existing approaches predominantly focus on single-image scenarios or intrinsic decomposition techniques, facing challenges with multi-view consistency, complex scenes, and diverse lighting conditions. Recent inverse rendering advancements, such as 3D Gaussian and diffusion-based methods, have enhanced consistency but are limited by scalability, heavy data requirements, or prolonged reconstruction time per scene. To broaden its applicability, we introduce MV-CoLight, a two-stage framework for illumination-consistent object compositing in both 2D images and 3D scenes. Our novel feed-forward architecture models lighting and shadows directly, avoiding the iterative biases of diffusion-based methods. We employ a Hilbert curve-based mapping to align 2D image inputs with 3D Gaussian scene representations seamlessly. To facilitate training and evaluation, we further introduce a large-scale 3D compositing dataset. Experiments demonstrate state-of-the-art harmonized results across standard benchmarks and our dataset, as well as casually captured real-world scenes demonstrate the framework's robustness and wide generalization.

对象合成在增强现实（AR）和具身智能应用中具有广阔前景。现有方法主要聚焦于单张图像场景或内在分解技术，难以应对多视角一致性、复杂场景以及多样化光照条件等挑战。近年来，逆向渲染技术的发展（如三维高斯和基于扩散的方法）在一致性方面取得了进展，但仍受限于可扩展性差、数据需求大或每个场景重建时间长等问题。为拓宽其实用性，我们提出了 MV-CoLight，一种适用于二维图像和三维场景中实现光照一致性对象合成的两阶段框架。我们提出的新型前馈架构能够直接建模光照与阴影，避免了扩散方法中存在的迭代偏差。我们引入基于希尔伯特曲线的映射策略，实现了二维图像输入与三维高斯场景表示之间的无缝对齐。为支持训练与评估，我们还构建了一个大规模三维合成数据集。实验结果表明，我们的方法在标准基准和自建数据集上均实现了最先进的光照一致性效果，且在随手采集的真实场景中也展现出较强的鲁棒性与广泛的泛化能力。
