### A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image

The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.

从单张图像进行法向估计时，缺乏空间维度信息仍是一个挑战。尽管近年来的扩散模型在二维到三维的隐式映射中展现了显著潜力，但它们依赖于数据驱动的统计先验，缺乏对光与表面交互的显式建模，从而导致多视角法向方向冲突。此外，扩散模型的离散采样机制会在可微分渲染重建模块中引起梯度不连续，阻碍三维几何误差向法向生成网络的反向传播，从而迫使现有方法依赖密集的法向标注数据。本文提出了 SINGAD，一种基于单张图像的法向估计自监督框架（Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion）。该框架结合了物理驱动的光交互建模与基于可微渲染的重投影策略，能够将三维几何误差直接转化为法向优化信号，从而解决多视角几何不一致和数据依赖问题。具体而言，框架构建了一个光交互驱动的 3DGS 重参数化模型，用于生成符合光传输原理的多尺度几何特征，从而确保多视角法向一致性；在条件扩散模型中设计了跨域特征融合模块，将几何先验嵌入以约束法向生成，同时保持几何误差的准确传播。此外，引入了一种可微的三维重投影损失策略，用于自监督优化，通过最小化重建图像与输入图像的几何误差，消除了对标注法向数据集的依赖。在 Google Scanned Objects 数据集上的定量评估表明，我们的方法在多项指标上均优于当前最先进的方法。
