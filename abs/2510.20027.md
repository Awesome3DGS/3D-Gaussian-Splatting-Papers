### Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses

When viewing a 3D Gaussian Splatting (3DGS) model from camera positions significantly outside the training data distribution, substantial visual noise commonly occurs. These artifacts result from the lack of training data in these extrapolated regions, leading to uncertain density, color, and geometry predictions from the model.
To address this issue, we propose a novel real-time render-aware filtering method. Our approach leverages sensitivity scores derived from intermediate gradients, explicitly targeting instabilities caused by anisotropic orientations rather than isotropic variance. This filtering method directly addresses the core issue of generative uncertainty, allowing 3D reconstruction systems to maintain high visual fidelity even when users freely navigate outside the original training viewpoints.
Experimental evaluation demonstrates that our method substantially improves visual quality, realism, and consistency compared to existing Neural Radiance Field (NeRF)-based approaches such as BayesRays. Critically, our filter seamlessly integrates into existing 3DGS rendering pipelines in real-time, unlike methods that require extensive post-hoc retraining or fine-tuning.

当从远离训练数据分布的摄像机位置观察三维高斯投影（3D Gaussian Splatting，简称3DGS）模型时，常常会出现明显的视觉噪声。这些伪影源于外推区域缺乏训练数据，导致模型在密度、颜色和几何预测方面产生不确定性。
为解决这一问题，本文提出一种新颖的实时渲染感知滤波方法。我们的方法利用中间梯度中提取的敏感度分数，重点抑制由各向异性方向引起的不稳定性，而非仅考虑各向同性方差。该滤波策略直接应对生成式不确定性的核心问题，使得三维重建系统即便在用户自由游览至训练视角之外时，也能保持较高的视觉保真度。
实验评估表明，与BayesRays等现有基于NeRF的方法相比，我们的方法在视觉质量、真实感和一致性方面均有显著提升。更为关键的是，该滤波器可无缝实时集成至现有3DGS渲染流程中，而无需复杂的后期再训练或微调步骤。
