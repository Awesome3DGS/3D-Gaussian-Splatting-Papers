### Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance

Pre-trained conditional diffusion models have demonstrated remarkable potential in image editing. However, they often face challenges with temporal consistency, particularly in the talking head domain, where continuous changes in facial expressions intensify the level of difficulty. These issues stem from the independent editing of individual images and the inherent loss of temporal continuity during the editing process. In this paper, we introduce Follow Your Motion (FYM), a generic framework for maintaining temporal consistency in portrait editing. Specifically, given portrait images rendered by a pre-trained 3D Gaussian Splatting model, we first develop a diffusion model that intuitively and inherently learns motion trajectory changes at different scales and pixel coordinates, from the first frame to each subsequent frame. This approach ensures that temporally inconsistent edited avatars inherit the motion information from the rendered avatars. Secondly, to maintain fine-grained expression temporal consistency in talking head editing, we propose a dynamic re-weighted attention mechanism. This mechanism assigns higher weight coefficients to landmark points in space and dynamically updates these weights based on landmark loss, achieving more consistent and refined facial expressions. Extensive experiments demonstrate that our method outperforms existing approaches in terms of temporal consistency and can be used to optimize and compensate for temporally inconsistent outputs in a range of applications, such as text-driven editing, relighting, and various other applications.

经过预训练的条件扩散模型在图像编辑任务中展现了卓越的潜力。然而，在说话人头像（talking head）领域，由于面部表情持续变化带来的高动态性，这些模型在保持时间一致性方面仍面临挑战。这一问题主要源于独立编辑单帧图像过程中导致的时间连续性丧失。
为了解决这一问题，本文提出了Follow Your Motion (FYM)，一个用于保持肖像编辑中时间一致性的通用框架。具体而言，针对由预训练的三维高斯泼洒（3D Gaussian Splatting）模型渲染得到的肖像图像，我们首先开发了一种扩散模型，能够在不同尺度和像素坐标上，直观且内在地学习从首帧到后续各帧的运动轨迹变化。该方法确保了即使编辑后的头像存在时间不一致，也能继承渲染头像中的运动信息。
其次，为了在说话人头像编辑中保持细粒度的表情时间一致性，我们提出了动态重加权注意力机制（dynamic re-weighted attention mechanism）。该机制在空间上对关键点（如人脸关键点）赋予更高的权重系数，并基于关键点损失动态更新这些权重，从而实现更加一致且细腻的面部表情变化。
大量实验表明，FYM在时间一致性方面显著优于现有方法，且能够用于优化和补偿一系列应用中出现的时间不一致输出，如文本驱动编辑、重光照（relighting）以及其他多种应用场景。
