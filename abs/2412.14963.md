### IDOL: Instant Photorealistic 3D Human Creation from a Single Image

Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K diverse, photorealistic sets of human images. Each set contains 24-view frames in specific human poses, generated using a pose-controllable image-to-multi-view model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space from a given human image. This model is trained to disentangle human pose, body shape, clothing geometry, and texture. The estimated Gaussians can be animated without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the ability to efficiently reconstruct photorealistic humans at 1K resolution from a single input image using a single GPU instantly. Additionally, it seamlessly supports various applications, as well as shape and texture editing tasks.

创建一个高保真、可动画的3D全身头像，仅从单张图像生成，是一个具有挑战性的任务，因为人类的外观和姿态多样，以及高质量训练数据的有限性。为了实现快速且高质量的人体重建，本研究从数据集、模型和表示方法的角度重新思考了这一任务。
首先，我们引入了一个大规模以人为中心的生成数据集 HuGe100K，该数据集由100K组多样化、逼真的人像图像组成。每组包含24视角帧，显示特定的人体姿态，利用一个可控制姿态的图像到多视图模型生成。
接着，利用 HuGe100K 数据集中丰富的视角、姿态和外观多样性，我们开发了一个可扩展的前馈Transformer模型，该模型能够从单个人像图像中预测一个统一空间内的3D人体高斯表示。模型经过训练，可以解耦人体的姿态、体型、服装几何形状和纹理。预测的高斯表示可直接用于动画生成，无需后处理。
我们进行了全面的实验，验证了所提出数据集和方法的有效性。实验表明，我们的模型能够高效地从单张输入图像重建分辨率达1K的逼真人体，并可在单张GPU上即时完成。此外，该方法还无缝支持多种应用，包括形状和纹理编辑任务。
