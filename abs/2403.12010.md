### VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model

Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects.

基于文本或单图提示生成多视图图像对于3D内容的创建来说是一个关键能力。围绕这一主题的两个基本问题是我们用于训练的数据是什么，以及如何确保多视图的一致性。本文引入了一个新颖的框架，对这两个问题都做出了基本的贡献。不同于利用2D扩散模型的图像进行训练，我们提出了一个从现成视频生成模型中微调的密集一致多视图生成模型。视频生成模型的图像更适合于多视图生成，因为生成它们的底层网络架构采用了时间模块来强制帧一致性。此外，用于训练这些模型的视频数据集丰富且多样，导致训练-微调领域的差距减小。为了增强多视图一致性，我们引入了一个3D感知去噪采样，首先利用前馈重建模块得到一个显式的全球3D模型，然后采用一种采样策略，有效地将从全球3D模型渲染的图像纳入去噪采样循环中，以提高最终图像的多视图一致性。作为一个附带产物，这个模块还提供了一种在几秒钟内快速创建由3D高斯表示的3D资产的方法。我们的方法可以生成24个密集视图，并且在训练中的收敛速度比现有的最先进方法（4个GPU小时对比数千个GPU小时）要快得多，同时在视觉质量和一致性上也相当。通过进一步的微调，我们的方法在定量指标和视觉效果上都超过了现有的最先进方法。
