### GSTalker: Real-time Audio-Driven Talking Face Generation via Deformable Gaussian Splatting

We present GStalker, a 3D audio-driven talking face generation model with Gaussian Splatting for both fast training (40 minutes) and real-time rendering (125 FPS) with a 3∼5 minute video for training material, in comparison with previous 2D and 3D NeRF-based modeling frameworks which require hours of training and seconds of rendering per frame. Specifically, GSTalker learns an audio-driven Gaussian deformation field to translate and transform 3D Gaussians to synchronize with audio information, in which multi-resolution hashing grid-based tri-plane and temporal smooth module are incorporated to learn accurate deformation for fine-grained facial details. In addition, a pose-conditioned deformation field is designed to model the stabilized torso. To enable efficient optimization of the condition Gaussian deformation field, we initialize 3D Gaussians by learning a coarse static Gaussian representation. Extensive experiments in person-specific videos with audio tracks validate that GSTalker can generate high-fidelity and audio-lips synchronized results with fast training and real-time rendering speed.

我们介绍了GStalker，这是一个3D音频驱动的对话面生成模型，采用高斯喷溅技术，实现快速训练（40分钟）和实时渲染（每秒125帧），只需3至5分钟的视频作为训练材料。相比之下，先前的基于2D和3D NeRF的建模框架需要几小时的训练和每帧几秒钟的渲染时间。具体来说，GSTalker学习一个由音频驱动的高斯变形场，以平移和变换3D高斯，使其与音频信息同步，其中包括多分辨率哈希网格基础的三平面和时间平滑模块，用于学习精确的变形以呈现细致的面部细节。此外，设计了一个姿势条件的变形场来模拟稳定的躯干。为了有效优化条件高斯变形场，我们通过学习粗略的静态高斯表示来初始化3D高斯。在包含音频轨道的特定人物视频中进行的广泛实验验证了GSTalker能够生成高保真度且音频与嘴唇同步的结果，具有快速训练和实时渲染的速度。
