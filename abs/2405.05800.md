### DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian Representation

User-friendly 3D object editing is a challenging task that has attracted significant attention recently. The limitations of direct 3D object editing without 2D prior knowledge have prompted increased attention towards utilizing 2D generative models for 3D editing. While existing methods like Instruct NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly due to semantic guided editing. In the realm of 3D representation, 3D Gaussian Splatting emerges as a promising approach for its efficiency and natural explicit property, facilitating precise editing tasks. Building upon these insights, we propose DragGaussian, a 3D object drag-editing framework based on 3D Gaussian Splatting, leveraging diffusion models for interactive image editing with open-vocabulary input. This framework enables users to perform drag-based editing on pre-trained 3D Gaussian object models, producing modified 2D images through multi-view consistent editing. Our contributions include the introduction of a new task, the development of DragGaussian for interactive point-based 3D editing, and comprehensive validation of its effectiveness through qualitative and quantitative experiments.

近期，用户友好的三维对象编辑成为一个具有挑战性的任务，引起了广泛关注。直接三维对象编辑在没有二维先验知识的情况下的局限性，促使人们更多地使用二维生成模型进行三维编辑。尽管现有方法如 Instruct NeRF-to-NeRF 提供了解决方案，但它们通常缺乏用户友好性，特别是在语义引导编辑方面。在三维表示领域中，三维高斯喷溅作为一种有效且自然明确的方法，成为了一个有前景的选择，便于进行精确编辑任务。基于这些洞见，我们提出了 DragGaussian，这是一个基于三维高斯喷溅的三维对象拖拽编辑框架，利用扩散模型进行交互式图像编辑，并支持开放词汇输入。该框架使用户能够对预训练的三维高斯对象模型进行基于拖拽的编辑，通过多视角一致性编辑产生修改后的二维图像。我们的贡献包括引入一个新的任务，开发用于交互式点基三维编辑的 DragGaussian，以及通过定性和定量实验全面验证其有效性。
