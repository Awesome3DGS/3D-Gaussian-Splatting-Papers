### Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly Training for 4D Reconstruction

The recent development of 3D Gaussian Splatting (3DGS) has led to great interest in 4D dynamic spatial reconstruction from multi-view visual inputs. While existing approaches mainly rely on processing full-length multi-view videos for 4D reconstruction, there has been limited exploration of iterative online reconstruction methods that enable on-the-fly training and per-frame streaming. Current 3DGS-based streaming methods treat the Gaussian primitives uniformly and constantly renew the densified Gaussians, thereby overlooking the difference between dynamic and static features and also neglecting the temporal continuity in the scene. To address these limitations, we propose a novel three-stage pipeline for iterative streamable 4D dynamic spatial reconstruction. Our pipeline comprises a selective inheritance stage to preserve temporal continuity, a dynamics-aware shift stage for distinguishing dynamic and static primitives and optimizing their movements, and an error-guided densification stage to accommodate emerging objects. Our method achieves state-of-the-art performance in online 4D reconstruction, demonstrating a 20% improvement in on-the-fly training speed, superior representation quality, and real-time rendering capability.

最近，3D Gaussian Splatting（3DGS）的发展引发了对从多视角视觉输入中进行4D动态空间重建的广泛兴趣。现有方法主要依赖处理完整长度的多视角视频来实现4D重建，而对能够实现实时训练和逐帧流式处理的迭代在线重建方法探索较少。目前基于3DGS的流式方法通常将高斯原语视为统一对象，并不断更新密集化的高斯体，忽视了动态和静态特征之间的差异，同时也未充分利用场景中的时间连续性。为了解决这些问题，我们提出了一种新颖的三阶段流程，用于迭代的可流式4D动态空间重建。该流程包括选择性继承阶段以保持时间连续性、动态感知偏移阶段用于区分动态和静态原语并优化其移动，以及误差引导的密集化阶段以适应新出现的对象。我们的方法在在线4D重建中实现了最先进的性能，表现出训练速度提升20%、更高的表示质量以及实时渲染能力。
