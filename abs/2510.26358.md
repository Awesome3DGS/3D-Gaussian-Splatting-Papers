### AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM

Autonomous robots in orchards require real-time 3D scene understanding despite repetitive row geometry, seasonal appearance changes, and wind-driven foliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian Splatting (3DGS) rendering. Batch rasterization across complementary viewpoints recovers orchard structure under occlusions, while a unified gradient-driven map lifecycle executed between keyframes preserves fine details and bounds memory. Pose refinement is guided by a probabilistic LiDAR-based depth consistency term, back-propagated through the camera projection to tighten geometry-appearance coupling. We deploy the system on a field platform in apple and pear orchards across dormancy, flowering, and harvesting, using a standardized trajectory protocol that evaluates both training-view and novel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons and sites, AgriGS-SLAM delivers sharper, more stable reconstructions and steadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while maintaining real-time performance on-tractor. While demonstrated in orchard monitoring, the approach can be applied to other outdoor domains requiring robust multimodal perception.

果园中的自主机器人需要在具有重复行间几何、季节性外观变化和风致叶片运动等复杂条件下，实现实时的三维场景理解。为此，我们提出了 AgriGS-SLAM，一种结合了直接激光雷达里程计与回环检测的视觉–激光 SLAM 框架，并集成了多相机三维高斯泼洒（3DGS）渲染模块。该系统通过从互补视角批量光栅化，有效恢复了被遮挡区域的果园结构，同时在关键帧之间执行统一的梯度驱动地图生命周期管理，既保持了细节，又控制了内存使用。位姿优化由基于激光雷达的概率深度一致性项引导，并通过相机投影反向传播，从而增强几何与外观之间的耦合。我们在实地平台上将该系统部署于苹果和梨果园中，覆盖休眠期、开花期和采摘期，并使用标准化轨迹协议对训练视角和新视角的合成效果进行评估，从而降低 3DGS 在评估阶段的过拟合风险。在不同季节和场地中，AgriGS-SLAM 相较于当前最先进的 3DGS-SLAM 基线方法，在保证实时性（可在拖拉机上运行）的同时，实现了更清晰、更稳定的重建效果与更平稳的轨迹表现。虽然该方法主要应用于果园监测，但同样适用于其他需要强鲁棒多模态感知的户外环境。
