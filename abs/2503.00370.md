### Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic Pick-and-Place Setups

Simulating object dynamics from real-world perception shows great promise for digital twins and robotic manipulation but often demands labor-intensive measurements and expertise. We present a fully automated Real2Sim pipeline that generates simulation-ready assets for real-world objects through robotic interaction. Using only a robot's joint torque sensors and an external camera, the pipeline identifies visual geometry, collision geometry, and physical properties such as inertial parameters. Our approach introduces a general method for extracting high-quality, object-centric meshes from photometric reconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing alpha-transparent training while explicitly distinguishing foreground occlusions from background subtraction. We validate the full pipeline through extensive experiments, demonstrating its effectiveness across diverse objects. By eliminating the need for manual intervention or environment modifications, our pipeline can be integrated directly into existing pick-and-place setups, enabling scalable and efficient dataset creation.

从真实世界感知中模拟物体动态对于数字孪生 (Digital Twins) 和机器人操控具有巨大潜力，但通常需要繁琐的测量和专业知识。为此，我们提出了一种全自动的 Real2Sim 流水线，通过机器人交互自动生成可用于仿真的真实世界物体资产。
该流水线仅依赖机器人关节力矩传感器和外部摄像头，即可识别视觉几何、碰撞几何及惯性参数等物理属性。我们的方法提出了一种通用的高质量物体中心网格提取方案，适用于基于光度重建 (Photometric Reconstruction) 的技术（如 NeRF 和 高斯散点 (Gaussian Splatting)）。其中，我们通过透明 Alpha 训练 (Alpha-transparent training) 方案，在训练过程中显式区分前景遮挡和背景信息，提高重建质量。
我们通过大量实验验证了完整流水线的有效性，适用于多种不同物体。由于无需人工干预或环境修改，该流水线可直接集成到现有的抓取与放置 (Pick-and-Place) 任务中，实现可扩展且高效的数据集创建。
