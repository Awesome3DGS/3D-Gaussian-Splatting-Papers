### Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos

Currently almost all state-of-the-art novel view synthesis and reconstruction models rely on calibrated cameras or additional geometric priors for training. These prerequisites significantly limit their applicability to massive uncalibrated data. To alleviate this requirement and unlock the potential for self-supervised training on large-scale uncalibrated videos, we propose a novel two-stage strategy to train a view synthesis model from only raw video frames or multi-view images, without providing camera parameters or other priors. In the first stage, we learn to reconstruct the scene implicitly in a latent space without relying on any explicit 3D representation. Specifically, we predict per-frame latent camera and scene context features, and employ a view synthesis model as a proxy for explicit rendering. This pretraining stage substantially reduces the optimization complexity and encourages the network to learn the underlying 3D consistency in a self-supervised manner. The learned latent camera and implicit scene representation have a large gap compared with the real 3D world. To reduce this gap, we introduce the second stage training by explicitly predicting 3D Gaussian primitives. We additionally apply explicit Gaussian Splatting rendering loss and depth projection loss to align the learned latent representations with physically grounded 3D geometry. In this way, Stage 1 provides a strong initialization and Stage 2 enforces 3D consistency - the two stages are complementary and mutually beneficial. Extensive experiments demonstrate the effectiveness of our approach, achieving high-quality novel view synthesis and accurate camera pose estimation, compared to methods that employ supervision with calibration, pose, or depth information.

目前，几乎所有最先进的新视角合成与重建模型在训练过程中都依赖于经过标定的相机参数或额外的几何先验。这些前提条件极大限制了其在大规模未标定数据上的应用潜力。为突破这一限制，释放在大规模未标定视频上进行自监督训练的能力，本文提出了一种仅基于原始视频帧或多视图图像、无需提供相机参数或其他先验信息的全新两阶段训练策略。
在第一阶段，我们在潜空间中对场景进行隐式重建，完全不依赖任何显式的三维表示。具体而言，我们为每一帧预测潜在的相机特征和场景上下文特征，并使用视图合成模型作为显式渲染的代理。该预训练阶段显著降低了优化复杂度，同时引导网络以自监督方式学习潜在的三维一致性。
然而，所学习的潜在相机表示与隐式场景表示与真实三维世界之间仍存在显著差距。为缩小这一差距，我们引入第二阶段训练，显式预测三维高斯图元，并进一步引入高斯投影渲染损失与深度投影损失，将第一阶段中学到的潜在表示与真实物理三维几何对齐。第一阶段提供了强有力的初始化，第二阶段则强化了三维一致性——两者相辅相成，互为促进。
大量实验验证了我们方法的有效性，在无需标定、位姿或深度监督的前提下，依然能够实现高质量的新视角合成与精确的相机姿态估计，优于多种依赖监督的现有方法。

