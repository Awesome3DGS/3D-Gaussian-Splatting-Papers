### GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation

Creating high-quality, generalizable speech-driven 3D talking heads remains a persistent challenge. Previous methods achieve satisfactory results for fixed viewpoints and small-scale audio variations, but they struggle with large head rotations and out-of-distribution (OOD) audio. Moreover, they are constrained by the need for time-consuming, identity-specific training. We believe the core issue lies in the lack of sufficient 3D priors, which limits the extrapolation capabilities of synthesized talking heads. To address this, we propose GGTalker, which synthesizes talking heads through a combination of generalizable priors and identity-specific adaptation. We introduce a two-stage Prior-Adaptation training strategy to learn Gaussian head priors and adapt to individual characteristics. We train Audio-Expression and Expression-Visual priors to capture the universal patterns of lip movements and the general distribution of head textures. During the Customized Adaptation, individual speaking styles and texture details are precisely modeled. Additionally, we introduce a color MLP to generate fine-grained, motion-aligned textures and a Body Inpainter to blend rendered results with the background, producing indistinguishable, photorealistic video frames. Comprehensive experiments show that GGTalker achieves state-of-the-art performance in rendering quality, 3D consistency, lip-sync accuracy, and training efficiency.

生成高质量、具有泛化能力的语音驱动三维数字人面部动画一直是一个具有挑战性的课题。以往方法在固定视角和小尺度音频变化下可实现令人满意的效果，但在面对大幅度头部旋转和超出训练分布（OOD）的音频时表现不佳。此外，这些方法通常依赖于耗时的身份特定训练过程，限制了其实用性。我们认为，其核心问题在于缺乏充分的三维先验，导致生成数字人面部动画时的外推能力受限。为此，我们提出了 **GGTalker**，通过结合可泛化的先验知识与身份特定的自适应机制，实现数字人口型动画的合成。我们引入了一个两阶段的“先验-适应”训练策略，先学习高斯人头先验，再进行个体化特征适应。我们训练了**音频-表情先验**与**表情-视觉先验**，用于捕捉口型运动的通用模式和人头纹理的整体分布。在个性化适应阶段，我们精确建模个体的说话风格和纹理细节。此外，我们引入了**颜色 MLP** 用于生成细粒度、与动作对齐的纹理图，并设计了**背景修复模块（Body Inpainter）**以将渲染结果自然融合至背景中，生成高度逼真的视频帧。大量实验表明，GGTalker 在渲染质量、三维一致性、唇形同步精度以及训练效率方面均达到了当前最优性能。
