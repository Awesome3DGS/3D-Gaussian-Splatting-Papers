### EgoSplat: Open-Vocabulary Egocentric Scene Understanding with Language Embedded 3D Gaussian Splatting

Egocentric scenes exhibit frequent occlusions, varied viewpoints, and dynamic interactions compared to typical scene understanding tasks. Occlusions and varied viewpoints can lead to multi-view semantic inconsistencies, while dynamic objects may act as transient distractors, introducing artifacts into semantic feature modeling. To address these challenges, we propose EgoSplat, a language-embedded 3D Gaussian Splatting framework for open-vocabulary egocentric scene understanding. A multi-view consistent instance feature aggregation method is designed to leverage the segmentation and tracking capabilities of SAM2 to selectively aggregate complementary features across views for each instance, ensuring precise semantic representation of scenes. Additionally, an instance-aware spatial-temporal transient prediction module is constructed to improve spatial integrity and temporal continuity in predictions by incorporating spatial-temporal associations across multi-view instances, effectively reducing artifacts in the semantic reconstruction of egocentric scenes. EgoSplat achieves state-of-the-art performance in both localization and segmentation tasks on two datasets, outperforming existing methods with a 8.2% improvement in localization accuracy and a 3.7% improvement in segmentation mIoU on the ADT dataset, and setting a new benchmark in open-vocabulary egocentric scene understanding.

与典型的场景理解任务相比，第一人称视角场景具有频繁的遮挡、变化的视角和动态交互。遮挡和视角变化可能导致多视角语义不一致，而动态物体可能作为瞬时干扰物，引入伪影到语义特征建模中。为了解决这些挑战，我们提出了 EgoSplat，一个语言嵌入的 3D 高斯溅射框架，用于开放词汇的第一人称视角场景理解。我们设计了一种多视角一致的实例特征聚合方法，利用 SAM2 的分割和跟踪能力，选择性地跨视角聚合每个实例的互补特征，从而确保场景的精确语义表示。此外，我们构建了一个实例感知的时空瞬时预测模块，通过结合跨视角实例的时空关联，改善预测中的空间完整性和时间连续性，有效地减少了第一人称视角场景语义重建中的伪影。EgoSplat 在两个数据集上的定位和分割任务中取得了最先进的性能，在 ADT 数据集上，定位精度提高了 8.2%，分割 mIoU 提高了 3.7%，超越了现有方法，设立了开放词汇第一人称视角场景理解的新基准。
