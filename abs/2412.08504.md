### PointTalk: Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based Talking Head Synthesis

Talking head synthesis with arbitrary speech audio is a crucial challenge in the field of digital humans. Recently, methods based on radiance fields have received increasing attention due to their ability to synthesize high-fidelity and identity-consistent talking heads from just a few minutes of training video. However, due to the limited scale of the training data, these methods often exhibit poor performance in audio-lip synchronization and visual quality. In this paper, we propose a novel 3D Gaussian-based method called PointTalk, which constructs a static 3D Gaussian field of the head and deforms it in sync with the audio. It also incorporates an audio-driven dynamic lip point cloud as a critical component of the conditional information, thereby facilitating the effective synthesis of talking heads. Specifically, the initial step involves generating the corresponding lip point cloud from the audio signal and capturing its topological structure. The design of the dynamic difference encoder aims to capture the subtle nuances inherent in dynamic lip movements more effectively. Furthermore, we integrate the audio-point enhancement module, which not only ensures the synchronization of the audio signal with the corresponding lip point cloud within the feature space, but also facilitates a deeper understanding of the interrelations among cross-modal conditional features. Extensive experiments demonstrate that our method achieves superior high-fidelity and audio-lip synchronization in talking head synthesis compared to previous methods.

使用任意语音音频生成数字人头部的说话动画是数字人领域的关键挑战。近年来，基于辐射场的方法因其能够从仅几分钟的训练视频中生成高保真且身份一致的说话头像而受到越来越多的关注。然而，由于训练数据规模有限，这些方法通常在音频与唇形的同步性及视觉质量方面表现欠佳。
为了解决这些问题，我们提出了一种新颖的基于3D高斯的生成方法，称为 PointTalk。该方法通过构建一个静态的头部3D高斯场，并根据音频进行同步变形。同时，PointTalk 引入了一个由音频驱动的动态唇部点云作为条件信息的关键组成部分，从而有效促进了说话头像的合成。
具体而言，PointTalk 的初始步骤是从音频信号生成对应的唇部点云并捕捉其拓扑结构。设计的动态差分编码器（Dynamic Difference Encoder）旨在更有效地捕捉唇部动态运动中细微的变化。此外，我们集成了音频-点云增强模块（Audio-Point Enhancement Module），不仅在特征空间中确保音频信号与对应唇部点云的同步性，还促进了跨模态条件特征之间关系的深入理解。
大量实验表明，与现有方法相比，我们的方法在说话头像合成的高保真度和音频-唇形同步性方面均表现出色，为该领域的进一步发展提供了新的技术基础。
