### SplatVoxel: History-Aware Novel View Streaming without Temporal Training

We study the problem of novel view streaming from sparse-view videos, which aims to generate a continuous sequence of high-quality, temporally consistent novel views as new input frames arrive. However, existing novel view synthesis methods struggle with temporal coherence and visual fidelity, leading to flickering and inconsistency. To address these challenges, we introduce history-awareness, leveraging previous frames to reconstruct the scene and improve quality and stability. We propose a hybrid splat-voxel feed-forward scene reconstruction approach that combines Gaussian Splatting to propagate information over time, with a hierarchical voxel grid for temporal fusion. Gaussian primitives are efficiently warped over time using a motion graph that extends 2D tracking models to 3D motion, while a sparse voxel transformer integrates new temporal observations in an error-aware manner. Crucially, our method does not require training on multi-view video datasets, which are currently limited in size and diversity, and can be directly applied to sparse-view video streams in a history-aware manner at inference time. Our approach achieves state-of-the-art performance in both static and streaming scene reconstruction, effectively reducing temporal artifacts and visual artifacts while running at interactive rates (15 fps with 350ms delay) on a single H100 GPU. Project Page: this https URL

我们研究了来自稀疏视角视频的新颖视角流媒体生成问题，旨在随着新的输入帧到来，生成连续的高质量、时间一致的新视角。然而，现有的新颖视角合成方法在时间一致性和视觉保真度方面存在困难，导致闪烁和不一致。为了解决这些挑战，我们引入了历史感知，通过利用前面的帧来重建场景，提升质量和稳定性。我们提出了一种混合点云-体素前馈场景重建方法，结合了高斯点云渲染用于传播信息，采用层次化体素网格进行时间融合。通过一个运动图，我们高效地对高斯原语进行时间扭曲，将二维跟踪模型扩展到三维运动，同时一个稀疏体素变换器以错误感知的方式整合新的时间观测数据。关键的是，我们的方法不需要在多视角视频数据集上进行训练（当前这些数据集在规模和多样性上都有限），并且可以直接在推理时以历史感知的方式应用于稀疏视角视频流。我们的 approach 在静态和流媒体场景重建中都实现了最先进的性能，有效减少了时间伪影和视觉伪影，同时在单个H100 GPU上以交互速率（15帧/秒，350毫秒延迟）运行。
