### T2Bs: Text-to-Character Blendshapes via Video Generation

We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.

我们提出了T2Bs框架，一种结合静态文本到三维（text-to-3D）生成与视频扩散（video diffusion）的方法，用于从文本生成高质量、可动画化的角色头部可变形模型（morphable models）。现有的text-to-3D模型能够生成细致的静态几何结构，但缺乏运动合成能力；而视频扩散模型虽然能够生成运动，但通常存在时间一致性差和多视图几何不一致等问题。T2Bs通过引入可变形三维高斯溅射（deformable 3D Gaussian splatting）来对齐静态三维资产与视频输出，从而弥合两者之间的差距。该方法通过静态几何约束运动，并引入视角相关的变形多层感知机（view-dependent deformation MLP），从而：（i）在准确性与表现力方面优于现有的四维生成（4D generation）方法，同时有效减少视频伪影与视角不一致问题；（ii）能够重建平滑、连贯且完全配准的三维几何，为构建具有多样化且逼真面部运动的可变形模型提供可扩展基础。这一框架实现了超越当前4D生成技术的高表现力可动画角色头部合成能力。
