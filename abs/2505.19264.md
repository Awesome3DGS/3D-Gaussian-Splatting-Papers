### Improving Novel view synthesis of 360∘ Scenes in Extremely Sparse Views by Jointly Training Hemisphere Sampled Synthetic Images

Novel view synthesis in 360∘ scenes from extremely sparse input views is essential for applications like virtual reality and augmented reality. This paper presents a novel framework for novel view synthesis in extremely sparse-view cases. As typical structure-from-motion methods are unable to estimate camera poses in extremely sparse-view cases, we apply DUSt3R to estimate camera poses and generate a dense point cloud. Using the poses of estimated cameras, we densely sample additional views from the upper hemisphere space of the scenes, from which we render synthetic images together with the point cloud. Training 3D Gaussian Splatting model on a combination of reference images from sparse views and densely sampled synthetic images allows a larger scene coverage in 3D space, addressing the overfitting challenge due to the limited input in sparse-view cases. Retraining a diffusion-based image enhancement model on our created dataset, we further improve the quality of the point-cloud-rendered images by removing artifacts. We compare our framework with benchmark methods in cases of only four input views, demonstrating significant improvement in novel view synthesis under extremely sparse-view conditions for 360∘ scenes.

在极度稀疏视角下进行 360∘ 场景的新视角合成，对于虚拟现实（VR）和增强现实（AR）等应用至关重要。本文提出了一个用于极度稀疏视角条件下的新视角合成的新颖框架。
由于传统的结构光束法（Structure-from-Motion）方法难以在极度稀疏视角条件下准确估计相机位姿，我们采用 DUSt3R 进行相机位姿估计，并生成稠密点云。基于估计得到的相机位姿，我们从场景上半球空间中稠密采样附加视角，并结合点云渲染合成图像。
将来自稀疏视角的参考图像与稠密采样生成的合成图像相结合，对 3D Gaussian Splatting 模型进行训练，可实现对三维空间中更大场景范围的覆盖，从而缓解因输入过少导致的过拟合问题。此外，我们在自构建的数据集上对基于扩散模型的图像增强网络进行再训练，有效去除伪影，进一步提升点云渲染图像的质量。
我们在仅提供四个输入视角的条件下，将所提出的框架与多种基准方法进行对比，实验结果表明在极度稀疏视角下的 360∘ 场景新视角合成任务中，我们的方法显著优于现有方法。
