### Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars

In this paper, we propose to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based methods designed for single subjects often yield unsatisfactory results due to limited input views, various hand poses, and occlusions. To address these challenges, we introduce a novel two-stage interaction-aware GS framework that exploits cross-subject hand priors and refines 3D Gaussians in interacting areas. Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps. Learning-based features are captured by trained networks to provide reliable priors for poses, shapes, and textures, while optimization-based identity maps enable efficient one-shot fitting of out-of-distribution hands. Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. These modules enhance image rendering quality in areas with intra- and inter-hand interactions, overcoming the limitations of existing GS-based methods. Our proposed method is validated via extensive experiments on the large-scale InterHand2.6M dataset, and it significantly improves the state-of-the-art performance in image quality.

在本文中，我们提出了一种基于3D高斯散射（GS）和单张图像输入的可动画交互手部头像生成方法。现有的基于GS的单主体方法由于输入视角有限、手部姿势多样以及遮挡问题，常常导致效果不佳。为了解决这些挑战，我们引入了一种新颖的两阶段交互感知GS框架，该框架利用跨主体的手部先验知识，并对交互区域的3D高斯进行精细化处理。特别地，为了处理手部变化，我们将手部的3D表示解耦为基于优化的身份映射和基于学习的潜在几何特征以及神经纹理图。基于学习的特征通过训练的网络捕捉，用于提供姿势、形状和纹理的可靠先验，而基于优化的身份映射则能够高效地进行超出分布手部的一次性拟合。此外，我们设计了一个交互感知注意力模块和一个自适应高斯细化模块。这些模块增强了手部内部及手部之间交互区域的图像渲染质量，克服了现有基于GS方法的局限性。通过在大规模InterHand2.6M数据集上的广泛实验验证，我们的方法在图像质量方面显著提升了当前最先进方法的性能。
