### AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views

We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduce rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings.

我们提出了 AnySplat，一种用于新视角合成的前馈网络，可从无标定图像集合中直接进行推理。与传统神经渲染方法依赖已知相机位姿及每个场景的优化不同，也区别于近年来在密集视图下计算开销巨大的前馈方法，AnySplat 能够一键完成全部预测。通过一次前向传播，模型即可输出一组编码了场景几何与外观的三维高斯图元（3D Gaussian primitives），以及每张输入图像的相应相机内参与外参。
这一统一设计可无缝扩展至无姿态标注的随手拍摄的多视角数据集，无需额外预处理。在大量零样本测试中，AnySplat 在稀疏视图和密集视图场景中均能达到与依赖位姿的基线方法相媲美的渲染质量，同时显著超越现有无需位姿的方案。
此外，与基于优化的神经场方法相比，AnySplat 大幅降低了渲染延迟，使得在非约束采集环境中实现实时新视角合成成为可能。
