### RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors

A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality.

单次驾驶采集常常无法完整扫描道路结构，因此对重建场景进行扩展已成为传感器模拟器有效回归驾驶行为的关键需求。尽管当代三维高斯投影（3D Gaussian Splatting，3DGS）技术在重建质量方面表现出色，但其通过直接集成扩散先验进行扩展，往往会引入累积的物理不一致性，并降低训练效率。为克服上述局限，我们提出了 **RGE-GS** —— 一种融合扩散生成与奖励引导高斯集成的全新扩展式重建框架。RGE-GS 包含两项核心创新：其一，我们设计了一个奖励网络，在进入重建阶段前学习识别并优先保留稳定生成的结构模式，从而实现对扩散结果的选择性保留，提升空间稳定性；其二，在重建过程中，我们提出了一种差异化训练策略，能够根据场景收敛指标自动调节高斯优化进程，从而实现比基线方法更优的收敛效果。我们在多个公开数据集上进行了全面评估，结果表明 RGE-GS 在重建质量方面达到了当前最先进水平。
