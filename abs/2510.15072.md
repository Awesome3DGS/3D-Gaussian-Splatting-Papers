### SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images

Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable, on-the-fly reconstruction of sequential input views. However, existing methods often predict per-pixel Gaussians and combine Gaussians from all views as the scene representation, leading to substantial redundancies and geometric inconsistencies in long-duration video sequences. To address this, we propose SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction. To our best knowledge, SaLon3R is the first online generalizable GS method capable of reconstructing over 50 views in over 10 FPS, with 50% to 90% redundancy removal. Our method introduces compact anchor primitives to eliminate redundancy through differentiable saliency-aware Gaussian quantization, coupled with a 3D Point Transformer that refines anchor attributes and saliency to resolve cross-frame geometric and photometric inconsistencies. Specifically, we first leverage a 3D reconstruction backbone to predict dense per-pixel Gaussians and a saliency map encoding regional geometric complexity. Redundant Gaussians are compressed into compact anchors by prioritizing high-complexity regions. The 3D Point Transformer then learns spatial structural priors in 3D space from training data to refine anchor attributes and saliency, enabling regionally adaptive Gaussian decoding for geometric fidelity. Without known camera parameters or test-time optimization, our approach effectively resolves artifacts and prunes the redundant 3DGS in a single feed-forward pass. Experiments on multiple datasets demonstrate our state-of-the-art performance on both novel view synthesis and depth estimation, demonstrating superior efficiency, robustness, and generalization ability for long-term generalizable 3D reconstruction.

近年来，三维高斯溅射（3D Gaussian Splatting, 3DGS）在实现通用性强、可在线处理的逐帧视角重建方面取得了显著进展。然而，现有方法通常基于逐像素预测高斯表示，并将所有视角的高斯点直接组合为场景表示，这在处理长时序视频时易导致大量冗余和几何不一致问题。为此，我们提出SaLon3R，一种结构感知的长期3DGS重建新框架。据我们所知，SaLon3R是首个支持在线、通用的3DGS方法，能够以每秒超过10帧的速度重建超过50个视角，并实现50%至90%的冗余消除。该方法引入紧凑的锚点基元，通过可微的显著性感知高斯量化机制消除冗余，并结合三维点云Transformer网络（3D Point Transformer）对锚点属性和显著性进行优化，从而缓解跨帧几何与光照不一致问题。具体而言，我们首先利用三维重建骨干网络预测密集的逐像素高斯表示和编码区域几何复杂度的显著性图；再根据复杂区域优先原则，将冗余高斯压缩为紧凑锚点。随后，3D Point Transformer在三维空间中学习空间结构先验，对锚点属性与显著性进行细化，使解码过程在区域层面具备几何自适应性。在无需已知相机参数或测试时优化的前提下，我们的方法可在一次前向推理中有效修复伪影并裁剪冗余高斯。大量数据集实验表明，SaLon3R在新视图合成与深度估计任务上均达成当前最优性能，展现出在长期通用三维重建任务中的卓越效率、鲁棒性与泛化能力。
