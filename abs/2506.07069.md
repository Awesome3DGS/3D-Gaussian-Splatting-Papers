### Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization

3D Gaussian Splatting (3DGS) has recently gained significant attention for high-quality and efficient view synthesis, making it widely adopted in fields such as AR/VR, robotics, and autonomous driving. Despite its impressive algorithmic performance, real-time rendering on resource-constrained devices remains a major challenge due to tight power and area budgets. This paper presents an architecture-algorithm co-design to address these inefficiencies. First, we reveal substantial redundancy caused by repeated computation of common terms/expressions during the conventional rasterization. To resolve this, we propose axis-oriented rasterization, which pre-computes and reuses shared terms along both the X and Y axes through a dedicated hardware design, effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by identifying the resource and performance inefficiency of the sorting process, we introduce a novel neural sorting approach that predicts order-independent blending weights using an efficient neural network, eliminating the need for costly hardware sorters. A dedicated training framework is also proposed to improve its algorithmic stability. Third, to uniformly support rasterization and neural network inference, we design an efficient reconfigurable processing array that maximizes hardware utilization and throughput. Furthermore, we introduce a π-trajectory tile schedule, inspired by Morton encoding and Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead. Comprehensive experiments demonstrate that the proposed design preserves rendering quality while achieving a speedup of 23.4∼27.8× and energy savings of 28.8∼51.4× compared to edge GPUs for real-world scenes. We plan to open-source our design to foster further development in this field.

三维高斯泼溅（3D Gaussian Splatting, 3DGS）因其在高质量、高效率的新视角合成中的优异表现，近年来备受关注，已广泛应用于增强/虚拟现实（AR/VR）、机器人和自动驾驶等领域。然而，受限于功耗与面积预算，在资源受限设备上实现实时渲染仍面临巨大挑战。
本文提出一种**架构-算法协同设计（architecture-algorithm co-design）方法，以解决上述效率瓶颈。首先，我们揭示了传统光栅化过程中，由于重复计算公共项/表达式所导致的大量冗余。为此，我们提出轴向光栅化（axis-oriented rasterization）**方案，通过专用硬件设计沿 X 和 Y 轴预计算并复用共享项，有效减少了高达 63% 的乘加操作（MAC）。
其次，针对排序过程中的资源与性能低效问题，我们提出一种新颖的神经排序方法（neural sorting），通过高效神经网络预测与顺序无关的混合权重，替代高开销的硬件排序器。我们还构建了专门的训练框架，以增强该算法的稳定性。
第三，为统一支持光栅化与神经网络推理，我们设计了一个高效的可重构处理阵列（reconfigurable processing array），以最大化硬件利用率与吞吐率。此外，我们引入一种受莫顿编码（Morton encoding）与希尔伯特曲线（Hilbert curve）启发的π轨迹瓦片调度（π-trajectory tile schedule），进一步提升高斯重用率并减少内存访问开销。
综合实验表明，在保持渲染质量的前提下，该设计相比边缘端 GPU 可实现 23.4～27.8× 的加速比 和 28.8～51.4× 的能耗节省，在真实场景中表现显著。我们计划开源该设计，以推动该领域的进一步发展。
