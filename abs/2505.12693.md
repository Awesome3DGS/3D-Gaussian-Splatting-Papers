### TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy

The performance of multi-modal 3D occupancy prediction is limited by ineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion strategies and surface detail loss caused by sparse, noisy annotations. The mismatch stems from the heterogeneous scale and distribution of point cloud and image features, leading to biased matching under fixed neighborhood fusion. To address this, we propose a target-scale adaptive, bidirectional symmetric retrieval mechanism. It expands the neighborhood for large targets to enhance context awareness and shrinks it for small ones to improve efficiency and suppress noise, enabling accurate cross-modal feature alignment. This mechanism explicitly establishes spatial correspondences and improves fusion accuracy. For surface detail loss, sparse labels provide limited supervision, resulting in poor predictions for small objects. We introduce an improved volume rendering pipeline based on 3D Gaussian Splatting, which takes fused features as input to render images, applies photometric consistency supervision, and jointly optimizes 2D-3D consistency. This enhances surface detail reconstruction while suppressing noise propagation. In summary, we propose TACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy prediction, enhanced by volume rendering supervision. Experiments on the nuScenes and SemanticKITTI benchmarks validate its effectiveness.

多模态三维占据预测的性能受限于融合效果不佳，主要原因在于固定融合策略所导致的几何与语义不匹配，以及由于标注稀疏和噪声而引起的表面细节丢失。几何与语义的不匹配源于点云与图像特征在尺度与分布上的异构性，导致在固定邻域融合下匹配结果存在偏差。
为解决上述问题，我们提出了一种目标尺度自适应的双向对称检索机制。该机制对于大目标扩展邻域以增强上下文感知，对于小目标则收缩邻域以提升效率并抑制噪声，从而实现跨模态特征的精准对齐。该机制显式建立空间对应关系，显著提升了融合精度。
针对表面细节丢失的问题，由于稀疏标签提供的监督有限，导致对小目标的预测效果较差。为此，我们引入了一种改进的体渲染流程，基于 3D Gaussian Splatting，将融合后的特征作为输入进行图像渲染，利用光度一致性监督，实现对二维-三维一致性的联合优化。该机制在增强表面细节重建的同时，有效抑制了噪声传播。
综上，我们提出了 TACOcc，一个结合体渲染监督的自适应多模态融合框架，用于三维语义占据预测。我们在 nuScenes 和 SemanticKITTI 基准数据集上进行了实验，结果验证了该方法的有效性。
