### UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting

Recent advancements in multi-modal 3D pre-training methods have shown promising efficacy in learning joint representations of text, images, and point clouds. However, adopting point clouds as 3D representation fails to fully capture the intricacies of the 3D world and exhibits a noticeable gap between the discrete points and the dense 2D pixels of images. To tackle this issue, we propose UniGS, integrating 3D Gaussian Splatting (3DGS) into multi-modal pre-training to enhance the 3D representation. We first rely on the 3DGS representation to model the 3D world as a collection of 3D Gaussians with color and opacity, incorporating all the information of the 3D scene while establishing a strong connection with 2D images. Then, to achieve Language-Image-3D pertaining, UniGS starts with a pre-trained vision-language model to establish a shared visual and textual space through extensive real-world image-text pairs. Subsequently, UniGS employs a 3D encoder to align the optimized 3DGS with the Language-Image representations to learn unified multi-modal representations. To facilitate the extraction of global explicit 3D features by the 3D encoder and achieve better cross-modal alignment, we additionally introduce a novel Gaussian-Aware Guidance module that guides the learning of fine-grained representations of the 3D domain. Through extensive experiments across the Objaverse, ABO, MVImgNet and SUN RGBD datasets with zero-shot classification, text-driven retrieval and open-world understanding tasks, we demonstrate the effectiveness of UniGS in learning a more general and stronger aligned multi-modal representation. Specifically, UniGS achieves leading results across different 3D tasks with remarkable improvements over previous SOTA, Uni3D, including on zero-shot classification (+9.36%), text-driven retrieval (+4.3%) and open-world understanding (+7.92%).

近年来，多模态 3D 预训练方法在学习文本、图像和点云的联合表示方面取得了显著进展。然而，将点云作为 3D 表示方式难以充分捕捉 3D 世界的复杂细节，并且在离散点云与图像中密集 2D 像素之间存在明显的鸿沟。为了解决这一问题，我们提出了 UniGS，将 3D Gaussian Splatting (3DGS) 引入多模态预训练，以增强 3D 表示能力。
首先，UniGS 依赖 3DGS 表示，将 3D 世界建模为具有颜色和不透明度的 3D 高斯分布集合，从而完整地保留 3D 场景的所有信息，并与 2D 图像建立紧密联系。然后，为了实现 语言-图像-3D 预训练，UniGS 以一个 预训练的视觉-语言模型 作为起点，通过大规模的真实世界图文对构建共享的视觉和文本空间。随后，UniGS 采用 3D 编码器，将优化后的 3DGS 与 语言-图像表示 进行对齐，从而学习统一的多模态表示。
此外，为了促进 3D 编码器 提取全局显式 3D 特征并实现更优的跨模态对齐，我们进一步引入了一种新颖的 高斯感知引导（Gaussian-Aware Guidance）模块，用于引导 3D 领域的细粒度表示学习。
在 Objaverse、ABO、MVImgNet 和 SUN RGBD 数据集上，我们进行了 零样本分类、文本驱动检索和开放世界理解 等广泛实验，以验证 UniGS 在学习更通用且更强对齐的多模态表示方面的有效性。实验结果表明，与现有 SOTA 方法 Uni3D 相比，UniGS 在不同的 3D 任务中均取得领先表现，并在多个指标上带来了显著提升，包括 零样本分类提升 +9.36%，文本驱动检索提升 +4.3%，开放世界理解提升 +7.92%。
