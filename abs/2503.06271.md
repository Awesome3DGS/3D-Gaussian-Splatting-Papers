### SplatTalk: 3D VQA with Gaussian Splatting

Language-guided 3D scene understanding is important for advancing applications in robotics, AR/VR, and human-computer interaction, enabling models to comprehend and interact with 3D environments through natural language. While 2D vision-language models (VLMs) have achieved remarkable success in 2D VQA tasks, progress in the 3D domain has been significantly slower due to the complexity of 3D data and the high cost of manual annotations. In this work, we introduce SplatTalk, a novel method that uses a generalizable 3D Gaussian Splatting (3DGS) framework to produce 3D tokens suitable for direct input into a pretrained LLM, enabling effective zero-shot 3D visual question answering (3D VQA) for scenes with only posed images. During experiments on multiple benchmarks, our approach outperforms both 3D models trained specifically for the task and previous 2D-LMM-based models utilizing only images (our setting), while achieving competitive performance with state-of-the-art 3D LMMs that additionally utilize 3D inputs.

三维高斯散点（3D Gaussian Splatting，3DGS）的出现推动了三维场景重建和新视角合成的发展。随着对需要即时反馈的交互式应用的关注不断增长，实时在线 3DGS 重建的需求也日益增加。然而，现有方法尚无法满足这一需求，主要受到以下三大挑战的限制：缺乏预设相机参数、需要可泛化的 3DGS 优化，以及减少冗余的必要性。
为此，我们提出 StreamGS，一种用于无位姿图像流的在线可泛化 3DGS 重建方法，该方法通过预测和聚合逐帧高斯点，逐步将图像流转换为 3D 高斯流。我们的方法克服了初始点重建方法 \cite{dust3r} 在处理域外（OOD）场景时的局限性，引入了一种内容自适应优化（content adaptive refinement）。该优化方法通过在相邻帧之间建立可靠的像素对应关系来增强跨帧一致性。这种对应关系进一步帮助通过跨帧特征聚合合并冗余高斯点，从而减少高斯点的密度，大幅降低计算和内存开销，使在线重建成为可能。
在多个不同数据集上的广泛实验表明，StreamGS 在重建质量上可与基于优化的方法相媲美，但速度提高 150 倍，同时在处理 OOD 场景时展现出更强的泛化能力。
