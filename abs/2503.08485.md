### TT-GaussOcc: Test-Time Compute for Self-Supervised Occupancy Prediction via Spatio-Temporal Gaussian Splatting

Self-supervised 3D occupancy prediction offers a promising solution for understanding complex driving scenes without requiring costly 3D annotations. However, training dense voxel decoders to capture fine-grained geometry and semantics can demand hundreds of GPU hours, and such models often fail to adapt to varying voxel resolutions or new classes without extensive retraining. To overcome these limitations, we propose a practical and flexible test-time occupancy prediction framework termed TT-GaussOcc. Our approach incrementally optimizes time-aware 3D Gaussians instantiated from raw sensor streams at runtime, enabling voxelization at arbitrary user-specified resolution. Specifically, TT-GaussOcc operates in a "lift-move-voxel" symphony: we first "lift" surrounding-view semantics obtained from 2D vision foundation models (VLMs) to instantiate Gaussians at non-empty 3D space; Next, we "move" dynamic Gaussians from previous frames along estimated Gaussian scene flow to complete appearance and eliminate trailing artifacts of fast-moving objects, while accumulating static Gaussians to enforce temporal consistency; Finally, we mitigate inherent noises in semantic predictions and scene flow vectors by periodically smoothing neighboring Gaussians during optimization, using proposed trilateral RBF kernels that jointly consider color, semantic, and spatial affinities. The historical static and current dynamic Gaussians are then combined and voxelized to generate occupancy prediction. Extensive experiments on Occ3D and nuCraft with varying voxel resolutions demonstrate that TT-GaussOcc surpasses self-supervised baselines by 46% on mIoU without any offline training, and supports finer voxel resolutions at 2.6 FPS inference speed.

自监督 3D 占用预测（occupancy prediction）为理解复杂的驾驶场景提供了一种有前景的解决方案，而无需昂贵的 3D 标注。然而，训练用于捕捉细粒度几何和语义信息的稠密体素解码器可能需要数百 GPU 小时，并且这些模型往往难以适应不同的体素分辨率或新类别，除非进行大规模的重新训练。
为了解决这些局限性，我们提出了一种实用且灵活的测试时占用预测框架，称为 TT-GaussOcc。该方法在推理过程中增量优化由原始传感器流实时实例化的时序 3D 高斯（time-aware 3D Gaussians），从而实现任意用户指定分辨率的体素化。具体而言，TT-GaussOcc 采用“提升-移动-体素化”（lift-move-voxel）协同机制：首先，我们利用 2D 视觉基础模型（VLMs）提取的周围视图语义信息，将其“提升”到 3D 空间，实例化非空 3D 高斯；然后，我们根据估计的高斯场景流（Gaussian scene flow）对动态高斯进行“移动”，以补全外观信息，并消除快速移动物体的拖影伪影，同时累积静态高斯以增强时序一致性；最后，我们通过提出的三边 RBF 核（trilateral RBF kernels），综合考虑颜色、语义和空间邻域关系，在优化过程中对相邻高斯进行平滑，以缓解语义预测和场景流向量中的固有噪声。最终，历史静态高斯和当前动态高斯被合并并体素化，生成最终的占用预测结果。
在 Occ3D 和 nuCraft 数据集上的大量实验表明，在不同体素分辨率下，TT-GaussOcc 无需任何离线训练，即可比自监督基线方法在 mIoU 上提升 46%，并在 2.6 FPS 的推理速度下支持更精细的体素分辨率。
