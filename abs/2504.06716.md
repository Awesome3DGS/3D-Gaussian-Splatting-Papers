### GSta: Efficient Training Scheme with Siestaed Gaussians for Monocular 3D Scene Reconstruction

Gaussian Splatting (GS) is a popular approach for 3D reconstruction, mostly due to its ability to converge reasonably fast, faithfully represent the scene and render (novel) views in a fast fashion. However, it suffers from large storage and memory requirements, and its training speed still lags behind the hash-grid based radiance field approaches (e.g. Instant-NGP), which makes it especially difficult to deploy them in robotics scenarios, where 3D reconstruction is crucial for accurate operation. In this paper, we propose GSta that dynamically identifies Gaussians that have converged well during training, based on their positional and color gradient norms. By forcing such Gaussians into a siesta and stopping their updates (freezing) during training, we improve training speed with competitive accuracy compared to state of the art. We also propose an early stopping mechanism based on the PSNR values computed on a subset of training images. Combined with other improvements, such as integrating a learning rate scheduler, GSta achieves an improved Pareto front in convergence speed, memory and storage requirements, while preserving quality. We also show that GSta can improve other methods and complement orthogonal approaches in efficiency improvement; once combined with Trick-GS, GSta achieves up to 5x faster training, 16x smaller disk size compared to vanilla GS, while having comparable accuracy and consuming only half the peak memory.

高斯投影（Gaussian Splatting, GS）因其收敛速度较快、场景表达能力强以及能实现快速（新视角）渲染，已成为三维重建中广泛采用的方法。然而，GS 也存在存储和内存开销大的问题，其训练速度仍落后于基于哈希网格的辐射场方法（如 Instant-NGP），这使得其在对三维重建有高实时性要求的机器人应用场景中部署变得尤为困难。
为此，本文提出了 GSta，该方法在训练过程中动态识别已经充分收敛的高斯点，依据其位置和颜色梯度范数进行判断。对于这些已收敛的高斯，我们将其置于“休眠状态”，即在训练中停止更新（冻结），从而在保持精度竞争力的同时显著提升训练速度。
此外，我们还提出了一种基于训练图像子集的 PSNR 值的早停机制。结合其他优化措施（如引入学习率调度器），GSta 在收敛速度、内存与存储开销等方面构建出更优的 Pareto 前沿，同时保持输出质量不变。
我们进一步展示了 GSta 对其他方法的适配性与可组合性。在与 Trick-GS 结合使用时，GSta 可实现最高 5 倍的训练加速、磁盘占用减少至原始 GS 的 1/16，同时精度相当、峰值内存占用减半。
