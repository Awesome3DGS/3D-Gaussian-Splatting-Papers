### KeyGS: A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences

Reconstructing high-quality 3D models from sparse 2D images has garnered significant attention in computer vision. Recently, 3D Gaussian Splatting (3DGS) has gained prominence due to its explicit representation with efficient training speed and real-time rendering capabilities. However, existing methods still heavily depend on accurate camera poses for reconstruction. Although some recent approaches attempt to train 3DGS models without the Structure-from-Motion (SfM) preprocessing from monocular video datasets, these methods suffer from prolonged training times, making them impractical for many applications. In this paper, we present an efficient framework that operates without any depth or matching model. Our approach initially uses SfM to quickly obtain rough camera poses within seconds, and then refines these poses by leveraging the dense representation in 3DGS. This framework effectively addresses the issue of long training times. Additionally, we integrate the densification process with joint refinement and propose a coarse-to-fine frequency-aware densification to reconstruct different levels of details. This approach prevents camera pose estimation from being trapped in local minima or drifting due to high-frequency signals. Our method significantly reduces training time from hours to minutes while achieving more accurate novel view synthesis and camera pose estimation compared to previous methods.

从稀疏二维图像重建高质量三维模型是计算机视觉中的一个重要研究方向。近年来，三维高斯散射（3D Gaussian Splatting, 3DGS）因其显式表示、高效的训练速度和实时渲染能力而受到广泛关注。然而，现有方法在重建中仍然严重依赖于精确的相机位姿。尽管一些最新方法尝试在单目视频数据集上训练3DGS模型而无需依赖结构化运动（Structure-from-Motion, SfM）预处理，这些方法却由于训练时间过长而在许多应用场景中变得不切实际。
在本文中，我们提出了一个无需深度或匹配模型的高效框架。我们的方法首先通过SfM在几秒内快速获得粗略的相机位姿，然后利用3DGS的密集表示对这些位姿进行优化，从而有效解决了长时间训练的问题。此外，我们将密集化过程与联合优化相结合，提出了一种从粗到细的频率感知密集化方法，用于重建不同层次的细节。该方法能够避免相机位姿估计因高频信号陷入局部最小值或发生漂移。
与现有方法相比，我们的方法显著减少了训练时间，从数小时缩短至数分钟，同时在新视角合成和相机位姿估计的准确性上实现了更优的性能。
