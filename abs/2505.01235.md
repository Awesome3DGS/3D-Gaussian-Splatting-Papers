### Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting

Online reconstruction of dynamic scenes is significant as it enables learning scenes from live-streaming video inputs, while existing offline dynamic reconstruction methods rely on recorded video inputs. However, previous online reconstruction approaches have primarily focused on efficiency and rendering quality, overlooking the temporal consistency of their results, which often contain noticeable artifacts in static regions. This paper identifies that errors such as noise in real-world recordings affect temporal inconsistency in online reconstruction. We propose a method that enhances temporal consistency in online reconstruction from observations with temporal inconsistency which is inevitable in cameras. We show that our method restores the ideal observation by subtracting the learned error. We demonstrate that applying our method to various baselines significantly enhances both temporal consistency and rendering quality across datasets. Code, video results, and checkpoints are available at this https URL.

动态场景的在线重建具有重要意义，因为它能够从实时视频流中学习场景，而现有的离线动态重建方法则依赖于预先录制的视频输入。然而，已有的在线重建方法主要关注效率和渲染质量，往往忽视了结果的时间一致性，导致在静态区域中出现明显伪影等问题。
本文指出，现实世界录制中的噪声等误差是导致在线重建时间不一致性的主要原因。为此，我们提出了一种方法，用于在存在时间不一致性的观测条件下提升在线重建的时间一致性，这种不一致性在实际相机中是难以避免的。我们的方法通过学习误差并将其从观测值中减除，从而恢复理想观测。
我们在多个基线方法上应用所提出的方法，实验结果表明该方法在多个数据集上显著提升了时间一致性和渲染质量。
