### UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction

Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.

用于自动驾驶的前馈式三维重建技术发展迅速，但现有方法在面对稀疏且无重叠的相机视角以及复杂的场景动态时仍然面临巨大挑战。我们提出了 UniSplat，一个通用的前馈框架，通过统一的时空潜表示融合学习稳健的动态场景重建。UniSplat 构建了一个三维潜在支架，这是一种结构化表示，借助预训练基础模型来捕捉场景的几何与语义上下文。为了高效整合空间视角与时间帧之间的信息，我们引入了一种直接在三维支架中进行操作的高效融合机制，实现一致的时空对齐。为保证完整且细致的重建效果，我们设计了一个双分支解码器，从融合后的支架中生成对动态敏感的高斯表示，结合点锚定精细化与基于体素的生成方式，同时保留静态高斯的持久记忆，从而实现超出当前相机覆盖范围的流式场景补全。我们在真实场景数据集上进行了大量实验，结果表明 UniSplat 在新视角合成方面达到了当前最先进的性能，并在原始相机视角之外仍能生成稳健且高质量的渲染效果。
