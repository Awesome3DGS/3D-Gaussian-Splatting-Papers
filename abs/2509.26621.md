### HART: Human Aligned Reconstruction Transformer

We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings.

我们提出了 HART，一个用于稀疏视角人体重建的统一框架。该方法以一小组未经校准的人体 RGB 图像作为输入，输出包括：闭合的穿衣网格、对齐的 SMPL-X 身体网格，以及用于真实感新视角渲染的高斯泼溅表示。此前的穿衣人体重建方法通常是优化参数化模板，这种方式忽略了宽松服饰与人-物交互，或是在简化相机假设下训练隐式函数，从而限制了其在真实场景中的适用性。相比之下，HART 能预测每像素的三维点图、法向量和身体对应关系，并使用遮挡感知的泊松重建方法，恢复包括自遮挡区域在内的完整几何结构。这些预测结果还与参数化的 SMPL-X 人体模型对齐，确保重建几何既保持人体结构的一致性，又能捕捉宽松服饰和交互行为。这些对齐的人体网格进一步用于初始化高斯泼溅表示，以支持稀疏视角渲染。尽管仅在 2.3K 个合成扫描数据上进行了训练，HART 仍取得了当前最优的效果：在穿衣网格重建方面，Chamfer 距离提升了 18%-23%；在 SMPL-X 估计方面，PA-V2V 降低了 6%-27%；在新视角合成方面，LPIPS 降低了 15%-27%，涵盖了多种数据集。这些结果表明，前馈 Transformer 可以作为一种可扩展的模型，在真实场景中实现稳健的人体重建。
