### Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field

Reconstruction and rendering-based talking head synthesis methods achieve high-quality results with strong identity preservation but are limited by their dependence on identity-specific models. Each new identity requires training from scratch, incurring high computational costs and reduced scalability compared to generative model-based approaches. To overcome this limitation, we propose FIAG, a novel 3D speaking head synthesis framework that enables efficient identity-specific adaptation using only a few training footage. FIAG incorporates Global Gaussian Field, which supports the representation of multiple identities within a shared field, and Universal Motion Field, which captures the common motion dynamics across diverse identities. Benefiting from the shared facial structure information encoded in the Global Gaussian Field and the general motion priors learned in the motion field, our framework enables rapid adaptation from canonical identity representations to specific ones with minimal data. Extensive comparative and ablation experiments demonstrate that our method outperforms existing state-of-the-art approaches, validating both the effectiveness and generalizability of the proposed framework.

基于重建与渲染的语音驱动人脸合成方法在身份保持方面表现优异，能够生成高质量的说话人头部动画，但受限于对身份特定模型的高度依赖。每新增一个身份都需从头训练，带来较高的计算成本，且相较于生成式方法可扩展性较差。为突破这一限制，我们提出了 FIAG —— 一种新颖的三维说话人脸合成框架，能够通过少量训练数据实现高效的身份特定自适应。FIAG 引入了**全局高斯场（Global Gaussian Field）**，用于在共享空间中表示多个身份，同时引入**通用运动场（Universal Motion Field）**，用于捕捉不同身份间的共通动态变化模式。得益于全局高斯场中编码的共享面部结构信息，以及运动场中学习到的通用运动先验，FIAG 能够以极少数据实现从标准身份表示到特定身份表示的快速适应。大量对比实验与消融分析表明，该方法在效果和泛化性方面均优于现有的主流方法，有效验证了所提出框架的先进性与实用性。
