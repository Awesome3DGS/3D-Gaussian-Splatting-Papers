### No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views

We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation.

我们提出了 SPFSplat，这是一种高效的三维高斯泼溅框架，可从稀疏多视图图像中进行建模，训练和推理过程中均无需真实位姿。该方法采用共享特征提取骨干网络，使得在单次前向推理中即可从无位姿输入同时预测标准空间下的三维高斯基元和相机位姿。除了基于估计的新视角位姿的渲染损失外，还引入了重投影损失，以强化像素对齐的高斯基元学习，从而增强几何约束。这种无位姿监督的训练范式与高效的一步前向设计，使 SPFSplat 非常适用于实际应用。值得注意的是，即使在缺乏位姿监督的情况下，SPFSplat 在新视角合成中依然取得了最新的性能，即便在视角变化较大和图像重叠有限的条件下亦如此。此外，它在相对位姿估计中也优于利用几何先验进行训练的最新方法。
