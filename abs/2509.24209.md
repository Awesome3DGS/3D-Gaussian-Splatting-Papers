### Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos

Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view videos is critical for numerous downstream applications. Existing methods, however, are either limited by the slow reconstruction speeds or incapable of generating novel-time representations. To address these challenges, we propose Forge4D, a feed-forward 4D human reconstruction and interpolation model that efficiently reconstructs temporally aligned representations from uncalibrated sparse-view videos, enabling both novel view and novel time synthesis. Our model simplifies the 4D reconstruction and interpolation problem as a joint task of streaming 3D Gaussian reconstruction and dense motion prediction. For the task of streaming 3D Gaussian reconstruction, we first reconstruct static 3D Gaussians from uncalibrated sparse-view images and then introduce learnable state tokens to enforce temporal consistency in a memory-friendly manner by interactively updating shared information across different timestamps. For novel time synthesis, we design a novel motion prediction module to predict dense motions for each 3D Gaussian between two adjacent frames, coupled with an occlusion-aware Gaussian fusion process to interpolate 3D Gaussians at arbitrary timestamps. To overcome the lack of the ground truth for dense motion supervision, we formulate dense motion prediction as a dense point matching task and introduce a self-supervised retargeting loss to optimize this module. An additional occlusion-aware optical flow loss is introduced to ensure motion consistency with plausible human movement, providing stronger regularization. Extensive experiments demonstrate the effectiveness of our model on both in-domain and out-of-domain datasets.

从未标定的稀疏视角视频中即时重建动态三维人体，对众多下游应用至关重要。然而，现有方法要么受限于较慢的重建速度，要么无法生成新的时间维度表示。为解决这些问题，我们提出了 **Forge4D**，一种前馈式的四维人体重建与插值模型，能够从未标定的稀疏视角视频中高效重建时间对齐的表示，从而同时实现新视角与新时间的合成。我们的模型将四维重建与插值问题简化为“流式三维高斯重建”和“稠密运动预测”的联合任务。对于流式三维高斯重建任务，我们首先从未标定的稀疏视角图像中重建静态三维高斯，然后引入可学习的状态标记（learnable state tokens），通过在不同时间戳间交互更新共享信息，以一种内存友好的方式强制时间一致性。针对新时间合成，我们设计了一个新颖的运动预测模块，用于预测相邻帧之间每个三维高斯的稠密运动，并结合遮挡感知的高斯融合过程（occlusion-aware Gaussian fusion），在任意时间戳插值出三维高斯。为克服缺乏稠密运动监督真值的问题，我们将稠密运动预测建模为稠密点匹配任务，并引入自监督的重定向损失（self-supervised retargeting loss）来优化该模块。此外，我们还引入了遮挡感知的光流损失（occlusion-aware optical flow loss），以保证运动的一致性与合理性，提供更强的正则约束。大量实验结果表明，Forge4D 在域内与跨域数据集上均取得了显著的性能提升。
