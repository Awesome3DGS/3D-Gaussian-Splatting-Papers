### Instruct-4DGS: Efficient Dynamic Scene Editing via 4D Gaussian-based Static-Dynamic Separation

Recent 4D dynamic scene editing methods require editing thousands of 2D images used for dynamic scene synthesis and updating the entire scene with additional training loops, resulting in several hours of processing to edit a single dynamic scene. Therefore, these methods are not scalable with respect to the temporal dimension of the dynamic scene (i.e., the number of timesteps). In this work, we propose Instruct-4DGS, an efficient dynamic scene editing method that is more scalable in terms of temporal dimension. To achieve computational efficiency, we leverage a 4D Gaussian representation that models a 4D dynamic scene by combining static 3D Gaussians with a Hexplane-based deformation field, which captures dynamic information. We then perform editing solely on the static 3D Gaussians, which is the minimal but sufficient component required for visual editing. To resolve the misalignment between the edited 3D Gaussians and the deformation field, which may arise from the editing process, we introduce a refinement stage using a score distillation mechanism. Extensive editing results demonstrate that Instruct-4DGS is efficient, reducing editing time by more than half compared to existing methods while achieving high-quality edits that better follow user instructions.

现有的4D动态场景编辑方法需要对用于动态场景合成的数千张2D图像进行修改，并通过额外的训练迭代更新整个场景，因此编辑单个动态场景往往需要数小时处理时间。因此，这些方法在动态场景的时间维度（即时间步数）上缺乏可扩展性。本文提出了 Instruct-4DGS，这是一种在时间维度上具有更高可扩展性的高效动态场景编辑方法。为实现计算效率，我们采用了一种4D高斯表示，通过结合静态3D高斯与基于Hexplane的形变场来建模4D动态场景，其中形变场用于捕捉动态信息。随后，我们仅在静态3D高斯上进行编辑，这是实现视觉编辑所需的最小但充分的部分。为了解决在编辑过程中可能出现的已编辑3D高斯与形变场之间的不对齐问题，我们引入了一个基于得分蒸馏机制的精炼阶段。大量的编辑结果表明，Instruct-4DGS高效，将编辑时间相比现有方法缩短了一半以上，同时能够实现更高质量的编辑，并更好地遵循用户指令。
