### CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation

Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.

指向性三维高斯投影分割（Referring 3D Gaussian Splatting Segmentation, R3DGS）旨在理解自由形式的语言表达，并在高斯场中定位对应的三维区域。尽管近年来在语言与三维几何之间的跨模态对齐方面取得了进展，现有方法仍依赖于二维渲染的伪监督和视图特定的特征学习，难以实现视角一致性。在本研究中，我们提出了Camera Aware Referring Field（CaRF），一个直接在三维高斯空间中操作并实现多视角一致性的全可微分框架。具体而言，CaRF引入了高斯场相机编码（Gaussian Field Camera Encoding, GFCE），将相机几何信息融入高斯与文本的交互之中，显式建模视图相关的变化并增强几何推理能力。在此基础上，我们提出了训练中配对视图监督（In Training Paired View Supervision, ITPVS），在训练过程中对校准视图下的每个高斯的logits进行对齐，有效缓解了单视图过拟合，并暴露视图间差异以供优化。我们在三个具有代表性的基准数据集上进行了大量实验，结果表明CaRF在Ref LERF、LERF OVS和3D OVS数据集上相较于现有最先进方法在mIoU指标上分别实现了16.8%、4.3%和2.0%的平均提升。此外，该方法有助于实现更可靠、视角一致的三维场景理解，在具身智能、增强/虚拟现实交互以及自动感知等领域具有广阔的应用前景。
