### Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields

We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that achieves similar or better accuracy than neural volumetric approaches and is 100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. In C-ToF radiance field reconstruction, the property of interest-depth-is not directly measured, causing an additional challenge. This problem has a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussian splatting, which is commonly used with multi-view data to produce satisfactory results and is brittle in its optimization otherwise. We incorporate two heuristics into the optimization to improve the accuracy of scene geometry represented by Gaussians. Experimental results show that our approach produces accurate reconstructions under constrained C-ToF sensing conditions, including for fast motions like swinging baseball bats.

我们提出了一种利用单目连续波飞行时间（Continuous-wave Time-of-Flight, C-ToF）相机的原始传感器采样数据重建动态场景的方法，其重建精度可与神经体积方法相媲美甚至更优，且速度提升达 100 倍。在计算机视觉中，从单一视角快速实现高保真动态三维重建仍是一个重大挑战。
在 C-ToF 辐射场重建中，目标属性——深度——并非直接观测量，这为重建任务带来了额外困难。当使用如三维高斯泼溅（3D Gaussian Splatting）等快速图元表示方法进行场景建模时，这一问题对优化过程的影响尤为显著。3DGS 通常依赖多视角数据才能获得令人满意的结果，在单视角条件下其优化过程极为脆弱。
为提升基于高斯表示的场景几何精度，我们在优化过程中引入了两条启发式策略。实验结果表明，在受限的 C-ToF 传感条件下，包括处理如挥棒等高速动态的情形，我们的方法依然能够实现高精度重建。
