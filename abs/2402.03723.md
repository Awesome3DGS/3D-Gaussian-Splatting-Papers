### Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos

Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments.

由于在增强现实/虚拟现实应用中的巨大价值，从普通智能手机视频中创建可控的3D人像变得非常受欢迎。最近3D高斯喷溅（3DGS）的发展在渲染质量和训练效率上显示出了改进。然而，从单一视角捕捉准确地建模和分离头部移动和面部表情以实现高质量渲染仍然是一个挑战。在这篇论文中，我们介绍了Rig3DGS来解决这个挑战。我们使用一组3D高斯在规范空间表示整个场景，包括动态主题。使用一组控制信号，如头部姿势和表情，我们通过学习到的形变将它们转换到3D空间中，以生成所需的渲染。我们的关键创新是一个精心设计的形变方法，该方法由来自3D可塑模型的可学习先验指导。这种方法在训练中高效，并且在控制面部表情、头部位置和跨各种捕捉的视图合成上非常有效。我们通过广泛的定量和定性实验演示了我们学习到的形变的有效性。
