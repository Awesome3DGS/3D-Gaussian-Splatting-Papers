### UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring

Reconstructing dynamic 3D scenes from monocular video has broad applications in AR/VR, robotics, and autonomous navigation, but often fails due to severe motion blur caused by camera and object motion. Existing methods commonly follow a two-step pipeline, where camera poses are first estimated and then 3D Gaussians are optimized. Since blurring artifacts usually undermine pose estimation, pose errors could be accumulated to produce inferior reconstruction results. To address this issue, we introduce a unified optimization framework by incorporating camera poses as learnable parameters complementary to 3DGS attributes for end-to-end optimization. Specifically, we recast camera and object motion as per-primitive SE(3) affine transformations on 3D Gaussians and formulate a unified optimization objective. For stable optimization, we introduce a three-stage training schedule that optimizes camera poses and Gaussians alternatively. Particularly, 3D Gaussians are first trained with poses being fixed, and then poses are optimized with 3D Gaussians being untouched. Finally, all learnable parameters are optimized together. Extensive experiments on the Stereo Blur dataset and challenging real-world sequences demonstrate that our method achieves significant gains in reconstruction quality and pose estimation accuracy over prior dynamic deblurring methods.

从单目视频中重建动态三维场景在AR/VR、机器人和自动导航中具有广泛应用，但常因相机和物体运动导致的严重运动模糊而失败。现有方法通常遵循两步流程：先估计相机位姿，再优化三维高斯。由于模糊伪影常常破坏位姿估计，位姿误差会不断累积，最终导致较差的重建结果。为解决这一问题，我们提出了一个统一优化框架，将相机位姿作为可学习参数，与3DGS属性互补，实现端到端优化。具体而言，我们将相机和物体运动重新表述为三维高斯上的逐基元SE(3)仿射变换，并构建统一的优化目标。为保证优化的稳定性，我们设计了一个三阶段训练策略，交替优化相机位姿和高斯：首先在固定位姿的情况下训练三维高斯，然后在保持三维高斯不变的情况下优化位姿，最后联合优化所有可学习参数。大量在Stereo Blur数据集和具有挑战性的真实序列上的实验表明，我们的方法在重建质量和位姿估计精度方面较现有动态去模糊方法取得了显著提升。
