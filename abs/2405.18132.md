### EG4D: Explicit Generation of 4D Object without Score Distillation

In recent years, the increasing demand for dynamic 3D assets in design and gaming applications has given rise to powerful generative pipelines capable of synthesizing high-quality 4D objects. Previous methods generally rely on score distillation sampling (SDS) algorithm to infer the unseen views and motion of 4D objects, thus leading to unsatisfactory results with defects like over-saturation and Janus problem. Therefore, inspired by recent progress of video diffusion models, we propose to optimize a 4D representation by explicitly generating multi-view videos from one input image. However, it is far from trivial to handle practical challenges faced by such a pipeline, including dramatic temporal inconsistency, inter-frame geometry and texture diversity, and semantic defects brought by video generation results. To address these issues, we propose DG4D, a novel multi-stage framework that generates high-quality and consistent 4D assets without score distillation. Specifically, collaborative techniques and solutions are developed, including an attention injection strategy to synthesize temporal-consistent multi-view videos, a robust and efficient dynamic reconstruction method based on Gaussian Splatting, and a refinement stage with diffusion prior for semantic restoration. The qualitative results and user preference study demonstrate that our framework outperforms the baselines in generation quality by a considerable margin.

近年来，设计和游戏应用中对动态3D资产的需求日益增加，促使强大的生成管道能够合成高质量的4D对象。以前的方法通常依赖于分数蒸馏采样（SDS）算法来推断4D对象的未见视图和运动，从而导致结果不尽人意，存在过饱和和“Janus问题”等缺陷。因此，受到最近视频扩散模型进展的启发，我们提出通过显式生成多视角视频从一个输入图像优化4D表示。然而，处理这样一个管道所面临的实际挑战，包括显著的时间不一致性、帧间几何和纹理多样性以及视频生成结果带来的语义缺陷，绝非易事。为了解决这些问题，我们提出了 DG4D，一个新的多阶段框架，它在不使用分数蒸馏的情况下生成高质量且一致的4D资产。具体而言，我们开发了包括注意力注入策略以合成时间一致的多视角视频、基于高斯喷溅的稳健且高效的动态重建方法，以及带有扩散先验的精炼阶段用于语义恢复的协作技术和解决方案。定性结果和用户偏好研究表明，我们的框架在生成质量上大幅超过了基准线。
