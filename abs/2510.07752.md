### DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream

Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.

从低帧率 RGB 视频中重建动态三维高斯泼溅（3DGS）是一项具有挑战性的任务。这是因为帧间存在较大的运动，会增加解空间的不确定性。例如，第一帧中的一个像素可能对应到第二帧中多个不同的位置。事件相机可以异步捕捉快速的视觉变化，并且对运动模糊具有鲁棒性，但它们不提供颜色信息。直观来看，事件流所记录的轨迹可以为帧间大位移提供确定性约束。因此，将低时域分辨率的图像与高帧率事件流结合使用，有望解决该问题。然而，由于 RGB 图像与事件数据之间存在显著模态差异，如何联合优化动态 3DGS 成为一大难点。本文提出了一种新颖的框架，用于融合这两种模态对动态 3DGS 进行联合优化。其核心思想是利用事件中的运动先验来引导变形场的优化。具体而言，我们首先提出了名为 LoCM 的无监督微调框架，用于将事件流估计器自适应到特定的未知场景中，从而提取事件流中蕴含的运动先验。接着，我们设计了几何感知的数据关联方法，建立事件与高斯之间的运动对应关系，这是整个流程的基础，并辅以运动分解与帧间伪标签两项策略以提升性能。大量实验结果表明，本文方法在合成与真实场景中均优于现有基于图像或事件的方案，验证了事件数据对于动态 3DGS 优化的有效性。
