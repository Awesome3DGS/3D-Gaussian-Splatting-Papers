### GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading

The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.

三维高斯溅射（3D Gaussian Splatting）的出现革新了图形渲染领域，在保证高视觉质量的同时实现了高速渲染。然而，在大规模场景下实现高质量训练仍然面临挑战，其主要瓶颈在于存储参数、梯度及优化器状态所需的巨大显存开销，这些需求极易耗尽 GPU 内存。为应对这一问题，我们提出了 **GS-Scale**——一种针对三维高斯溅射的快速且内存高效的训练系统。GS-Scale 将所有高斯基元存储在主机内存中，并在每次前向与反向传播时按需将部分数据传输至 GPU。尽管这一设计显著降低了 GPU 内存占用，但也导致视锥裁剪（frustum culling）与优化器更新需在 CPU 上执行，从而因 CPU 计算与内存带宽受限而引入性能瓶颈。为缓解这一问题，GS-Scale 采用了三项系统级优化策略：（1）**几何参数的选择性卸载**，以加速视锥裁剪；（2）**参数前传机制**，通过流水线方式并行 CPU 优化器更新与 GPU 计算；（3）**延迟优化更新策略**，以减少对梯度为零的高斯基元的冗余内存访问。我们在大规模数据集上的实验表明，GS-Scale 可将 GPU 内存需求降低 3.3–5.6 倍，同时在训练速度上与无主机卸载的纯 GPU 训练相当。该系统使得在消费级 GPU 上进行大规模 3DGS 训练成为可能——例如，GS-Scale 可在 RTX 4070 Mobile GPU 上将高斯数量从 400 万扩展至 1800 万，从而在 LPIPS（学习感知图像块相似度）指标上提升 23–35%。
