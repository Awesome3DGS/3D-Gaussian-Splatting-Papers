### Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors

Synthesizing consistent and photorealistic 3D scenes is an open problem in computer vision. Video diffusion models generate impressive videos but cannot directly synthesize 3D representations, i.e., lack 3D consistency in the generated sequences. In addition, directly training generative 3D models is challenging due to a lack of 3D training data at scale. In this work, we present Generative Gaussian Splatting (GGS) -- a novel approach that integrates a 3D representation with a pre-trained latent video diffusion model. Specifically, our model synthesizes a feature field parameterized via 3D Gaussian primitives. The feature field is then either rendered to feature maps and decoded into multi-view images, or directly upsampled into a 3D radiance field. We evaluate our approach on two common benchmark datasets for scene synthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model significantly improves both the 3D consistency of the generated multi-view images, and the quality of the generated 3D scenes over all relevant baselines. Compared to a similar model without 3D representation, GGS improves FID on the generated 3D scenes by ~20% on both RealEstate10K and ScanNet+.

合成一致且逼真的三维场景是计算机视觉中的一个开放问题。视频扩散模型能够生成令人印象深刻的视频，但不能直接合成三维表示，即在生成的序列中缺乏三维一致性。此外，由于缺乏大规模的三维训练数据，直接训练生成式三维模型也非常具有挑战性。在这项工作中，我们提出了生成式高斯点云渲染（GGS）——一种将三维表示与预训练的潜在视频扩散模型结合的创新方法。具体而言，我们的模型合成一个通过三维高斯原语参数化的特征场。然后，这个特征场可以被渲染为特征图并解码为多视角图像，或者直接上采样为三维辐射场。我们在两个常见的场景合成基准数据集——RealEstate10K和ScanNet+上评估了我们的方法，发现我们提出的GGS模型显著提高了生成的多视角图像的三维一致性以及生成的三维场景的质量，优于所有相关基线。与没有三维表示的类似模型相比，GGS在生成的三维场景上的FID指标在RealEstate10K和ScanNet+上分别提高了约20%。
