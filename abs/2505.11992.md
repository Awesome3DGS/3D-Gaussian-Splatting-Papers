### SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations

Novel view synthesis (NVS) boosts immersive experiences in computer vision and graphics. Existing techniques, though progressed, rely on dense multi-view observations, restricting their application. This work takes on the challenge of reconstructing photorealistic 3D scenes from sparse or single-view inputs. We introduce SpatialCrafter, a framework that leverages the rich knowledge in video diffusion models to generate plausible additional observations, thereby alleviating reconstruction ambiguity. Through a trainable camera encoder and an epipolar attention mechanism for explicit geometric constraints, we achieve precise camera control and 3D consistency, further reinforced by a unified scale estimation strategy to handle scale discrepancies across datasets. Furthermore, by integrating monocular depth priors with semantic features in the video latent space, our framework directly regresses 3D Gaussian primitives and efficiently processes long-sequence features using a hybrid network structure. Extensive experiments show our method enhances sparse view reconstruction and restores the realistic appearance of 3D scenes.

新视角合成（Novel View Synthesis, NVS）在计算机视觉与图形学中极大提升了沉浸式体验。尽管现有技术已取得显著进展，但普遍依赖密集多视角观测，限制了其在实际应用中的可用性。本文针对从稀疏甚至单视角输入中重建真实感三维场景的挑战，提出了 SpatialCrafter 框架。
该框架通过利用视频扩散模型中蕴含的丰富先验知识，生成合理的补充观测，从而缓解重建歧义问题。我们设计了一个可训练的摄像机编码器以及带有显式几何约束的极线注意力机制（epipolar attention mechanism），实现了精确的摄像机控制与三维一致性，并引入统一的尺度估计策略以应对不同数据集间的尺度差异。
此外，SpatialCrafter 将单目深度先验与语义特征融合到视频潜空间中，直接回归生成三维高斯图元（3D Gaussian primitives），并通过混合神经网络结构高效处理长序列特征。大量实验表明，该方法在稀疏视角重建任务中表现优异，能够有效还原三维场景的真实外观。
