### Learning 3D-Gaussian Simulators from RGB Videos

Learning physics simulations from video data requires maintaining spatial and temporal consistency, a challenge often addressed with strong inductive biases or ground-truth 3D information -- limiting scalability and generalization. We introduce 3DGSim, a 3D physics simulator that learns object dynamics end-to-end from multi-view RGB videos. It encodes images into a 3D Gaussian particle representation, propagates dynamics via a transformer, and renders frames using 3D Gaussian splatting. By jointly training inverse rendering with a dynamics transformer using a temporal encoding and merging layer, 3DGSimembeds physical properties into point-wise latent vectors without enforcing explicit connectivity constraints. This enables the model to capture diverse physical behaviors, from rigid to elastic and cloth-like interactions, along with realistic lighting effects that also generalize to unseen multi-body interactions and novel scene edits.

从视频数据中学习物理仿真需要同时保持空间与时间的一致性，而现有方法通常依赖强归纳偏置或真实三维信息，限制了其可扩展性与泛化能力。
本文提出了3DGSim，一种能够从多视角RGB视频端到端学习物体动态的三维物理仿真器。3DGSim将图像编码为三维高斯粒子表示（3D Gaussian particle representation），通过变换器（transformer）传播动态信息，并利用三维高斯泼洒（3D Gaussian Splatting）渲染各帧图像。通过联合训练逆向渲染模块与动态变换器，结合时间编码与融合层，3DGSim在无需显式连接约束的情况下，将物理属性嵌入到逐点的潜变量中。
这种方法使得模型能够捕捉多样的物理行为，从刚体、弹性体到类似布料的交互，并呈现真实的光照效果，同时在面对未见过的多物体交互和新场景编辑时依然具有良好的泛化能力。
