### MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction

Recent breakthroughs in radiance fields have significantly advanced 3D scene reconstruction and novel view synthesis (NVS) in autonomous driving. Nevertheless, critical limitations persist: reconstruction-based methods exhibit substantial performance deterioration under significant viewpoint deviations from training trajectories, while generation-based techniques struggle with temporal coherence and precise scene controllability. To overcome these challenges, we present MuDG, an innovative framework that integrates Multi-modal Diffusion model with Gaussian Splatting (GS) for Urban Scene Reconstruction. MuDG leverages aggregated LiDAR point clouds with RGB and geometric priors to condition a multi-modal video diffusion model, synthesizing photorealistic RGB, depth, and semantic outputs for novel viewpoints. This synthesis pipeline enables feed-forward NVS without computationally intensive per-scene optimization, providing comprehensive supervision signals to refine 3DGS representations for rendering robustness enhancement under extreme viewpoint changes. Experiments on the Open Waymo Dataset demonstrate that MuDG outperforms existing methods in both reconstruction and synthesis quality.

最近，辐射场的突破性进展显著推动了自动驾驶中的 3D 场景重建和新视角合成（NVS）。然而，仍然存在一些关键的局限性：基于重建的方法在视角偏离训练轨迹较大时性能显著下降，而基于生成的技术则在时间一致性和精确场景可控性方面存在问题。为了解决这些挑战，我们提出了 MuDG，这是一个创新框架，将多模态扩散模型与高斯溅射（GS）结合用于城市场景重建。MuDG 利用聚合的 LiDAR 点云、RGB 图像和几何先验来调节多模态视频扩散模型，从而合成新视角下的光照逼真的 RGB 图像、深度图和语义输出。该合成流程实现了前馈式的新视角合成，无需对每个场景进行计算密集型的优化，并提供全面的监督信号，优化 3DGS 表征，从而提升在极端视角变化下的渲染鲁棒性。在 Open Waymo 数据集上的实验表明，MuDG 在重建和合成质量方面均优于现有方法。
