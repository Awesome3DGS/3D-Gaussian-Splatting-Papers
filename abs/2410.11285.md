### Scalable Indoor Novel-View Synthesis using Drone-Captured 360 Imagery with 3D Gaussian Splatting

Scene reconstruction and novel-view synthesis for large, complex, multi-story, indoor scenes is a challenging and time-consuming task. Prior methods have utilized drones for data capture and radiance fields for scene reconstruction, both of which present certain challenges. First, in order to capture diverse viewpoints with the drone's front-facing camera, some approaches fly the drone in an unstable zig-zag fashion, which hinders drone-piloting and generates motion blur in the captured data. Secondly, most radiance field methods do not easily scale to arbitrarily large number of images. This paper proposes an efficient and scalable pipeline for indoor novel-view synthesis from drone-captured 360 videos using 3D Gaussian Splatting. 360 cameras capture a wide set of viewpoints, allowing for comprehensive scene capture under a simple straightforward drone trajectory. To scale our method to large scenes, we devise a divide-and-conquer strategy to automatically split the scene into smaller blocks that can be reconstructed individually and in parallel. We also propose a coarse-to-fine alignment strategy to seamlessly match these blocks together to compose the entire scene. Our experiments demonstrate marked improvement in both reconstruction quality, i.e. PSNR and SSIM, and computation time compared to prior approaches.

对于大规模、复杂的多层室内场景，场景重建和新视角合成是一项充满挑战且耗时的任务。以往的方法使用无人机进行数据捕捉和辐射场进行场景重建，但面临一些挑战。首先，为了使用无人机的前置摄像头捕捉多样化的视角，一些方法采用不稳定的Z字形飞行模式，这不仅影响无人机的操作，还会导致捕捉数据时出现运动模糊。其次，大多数辐射场方法难以轻松扩展至任意大量的图像。本论文提出了一种高效且可扩展的管线，利用3D高斯散射从无人机捕捉的360度视频中进行室内新视角合成。360度相机捕捉到了广泛的视角范围，允许在简单直线飞行轨迹下全面捕捉场景。为了使我们的方法能够扩展到大场景，我们设计了一种分而治之的策略，自动将场景划分为可独立并行重建的小块。我们还提出了一种由粗到精的对齐策略，能够无缝匹配这些小块，从而构建整个场景。实验表明，与以往方法相比，我们的方法在重建质量（如PSNR和SSIM）和计算时间上都有显著提升。
