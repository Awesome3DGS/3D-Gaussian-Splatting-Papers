### DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos

Existing VLMs can track in-the-wild 2D video objects while current generative models provide powerful visual priors for synthesizing novel views for the highly under-constrained 2D-to-3D object lifting. Building upon this exciting progress, we present DreamScene4D, the first approach that can generate three-dimensional dynamic scenes of multiple objects from monocular in-the-wild videos with large object motion across occlusions and novel viewpoints. Our key insight is to design a "decompose-then-recompose" scheme to factorize both the whole video scene and each object's 3D motion. We first decompose the video scene by using open-vocabulary mask trackers and an adapted image diffusion model to segment, track, and amodally complete the objects and background in the video. Each object track is mapped to a set of 3D Gaussians that deform and move in space and time. We also factorize the observed motion into multiple components to handle fast motion. The camera motion can be inferred by re-rendering the background to match the video frames. For the object motion, we first model the object-centric deformation of the objects by leveraging rendering losses and multi-view generative priors in an object-centric frame, then optimize object-centric to world-frame transformations by comparing the rendered outputs against the perceived pixel and optical flow. Finally, we recompose the background and objects and optimize for relative object scales using monocular depth prediction guidance. We show extensive results on the challenging DAVIS, Kubric, and self-captured videos, detail some limitations, and provide future directions. Besides 4D scene generation, our results show that DreamScene4D enables accurate 2D point motion tracking by projecting the inferred 3D trajectories to 2D, while never explicitly trained to do so.

现有的视觉语言模型（VLMs）能够跟踪野外环境中二维视频中的对象，而当前的生成模型为高度不受约束的二维至三维物体提升提供了强大的视觉先验，以合成新颖视角的视图。在这一令人兴奋的进展基础上，我们介绍了DreamScene4D，这是第一种能从单目野外视频中生成多个对象的三维动态场景的方法，该方法能处理大范围物体运动、遮挡和新颖视点。我们的关键见解是设计一个“分解再重组”的方案，以分解整个视频场景和每个对象的三维运动。我们首先使用开放词汇的遮罩跟踪器和适应性图像扩散模型分解视频场景，以分割、跟踪和完全表示视频中的对象和背景。每个对象的轨迹被映射到一组在空间和时间中变形和移动的3D高斯。我们还将观察到的运动分解为多个组件以处理快速运动。相机运动可以通过重新渲染背景以匹配视频帧来推断。对于对象运动，我们首先利用渲染损失和多视图生成先验，在对象中心框架中建模对象的变形，然后通过比较渲染输出与感知的像素和光流，优化对象中心到世界框架的转换。最后，我们重新组合背景和对象，并利用单目深度预测指导优化相对对象比例。我们在挑战性的DAVIS、Kubric和自采视频上展示了广泛的结果，详细说明了一些限制，并提供了未来的方向。除了4D场景生成，我们的结果还显示，DreamScene4D能通过将推断出的3D轨迹投影到2D来实现精确的2D点运动跟踪，尽管从未明确训练过这一功能。
