### Event3DGS: Event-based 3D Gaussian Splatting for Fast Egomotion

The recent emergence of 3D Gaussian splatting (3DGS) leverages the advantage of explicit point-based representations, which significantly improves the rendering speed and quality of novel-view synthesis. However, 3D radiance field rendering in environments with high-dynamic motion or challenging illumination condition remains problematic in real-world robotic tasks. The reason is that fast egomotion is prevalent real-world robotic tasks, which induces motion blur, leading to inaccuracies and artifacts in the reconstructed structure. To alleviate this problem, we propose Event3DGS, the first method that learns Gaussian Splatting solely from raw event streams. By exploiting the high temporal resolution of event cameras and explicit point-based representation, Event3DGS can reconstruct high-fidelity 3D structures solely from the event streams under fast egomotion. Our sparsity-aware sampling and progressive training approaches allow for better reconstruction quality and consistency. To further enhance the fidelity of appearance, we explicitly incorporate the motion blur formation process into a differentiable rasterizer, which is used with a limited set of blurred RGB images to refine the appearance. Extensive experiments on multiple datasets validate the superior rendering quality of Event3DGS compared with existing approaches, with over 95% lower training time and faster rendering speed in orders of magnitude.

最近3D高斯涂抹（3DGS）的出现利用了显式点基表示的优势，显著提高了新视角合成的渲染速度和质量。然而，在具有高动态运动或具挑战性照明条件的环境中，3D辐射场渲染在现实世界的机器人任务中仍然存在问题。问题的原因是，在现实世界的机器人任务中，快速自我运动是普遍现象，这会导致运动模糊，从而导致重建结构中的不准确和伪像。为了缓解这个问题，我们提出了Event3DGS，这是第一个仅从原始事件流中学习高斯涂抹的方法。通过利用事件相机的高时间分辨率和显式点基表示，Event3DGS可以仅从快速自我运动下的事件流中重建高保真3D结构。我们的稀疏感知采样和渐进式训练方法允许更好的重建质量和一致性。为了进一步提高外观的保真度，我们将运动模糊形成过程显式地纳入到一个可微光栅化器中，该光栅化器与有限组模糊的RGB图像一起使用，以细化外观。在多个数据集上进行的广泛实验验证了Event3DGS与现有方法相比的优越渲染质量，训练时间减少了95％以上，渲染速度也有数量级的提升。
