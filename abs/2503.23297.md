### ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning

Open-vocabulary 3D visual grounding and reasoning aim to localize objects in a scene based on implicit language descriptions, even when they are occluded. This ability is crucial for tasks such as vision-language navigation and autonomous robotics. However, current methods struggle because they rely heavily on fine-tuning with 3D annotations and mask proposals, which limits their ability to handle diverse semantics and common knowledge required for effective reasoning. In this work, we propose ReasonGrounder, an LVLM-guided framework that uses hierarchical 3D feature Gaussian fields for adaptive grouping based on physical scale, enabling open-vocabulary 3D grounding and reasoning. ReasonGrounder interprets implicit instructions using large vision-language models (LVLM) and localizes occluded objects through 3D Gaussian splatting. By incorporating 2D segmentation masks from the SAM and multi-view CLIP embeddings, ReasonGrounder selects Gaussian groups based on object scale, enabling accurate localization through both explicit and implicit language understanding, even in novel, occluded views. We also contribute ReasoningGD, a new dataset containing over 10K scenes and 2 million annotations for evaluating open-vocabulary 3D grounding and amodal perception under occlusion. Experiments show that ReasonGrounder significantly improves 3D grounding accuracy in real-world scenarios.

开放词汇的三维视觉指引与推理（Open-vocabulary 3D Visual Grounding and Reasoning）旨在根据隐式语言描述在场景中定位物体，即使物体被遮挡也能准确识别。这种能力对于视觉-语言导航（Vision-Language Navigation）和自主机器人等任务至关重要。然而，当前方法普遍存在困难，因为它们过度依赖于带有三维标注和掩码提议（mask proposals）的微调过程，限制了对丰富语义和常识推理能力的支持。
为此，本文提出了ReasonGrounder，一种由大规模视觉-语言模型（Large Vision-Language Model, LVLM）引导的框架，基于**分层三维特征高斯场（hierarchical 3D feature Gaussian fields）**按物理尺度进行自适应分组，实现开放词汇的三维指引与推理。ReasonGrounder利用LVLM解析隐式指令，并通过三维高斯泼洒（3D Gaussian Splatting）定位被遮挡的物体。通过结合来自SAM的二维分割掩码和多视角CLIP嵌入，ReasonGrounder能够根据物体尺度选择高斯组，从而在新颖且被遮挡的视角下，依然实现基于显式与隐式语言理解的精准定位。
此外，本文贡献了ReasoningGD数据集，包含超过1万组场景和200万条标注，用于评估遮挡条件下的开放词汇三维指引与非显式感知（amodal perception）。实验结果表明，ReasonGrounder在真实场景中显著提升了三维指引的准确性。
