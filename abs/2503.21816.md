### EVPGS: Enhanced View Prior Guidance for Splatting-based Extrapolated View Synthesis

Gaussian Splatting (GS)-based methods rely on sufficient training view coverage and perform synthesis on interpolated views. In this work, we tackle the more challenging and underexplored Extrapolated View Synthesis (EVS) task. Here we enable GS-based models trained with limited view coverage to generalize well to extrapolated views. To achieve our goal, we propose a view augmentation framework to guide training through a coarse-to-fine process. At the coarse stage, we reduce rendering artifacts due to insufficient view coverage by introducing a regularization strategy at both appearance and geometry levels. At the fine stage, we generate reliable view priors to provide further training guidance. To this end, we incorporate an occlusion awareness into the view prior generation process, and refine the view priors with the aid of coarse stage output. We call our framework Enhanced View Prior Guidance for Splatting (EVPGS). To comprehensively evaluate EVPGS on the EVS task, we collect a real-world dataset called Merchandise3D dedicated to the EVS scenario. Experiments on three datasets including both real and synthetic demonstrate EVPGS achieves state-of-the-art performance, while improving synthesis quality at extrapolated views for GS-based methods both qualitatively and quantitatively.


基于高斯投影（Gaussian Splatting, GS）的方法依赖于充足的训练视角覆盖，并通常在插值视角上进行图像合成。在本研究中，我们聚焦于一个更具挑战性且尚未被充分探索的任务：外推视角合成（Extrapolated View Synthesis, EVS）。我们的目标是使基于 GS 的模型即便在训练视角覆盖受限的情况下，也能很好地泛化到外推视角。
为此，我们提出了一种视角增强训练框架，通过粗到细的过程引导模型学习。在粗阶段，我们在外观和几何两个层面引入正则化策略，以减少由视角覆盖不足带来的渲染伪影；在精阶段，我们生成可靠的视角先验以进一步提供训练指导。为提升视角先验的可靠性，我们在生成过程中引入了遮挡感知机制，并利用粗阶段的输出对视角先验进行细化。
我们将该框架命名为 EVPGS（Enhanced View Prior Guidance for Splatting）。为全面评估 EVPGS 在 EVS 任务中的表现，我们构建了一个专用于 EVS 场景的真实世界数据集 Merchandise3D。在三个数据集（包括真实和合成数据）上的实验表明，EVPGS 在外推视角合成任务中达到了当前最先进性能，在定性与定量指标上均显著提升了基于 GS 方法的合成质量。
