### Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation

The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.

虚拟环境的生成能力对于从游戏到机器人、自主驾驶和工业智能等物理人工智能领域的应用至关重要。当前基于学习的三维重建方法依赖于真实世界多视角数据的获取，而这类数据往往难以获得。近年来的视频扩散模型展现出了强大的想象能力，但其二维特性限制了其在需要机器人导航与交互的仿真环境中的应用。为此，本文提出了一种自蒸馏框架，旨在将视频扩散模型中隐含的三维知识蒸馏为显式的三维高斯溅射（3D Gaussian Splatting，3DGS）表示，从而无需依赖多视角训练数据。具体而言，我们在常规的 RGB 解码器基础上增加了一个 3DGS 解码器，并通过 RGB 解码器的输出对其进行监督。在该框架下，3DGS 解码器可完全基于视频扩散模型生成的合成数据进行训练。在推理阶段，我们的模型能够从文本提示或单张图像合成三维场景，并实现实时渲染。此外，该框架还可扩展至从单目视频生成动态三维场景。实验结果表明，我们的框架在静态与动态三维场景生成任务中均达到了当前最先进的性能。
