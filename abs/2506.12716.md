### Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors

We tackle the challenge of generating dynamic 4D scenes from monocular, multi-object videos with heavy occlusions, and introduce GenMOJO, a novel approach that integrates rendering-based deformable 3D Gaussian optimization with generative priors for view synthesis. While existing models perform well on novel view synthesis for isolated objects, they struggle to generalize to complex, cluttered scenes. To address this, GenMOJO decomposes the scene into individual objects, optimizing a differentiable set of deformable Gaussians per object. This object-wise decomposition allows leveraging object-centric diffusion models to infer unobserved regions in novel viewpoints. It performs joint Gaussian splatting to render the full scene, capturing cross-object occlusions, and enabling occlusion-aware supervision. To bridge the gap between object-centric priors and the global frame-centric coordinate system of videos, GenMOJO uses differentiable transformations that align generative and rendering constraints within a unified framework. The resulting model generates 4D object reconstructions over space and time, and produces accurate 2D and 3D point tracks from monocular input. Quantitative evaluations and perceptual human studies confirm that GenMOJO generates more realistic novel views of scenes and produces more accurate point tracks compared to existing approaches.

我们针对从存在严重遮挡的单目多目标视频中生成动态四维场景的挑战，提出了 **GenMOJO**，一种将基于渲染的可变形三维高斯优化与生成式先验相结合的新方法，用于新视角合成。现有模型在孤立物体的新视角合成上表现良好，但难以推广到复杂、杂乱的场景。为解决这一问题，**GenMOJO** 将场景分解为独立的物体，并为每个物体优化一组可微的可变形高斯。这种按物体分解的方式能够利用基于物体的扩散模型，在新视角下推断未观测到的区域。该方法执行联合高斯溅射以渲染完整场景，捕捉跨物体遮挡关系，并实现遮挡感知的监督。为弥合物体中心先验与视频全局帧中心坐标系之间的差距，**GenMOJO** 使用可微变换将生成与渲染约束对齐到统一框架中。最终，该模型能够在时空中生成四维物体重建，并从单目输入生成精确的二维和三维点跟踪。定量评估与人类感知研究均表明，**GenMOJO** 在生成场景新视角和生成更精确的点跟踪方面均优于现有方法。
