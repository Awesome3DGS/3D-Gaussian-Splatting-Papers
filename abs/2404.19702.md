### GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting

We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks.

我们提出了GS-LRM，一种可扩展的大型重建模型，能够从2-4张摆好姿势的稀疏图像中，在单个A100 GPU上仅用0.23秒预测高质量的3D高斯原始体。我们的模型采用了非常简单的基于变压器的架构；我们将输入的摆好姿势的图像进行打块处理，将多视图图像令牌串联起来，通过一系列变压器块传递，并直接从这些令牌解码出最终的每像素高斯参数，用于可微分渲染。与只能重建对象的以往LRM相比，通过预测每像素高斯，GS-LRM自然地处理具有大尺度和复杂性变化的场景。我们展示了我们的模型可以通过分别在Objaverse和RealEstate10K上训练，用于物体和场景捕获。在这两种情况下，模型均大幅超越了最先进的基准。我们还展示了该模型在下游3D生成任务中的应用。
