### RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting

Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive and expert-driven process, often resulting in inconsistent reconstructions and suboptimal results. We propose RLGS, a plug-and-play reinforcement learning framework for adaptive hyperparameter tuning in 3DGS through lightweight policy modules, dynamically adjusting critical hyperparameters such as learning rates and densification thresholds. The framework is model-agnostic and seamlessly integrates into existing 3DGS pipelines without architectural modifications. We demonstrate its generalization ability across multiple state-of-the-art 3DGS variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness across diverse datasets. RLGS consistently enhances rendering quality. For example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT) dataset, under a fixed Gaussian budget, and continues to yield gains even when baseline performance saturates. Our results suggest that RLGS provides an effective and general solution for automating hyperparameter tuning in 3DGS training, bridging a gap in applying reinforcement learning to 3DGS.

在三维高斯溅射（3D Gaussian Splatting，3DGS）中，超参数调优是一个耗时且依赖专家经验的过程，往往导致重建结果不一致和效果次优。我们提出了 RLGS，这是一种即插即用的强化学习框架，通过轻量级策略模块在 3DGS 中自适应调整超参数，如学习率和加密阈值。该框架与具体模型无关，可无缝集成到现有的 3DGS 管线中，无需修改架构。我们验证了其在多种最先进的 3DGS 变体（包括 Taming-3DGS 和 3DGS-MCMC）中的泛化能力，并在多种数据集上验证了其鲁棒性。RLGS 能够持续提升渲染质量，例如，在固定高斯预算下，它使 Taming-3DGS 在 Tanks and Temple（TNT）数据集上的 PSNR 提高了 0.7 dB，并且即使在基线性能饱和时仍能带来收益。结果表明，RLGS 为 3DGS 训练中的超参数调优提供了一种高效且通用的自动化解决方案，填补了强化学习在 3DGS 应用中的空白。
