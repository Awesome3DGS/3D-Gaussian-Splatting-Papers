### ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting

3D occupancy prediction is critical for comprehensive scene understanding in vision-centric autonomous driving. Recent advances have explored utilizing 3D semantic Gaussians to model occupancy while reducing computational overhead, but they remain constrained by insufficient multi-view spatial interaction and limited multi-frame temporal consistency. To overcome these issues, in this paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework to enhance both spatial and temporal modeling in existing Gaussian-based pipelines. Specifically, we develop a guidance-informed spatial aggregation strategy within a dual-mode attention mechanism to strengthen spatial interaction in Gaussian representations. Furthermore, we introduce a geometry-aware temporal fusion scheme that effectively leverages historical context to improve temporal continuity in scene completion. Extensive experiments on the large-scale nuScenes occupancy prediction benchmark showcase that our proposed approach not only achieves state-of-the-art performance but also delivers markedly better temporal consistency compared to existing Gaussian-based methods.

三维占据预测对于以视觉为核心的自动驾驶场景理解至关重要。近年来的研究尝试利用三维语义高斯来建模占据信息，从而降低计算开销，但仍受到多视角空间交互不足和多帧时序一致性有限的制约。为了解决这些问题，本文提出了一种新的时空高斯溅射（Spatial-Temporal Gaussian Splatting, ST-GS）框架，以增强现有基于高斯管线的空间与时间建模能力。具体而言，我们在双模注意力机制中设计了一种基于引导的空间聚合策略，以强化高斯表示的空间交互。此外，我们提出了一种几何感知的时序融合方案，能够有效利用历史上下文信息以提升场景补全的时间连续性。在大规模 nuScenes 占据预测基准上的大量实验表明，我们的方法不仅达到了当前最优性能，还在时序一致性方面显著优于现有的基于高斯的方法。
