### GaussianLens: Localized High-Resolution Reconstruction via On-Demand Gaussian Densification

We perceive our surroundings with an active focus, paying more attention to regions of interest, such as the shelf labels in a grocery store. When it comes to scene reconstruction, this human perception trait calls for spatially varying degrees of detail ready for closer inspection in critical regions, preferably reconstructed on demand. While recent works in 3D Gaussian Splatting (3DGS) achieve fast, generalizable reconstruction from sparse views, their uniform resolution output leads to high computational costs unscalable to high-resolution training. As a result, they cannot leverage available images at their original high resolution to reconstruct details. Per-scene optimization methods reconstruct finer details with adaptive density control, yet require dense observations and lengthy offline optimization. To bridge the gap between the prohibitive cost of high-resolution holistic reconstructions and the user needs for localized fine details, we propose the problem of localized high-resolution reconstruction via on-demand Gaussian densification. Given a low-resolution 3DGS reconstruction, the goal is to learn a generalizable network that densifies the initial 3DGS to capture fine details in a user-specified local region of interest (RoI), based on sparse high-resolution observations of the RoI. This formulation avoids the high cost and redundancy of uniformly high-resolution reconstructions and fully leverages high-resolution captures in critical regions. We propose GaussianLens, a feed-forward densification framework that fuses multi-modal information from the initial 3DGS and multi-view images. We further design a pixel-guided densification mechanism that effectively captures details under large resolution increases. Experiments demonstrate our method's superior performance in local fine detail reconstruction and strong scalability to images of up to 1024×1024 resolution.

我们以主动聚焦的方式感知周围环境，对感兴趣区域（例如超市货架上的标签）给予更多注意。在场景重建中，这种人类感知特性启示我们应针对关键区域提供空间分辨率可变、可按需精细重建的能力。尽管近期的三维高斯溅射（3D Gaussian Splatting, 3DGS）方法能够从稀疏视角实现快速且具泛化能力的重建，但其输出为均匀分辨率，导致计算成本高昂，难以扩展至高分辨率训练。因此，它们无法充分利用原始高分辨率图像来重建细节。基于逐场景优化的传统方法虽然通过自适应密度控制能够恢复更精细的细节，但依赖密集观测且训练耗时长。为弥合高分辨率整体重建的高昂代价与用户在局部精细区域需求之间的差距，我们提出了 **基于按需高斯密化的局部高分辨率重建问题**。在给定低分辨率 3DGS 重建结果的情况下，我们的目标是学习一个具备泛化能力的网络，利用稀疏的高分辨率观测，对用户指定的兴趣区域（Region of Interest, RoI）进行密化，从而捕获细节结构。该方案避免了全局高分辨率重建的高成本与冗余，同时充分利用关键区域的高分辨率图像。为此，我们提出了 **GaussianLens**，一个基于前馈架构的密化框架，可融合初始 3DGS 与多视图图像的多模态信息。此外，我们设计了 **像素引导的密化机制（pixel-guided densification）**，能够在大幅提升分辨率时有效捕获细节。实验结果表明，GaussianLens 在局部细节重建方面表现优异，并在高达 1024×1024 分辨率的图像上展现出强大的可扩展性。
