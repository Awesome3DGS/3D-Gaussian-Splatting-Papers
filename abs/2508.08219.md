### SAGOnline: Segment Any Gaussians Online

3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Current methods suffer from prohibitive computational costs, limited 3D spatial reasoning, and an inability to track multiple objects simultaneously. We present Segment Any Gaussians Online (SAGOnline), a lightweight and zero-shot framework for real-time 3D segmentation in Gaussian scenes that addresses these limitations through two key innovations: (1) a decoupled strategy that integrates video foundation models (e.g., SAM2) for view-consistent 2D mask propagation across synthesized views; and (2) a GPU-accelerated 3D mask generation and Gaussian-level instance labeling algorithm that assigns unique identifiers to 3D primitives, enabling lossless multi-object tracking and segmentation across views. SAGOnline achieves state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU) benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times in inference speed (27 ms/frame). Qualitative results demonstrate robust multi-object segmentation and tracking in complex scenes. Our contributions include: (i) a lightweight and zero-shot framework for 3D segmentation in Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling simultaneous segmentation and tracking, and (iii) the effective adaptation of 2D video foundation models to the 3D domain. This work allows real-time rendering and 3D scene understanding, paving the way for practical AR/VR and robotic applications.

三维高斯溅射（3DGS）已成为一种强大的显式三维场景表示范式，但实现高效且一致的三维分割仍然面临挑战。现有方法存在计算开销过大、三维空间推理受限以及无法同时跟踪多个物体的问题。我们提出了 **SAGOnline（Segment Any Gaussians Online）**，一个轻量级的零样本实时三维分割框架，通过两项关键创新解决了上述局限：（1）解耦策略：集成视频基础模型（如 SAM2），在合成视角间进行一致的二维掩膜传播；（2）GPU 加速的三维掩膜生成与高斯级别实例标注算法：为三维基元分配唯一标识符，实现跨视角的无损多目标跟踪与分割。SAGOnline 在 NVOS（92.7% mIoU）和 Spin-NeRF（95.2% mIoU）基准上取得了最新最优表现，在推理速度上比 Feature3DGS、OmniSeg3D-gs 和 SA3D 快 15 至 1500 倍（27 ms/帧）。定性结果显示其在复杂场景中具备鲁棒的多目标分割与跟踪能力。我们的贡献包括：（i）提出了一个轻量级零样本三维高斯场景分割框架；（ii）通过显式标注高斯基元，实现分割与跟踪的同时进行；（iii）将二维视频基础模型有效迁移至三维领域。本研究实现了实时渲染与三维场景理解，为 AR/VR 与机器人应用的落地铺平了道路。
