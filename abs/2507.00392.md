### Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space

Feature matching plays a fundamental role in many computer vision tasks, yet existing methods heavily rely on scarce and clean multi-view image collections, which constrains their generalization to diverse and challenging scenarios. Moreover, conventional feature encoders are typically trained on single-view 2D images, limiting their capacity to capture 3D-aware correspondences. In this paper, we propose a novel two-stage framework that lifts 2D images to 3D space, named as **Lift to Match (L2M)**, taking full advantage of large-scale and diverse single-view images. To be specific, in the first stage, we learn a 3D-aware feature encoder using a combination of multi-view image synthesis and 3D feature Gaussian representation, which injects 3D geometry knowledge into the encoder. In the second stage, a novel-view rendering strategy, combined with large-scale synthetic data generation from single-view images, is employed to learn a feature decoder for robust feature matching, thus achieving generalization across diverse domains. Extensive experiments demonstrate that our method achieves superior generalization across zero-shot evaluation benchmarks, highlighting the effectiveness of the proposed framework for robust feature matching.

特征匹配在许多计算机视觉任务中发挥着基础性作用，但现有方法严重依赖稀缺且干净的多视图图像集合，这限制了其在多样化和具有挑战性的场景中的泛化能力。此外，传统的特征编码器通常在单视图二维图像上训练，限制了其捕捉具备三维感知的对应关系的能力。本文提出了一种将二维图像提升到三维空间的新型两阶段框架，称为 **Lift to Match (L2M)**，充分利用了大规模、多样化的单视图图像。具体而言，在第一阶段，我们通过结合多视图图像合成与三维特征高斯表示，学习一个具备三维感知的特征编码器，从而将三维几何知识注入编码器。在第二阶段，我们采用新视角渲染策略，并结合从单视图图像生成的大规模合成数据，来训练特征解码器以实现稳健的特征匹配，从而在不同领域中实现泛化。大量实验表明，我们的方法在零样本评估基准上表现出优越的泛化能力，凸显了所提框架在稳健特征匹配中的有效性。
