### PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control

Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment.

基于音频驱动的说话人头像生成在虚拟现实、数字人和影视制作等领域具有重要应用价值。尽管基于 NeRF 的方法能够实现高保真重建，但其渲染效率较低，音视频同步效果不理想。本文提出了一种基于三维高斯溅射（3D Gaussian Splatting, 3DGS）的实时音频驱动说话人合成框架——**PGSTalker**。为提升渲染性能，我们提出了一种**像素感知密度控制策略**，可自适应地分配点的密度，在动态面部区域增强细节，同时减少其他区域的冗余。此外，我们设计了一个轻量级的**多模态门控融合模块（Multimodal Gated Fusion Module）**，用于有效融合音频与空间特征，从而提升高斯形变预测的精度。在多个公开数据集上的广泛实验表明，PGSTalker 在渲染质量、唇形同步精度及推理速度方面均优于现有的基于 NeRF 和 3DGS 的方法。该方法展现出较强的泛化能力与实际部署潜力。
