### DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for Refocusing,Defocus Rendering and Blur Removal

3D Gaussian Splatting-based techniques have recently advanced 3D scene reconstruction and novel view synthesis, achieving high-quality real-time rendering. However, these approaches are inherently limited by the underlying pinhole camera assumption in modeling the images and hence only work for All-in-Focus (AiF) sharp image inputs. This severely affects their applicability in real-world scenarios where images often exhibit defocus blur due to the limited depth-of-field (DOF) of imaging devices. Additionally, existing 3D Gaussian Splatting (3DGS) methods also do not support rendering of DOF effects.
To address these challenges, we introduce DOF-GS that allows for rendering adjustable DOF effects, removing defocus blur as well as refocusing of 3D scenes, all from multi-view images degraded by defocus blur. To this end, we re-imagine the traditional Gaussian Splatting pipeline by employing a finite aperture camera model coupled with explicit, differentiable defocus rendering guided by the Circle-of-Confusion (CoC). The proposed framework provides for dynamic adjustment of DOF effects by changing the aperture and focal distance of the underlying camera model on-demand. It also enables rendering varying DOF effects of 3D scenes post-optimization, and generating AiF images from defocused training images. Furthermore, we devise a joint optimization strategy to further enhance details in the reconstructed scenes by jointly optimizing rendered defocused and AiF images. Our experimental results indicate that DOF-GS produces high-quality sharp all-in-focus renderings conditioned on inputs compromised by defocus blur, with the training process incurring only a modest increase in GPU memory consumption. We further demonstrate the applications of the proposed method for adjustable defocus rendering and refocusing of the 3D scene from input images degraded by defocus blur.

基于三维高斯喷溅的技术最近在三维场景重建和新视角合成方面取得了进展，实现了高质量的实时渲染。然而，这些方法固有地受到基础针孔相机模型的限制，因此只适用于所有焦点（AiF）锐利图像输入。这严重影响了它们在实际应用场景中的适用性，因为成像设备的有限景深（DOF）常常导致图像出现散焦模糊。此外，现有的三维高斯喷溅（3DGS）方法也不支持渲染景深效果。
为了解决这些挑战，我们引入了DOF-GS，它允许渲染可调节的景深效果，消除散焦模糊以及从多视角图像重聚焦三维场景，这些图像都因散焦模糊而质量下降。为此，我们重新设想了传统的高斯喷溅流程，采用了具有明确的、可微的散焦渲染引导的有限光圈相机模型，该模型以圆锥混淆（CoC）为指导。提出的框架通过按需改变相机模型的光圈和焦距，为景深效果的动态调整提供了支持。它还允许在优化后渲染三维场景的不同景深效果，并从散焦训练图像生成AiF图像。此外，我们设计了一种联合优化策略，通过联合优化渲染的散焦和AiF图像进一步提升重建场景中的细节。我们的实验结果表明，DOF-GS在输入受散焦模糊影响的条件下，能够产生高质量的锐利全焦点渲染，训练过程中GPU内存消耗只增加了一点。我们进一步展示了该方法在可调散焦渲染和从受散焦模糊影响的输入图像中重聚焦三维场景的应用。
