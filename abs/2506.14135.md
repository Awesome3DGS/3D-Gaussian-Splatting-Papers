### GAF: Gaussian Action Field as a Dynamic World Model for Robotic Manipulation

Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a Vision-to-4D-to-Action (V-4D-A) framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods.

精确的动作推理对于基于视觉的机器人操作至关重要。现有方法通常遵循“视觉到动作”（V-A）范式，即直接从视觉输入预测动作，或“视觉到三维到动作”（V-3D-A）范式，即利用中间的三维表示。然而，这些方法在复杂且动态的操作场景中往往难以避免动作预测不准确的问题。本文提出了一种**视觉到四维到动作（V-4D-A）**框架，通过**高斯动作场（Gaussian Action Field, GAF）**实现基于具备运动感知能力的四维表示的直接动作推理。GAF 在三维高斯溅射（3DGS）的基础上引入可学习的运动属性，从而能够同时建模动态场景与操作动作。为了学习时变场景几何与动作感知的机器人运动，GAF 支持三类关键查询：当前场景重建、未来帧预测，以及通过机器人运动估计初始动作。此外，GAF 生成的高质量当前帧与未来帧可通过**GAF 引导的扩散模型**进一步优化操作动作。大量实验表明，GAF 在重建质量方面相比当前最先进方法实现了 **+11.5385 dB** 的 PSNR 提升与 **-0.5574** 的 LPIPS 降低，同时在机器人操作任务的平均成功率上提升了 **10.33%**。
