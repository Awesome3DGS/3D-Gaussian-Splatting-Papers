### 2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting

Real-time rendering of high-fidelity and animatable avatars from monocular videos remains a challenging problem in computer vision and graphics. Over the past few years, the Neural Radiance Field (NeRF) has made significant progress in rendering quality but behaves poorly in run-time performance due to the low efficiency of volumetric rendering. Recently, methods based on 3D Gaussian Splatting (3DGS) have shown great potential in fast training and real-time rendering. However, they still suffer from artifacts caused by inaccurate geometry. To address these problems, we propose 2DGS-Avatar, a novel approach based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars with high-fidelity and fast training performance. Given monocular RGB videos as input, our method generates an avatar that can be driven by poses and rendered in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the advantages of fast training and rendering while also capturing detailed, dynamic, and photo-realistic appearances. We conduct abundant experiments on popular datasets such as AvatarRex and THuman4.0, demonstrating impressive performance in both qualitative and quantitative metrics.

从单目视频实时渲染高保真且可动画的虚拟人（avatar）仍然是计算机视觉和计算机图形学中的一项挑战性任务。近年来，Neural Radiance Field (NeRF) 在渲染质量方面取得了显著进展，但由于体渲染效率低，其实时性能较差。近期，基于3D Gaussian Splatting (3DGS) 的方法在快速训练和实时渲染方面展现出巨大潜力，但仍然受到几何不准确导致的伪影问题影响。
为了解决这些问题，我们提出 2DGS-Avatar，一种基于2D Gaussian Splatting (2DGS) 的新方法，可用于建模高保真、可动画的穿着服饰虚拟人，同时具备高效训练性能。在输入单目 RGB 视频的情况下，我们的方法能够生成可由姿态驱动、实时渲染的虚拟人。
与基于 3DGS 的方法相比，2DGS-Avatar 保持了快速训练和实时渲染的优势，同时能够捕捉更精细、动态且具备真实感的外观。我们在 AvatarRex 和 THuman4.0 等主流数据集上进行了大量实验，结果表明，在定性和定量指标上均取得了卓越性能。
