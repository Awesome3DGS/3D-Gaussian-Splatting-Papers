### IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments

This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.

本工作提出了 IAAO，一个新颖的框架，用于为智能体构建显式三维模型，从而通过交互理解其环境中的可动物体。与以往依赖任务特定网络和对可移动部件进行假设的方法不同，IAAO 利用大规模基础模型，在三个阶段中估计交互可供性和部件关节结构。
首先，我们通过 3D Gaussian Splatting（3DGS） 构建每个物体状态的分层特征与标签场，方法是从多视角图像中提取遮罩特征并蒸馏获得视角一致的标签。接着，在三维高斯基元上进行物体级和部件级的查询，用于识别静态与可动部分，并估计全局变换与局部关节参数，同时获得可供性信息。最后，将来自不同状态的场景根据估计的变换进行融合与优化，从而支持稳健的基于可供性的交互与操控。
实验结果表明，我们的方法具有良好的有效性。
