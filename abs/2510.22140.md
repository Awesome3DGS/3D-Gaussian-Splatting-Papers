### STG-Avatar: Animatable Human Avatars via Spacetime Gaussian

Realistic animatable human avatars from monocular videos are crucial for advancing human-robot interaction and enhancing immersive virtual experiences. While recent research on 3DGS-based human avatars has made progress, it still struggles with accurately representing detailed features of non-rigid objects (e.g., clothing deformations) and dynamic regions (e.g., rapidly moving limbs). To address these challenges, we present STG-Avatar, a 3DGS-based framework for high-fidelity animatable human avatar reconstruction. Specifically, our framework introduces a rigid-nonrigid coupled deformation framework that synergistically integrates Spacetime Gaussians (STG) with linear blend skinning (LBS). In this hybrid design, LBS enables real-time skeletal control by driving global pose transformations, while STG complements it through spacetime adaptive optimization of 3D Gaussians. Furthermore, we employ optical flow to identify high-dynamic regions and guide the adaptive densification of 3D Gaussians in these regions. Experimental results demonstrate that our method consistently outperforms state-of-the-art baselines in both reconstruction quality and operational efficiency, achieving superior quantitative metrics while retaining real-time rendering capabilities.

从单目视频中构建真实可动画的人体头像对推动人机交互和增强沉浸式虚拟体验至关重要。尽管近期基于 3DGS 的人体头像研究取得了进展，但在精确表示非刚体对象（如衣物变形）和动态区域（如快速移动的四肢）方面仍面临挑战。为了解决这些问题，我们提出了 STG-Avatar，这是一个基于 3DGS 的高保真可动画人体头像重建框架。具体而言，我们的框架引入了刚体-非刚体耦合的变形机制，将时空高斯（Spacetime Gaussians, STG）与线性混合蒙皮（Linear Blend Skinning, LBS）协同融合。在这种混合设计中，LBS 通过驱动全局姿态变换实现实时骨骼控制，而 STG 则通过对三维高斯进行时空自适应优化予以补充。此外，我们还利用光流识别高动态区域，并引导这些区域中 3D 高斯的自适应加密。实验结果表明，我们的方法在重建质量和运行效率方面均优于现有最先进的方法，在保持实时渲染能力的同时实现了更优的定量指标。
