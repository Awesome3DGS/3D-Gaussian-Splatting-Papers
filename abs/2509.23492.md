### Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos

We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework for high-quality 4D reconstruction from casually captured monocular videos. While recent advances extend 3D Gaussian Splatting to dynamic scenes via various motion anchors, such as graph nodes or spline control points, they often rely on low-rank assumptions and fall short in modeling complex, region-specific deformations inherent to unconstrained dynamics. OriGS addresses this by introducing a hyperdimensional representation grounded in scene orientation. We first estimate a Global Orientation Field that propagates principal forward directions across space and time, serving as stable structural guidance for dynamic modeling. Built upon this, we propose Orientation-aware Hyper-Gaussian, a unified formulation that embeds time, space, geometry, and orientation into a coherent probabilistic state. This enables inferring region-specific deformation through principled conditioned slicing, adaptively capturing diverse local dynamics in alignment with global motion intent. Experiments demonstrate the superior reconstruction fidelity of OriGS over mainstream methods in challenging real-world dynamic scenes.

我们提出了 **Orientation-anchored Gaussian Splatting（OriGS）**，一种从随手拍摄的单目视频中实现高质量四维重建的新框架。尽管近期的研究已通过图节点或样条控制点等不同运动锚点，将三维高斯溅射（3D Gaussian Splatting）扩展至动态场景，但这些方法通常依赖低秩假设，难以准确建模真实复杂场景中区域特定的非刚性形变。OriGS 通过引入一种基于场景方向的高维表示（hyperdimensional representation）有效解决了这一问题。我们首先估计 **全局方向场（Global Orientation Field）**，以在时空中传播主要的前向方向，为动态建模提供稳定的结构性约束。基于此，我们提出了 **方向感知超高斯（Orientation-aware Hyper-Gaussian）**，一种统一的概率表征形式，将时间、空间、几何与方向信息共同嵌入到一致的状态空间中。该表征能够通过有原则的条件切片（conditioned slicing）推断区域特定的形变，从而自适应地捕获多样化的局部动态，并保持与全局运动意图的一致性。实验结果表明，OriGS 在复杂的真实动态场景中显著优于主流方法，展现出更高的重建保真度。
