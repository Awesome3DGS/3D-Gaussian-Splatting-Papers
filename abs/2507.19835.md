### SonicGauss: Position-Aware Physical Sound Synthesis for 3D Gaussian Representations

While 3D Gaussian representations (3DGS) have proven effective for modeling the geometry and appearance of objects, their potential for capturing other physical attributes-such as sound-remains largely unexplored. In this paper, we present a novel framework dubbed SonicGauss for synthesizing impact sounds from 3DGS representations by leveraging their inherent geometric and material properties. Specifically, we integrate a diffusion-based sound synthesis model with a PointTransformer-based feature extractor to infer material characteristics and spatial-acoustic correlations directly from Gaussian ellipsoids. Our approach supports spatially varying sound responses conditioned on impact locations and generalizes across a wide range of object categories. Experiments on the ObjectFolder dataset and real-world recordings demonstrate that our method produces realistic, position-aware auditory feedback. The results highlight the framework's robustness and generalization ability, offering a promising step toward bridging 3D visual representations and interactive sound synthesis.

虽然三维高斯表示（3D Gaussian Splatting, 3DGS）已被证明在建模物体几何与外观方面非常有效，但其在捕捉声音等其他物理属性上的潜力仍几乎未被探索。本文提出了一种名为 SonicGauss 的新框架，通过利用 3DGS 固有的几何与材质属性，从三维高斯表示中合成碰撞声音。具体而言，我们将基于扩散模型的声音合成方法与基于 PointTransformer 的特征提取器相结合，直接从高斯椭球中推断材质特性与空间-声学相关性。我们的方法支持基于碰撞位置的空间可变声音响应，并可在多种物体类别中实现泛化。在 ObjectFolder 数据集和真实录音上的实验表明，我们的方法能够生成逼真且具备位置感知能力的听觉反馈。结果展示了该框架的鲁棒性与泛化能力，为连接三维视觉表示与交互式声音合成迈出了有前景的一步。
