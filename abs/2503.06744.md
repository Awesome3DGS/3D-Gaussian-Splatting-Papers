### CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation Awareness for Autonomous Driving

Dynamic scene rendering opens new avenues in autonomous driving by enabling closed-loop simulations with photorealistic data, which is crucial for validating end-to-end algorithms. However, the complex and highly dynamic nature of traffic environments presents significant challenges in accurately rendering these scenes. In this paper, we introduce a novel 4D Gaussian Splatting (4DGS) approach, which incorporates context and temporal deformation awareness to improve dynamic scene rendering. Specifically, we employ a 2D semantic segmentation foundation model to self-supervise the 4D semantic features of Gaussians, ensuring meaningful contextual embedding. Simultaneously, we track the temporal deformation of each Gaussian across adjacent frames. By aggregating and encoding both semantic and temporal deformation features, each Gaussian is equipped with cues for potential deformation compensation within 3D space, facilitating a more precise representation of dynamic scenes. Experimental results show that our method improves 4DGS's ability to capture fine details in dynamic scene rendering for autonomous driving and outperforms other self-supervised methods in 4D reconstruction and novel view synthesis. Furthermore, CoDa-4DGS deforms semantic features with each Gaussian, enabling broader applications.

动态场景渲染在自动驾驶领域开辟了新的研究方向，使得基于**光真实数据（photorealistic data）的闭环仿真成为可能，对于端到端算法（end-to-end algorithms）**的验证至关重要。然而，交通环境的高度动态性和复杂性使得准确渲染此类场景极具挑战性。
在本文中，我们提出了一种新颖的 四维高斯散点（4D Gaussian Splatting, 4DGS） 方法，该方法结合上下文感知和时序变形感知来提升动态场景渲染的质量。具体而言，我们采用二维语义分割基础模型（2D semantic segmentation foundation model）对高斯点的 4D 语义特征进行自监督学习，从而确保语义特征的有效嵌入。同时，我们跟踪每个高斯点在相邻帧间的时序变形。通过聚合并编码语义信息与时序变形特征，每个高斯点都具备时序变形补偿能力，从而在三维空间中更精准地表示动态场景。
实验结果表明，我们的方法显著提升了 4DGS 在自动驾驶场景中的动态细节捕捉能力，在4D 重建和新视角合成（novel view synthesis）方面优于其他自监督方法。此外，CoDa-4DGS 使得语义特征能够随高斯点一起变形，从而拓展了更广泛的应用场景。
