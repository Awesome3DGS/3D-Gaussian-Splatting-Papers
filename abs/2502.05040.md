### GaussRender: Learning 3D Occupancy with Gaussian Rendering

Understanding the 3D geometry and semantics of driving scenes is critical for developing of safe autonomous vehicles. While 3D occupancy models are typically trained using voxel-based supervision with standard losses (e.g., cross-entropy, Lovasz, dice), these approaches treat voxel predictions independently, neglecting their spatial relationships. In this paper, we propose GaussRender, a plug-and-play 3D-to-2D reprojection loss that enhances voxel-based supervision. Our method projects 3D voxel representations into arbitrary 2D perspectives and leverages Gaussian splatting as an efficient, differentiable rendering proxy of voxels, introducing spatial dependencies across projected elements. This approach improves semantic and geometric consistency, handles occlusions more efficiently, and requires no architectural modifications. Extensive experiments on multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate consistent performance gains across various 3D occupancy models (TPVFormer, SurroundOcc, Symphonies), highlighting the robustness and versatility of our framework.

理解驾驶场景的3D几何和语义对于开发安全的自动驾驶车辆至关重要。虽然3D占据模型通常使用基于体素的监督与标准损失（例如交叉熵、Lovasz、dice）进行训练，但这些方法将体素预测视为独立的，忽视了它们之间的空间关系。本文提出了GaussRender，一种即插即用的3D到2D重投影损失，旨在增强基于体素的监督。我们的方法将3D体素表示投影到任意2D视角，并利用高斯溅射作为体素的高效、可微分渲染代理，引入了投影元素之间的空间依赖关系。这种方法改善了语义和几何一致性，更高效地处理了遮挡问题，并且不需要对架构进行修改。在多个基准测试（SurroundOcc-nuScenes、Occ3D-nuScenes、SSCBench-KITTI360）上的大量实验表明，我们的方法在各种3D占据模型（TPVFormer、SurroundOcc、Symphonies）中都表现出了稳定的性能提升，凸显了我们框架的鲁棒性和多样性。
