### Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes

Reconstructing 3D scenes from a single image is a fundamentally ill-posed task due to the severely under-constrained nature of the problem. Consequently, when the scene is rendered from novel camera views, existing single image to 3D reconstruction methods render incoherent and blurry views. This problem is exacerbated when the unseen regions are far away from the input camera. In this work, we address these inherent limitations in existing single image-to-3D scene feedforward networks. To alleviate the poor performance due to insufficient information beyond the input image's view, we leverage a strong generative prior in the form of a pre-trained latent video diffusion model, for iterative refinement of a coarse scene represented by optimizable Gaussian parameters. To ensure that the style and texture of the generated images align with that of the input image, we incorporate on-the-fly Fourier-style transfer between the generated images and the input image. Additionally, we design a semantic uncertainty quantification module that calculates the per-pixel entropy and yields uncertainty maps used to guide the refinement process from the most confident pixels while discarding the remaining highly uncertain ones. We conduct extensive experiments on real-world scene datasets, including in-domain RealEstate-10K and out-of-domain KITTI-v2, showing that our approach can provide more realistic and high-fidelity novel view synthesis results compared to existing state-of-the-art methods.

从单张图像重建 3D 场景是一个本质上不适定的问题，因为该问题的约束条件非常不足。因此，当从新的相机视角渲染场景时，现有的单图像到 3D 重建方法通常会渲染出不连贯且模糊的视图。当未见区域距离输入相机较远时，这一问题尤为严重。
在本研究中，我们解决了现有单图像到 3D 场景前馈网络中的这些固有限制。为了缓解由于输入图像视角之外信息不足所导致的较差性能，我们利用了强大的生成先验，即预训练的潜在视频扩散模型，用于对通过可优化高斯参数表示的粗糙场景进行迭代优化。为了确保生成图像的风格和纹理与输入图像一致，我们在生成图像和输入图像之间引入了实时傅里叶风格转换。
此外，我们设计了一个语义不确定性量化模块，该模块计算每个像素的熵并生成不确定性图，用于指导从最有信心的像素开始优化，同时丢弃剩余的不确定像素。
我们在多个真实场景数据集上进行了广泛实验，包括领域内的 RealEstate-10K 和领域外的 KITTI-v2，结果表明，我们的方法在新视角合成中提供了比现有最先进方法更为逼真和高保真的结果。
