### GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D Pretraining from Real-World Data

3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object's surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results.

将3D形状表示为点云，在将图像和语言描述对齐的多模态预训练方面取得了进展，这对于对象识别、分类和检索至关重要。然而，点云的离散表示丢失了对象表面形状信息，并在渲染结果与2D对应项之间创建了差距。为了解决这个问题，我们提出了GS-CLIP，这是首次尝试将3DGS（3D高斯喷溅）引入多模态预训练以增强3D表示。GS-CLIP利用预训练的视觉-语言模型，为大量真实世界的图像-文本对学习一个共有的视觉和文本空间，然后学习一个3D编码器以针对每个对象优化3DGS对齐。此外，提出了一种新颖的高斯感知融合方法，以提取和融合全局显式特征。作为一个针对语言-图像-3D预训练的通用框架，GS-CLIP对3D基础网络是不可知的。在具有挑战性的实验中，GS-CLIP显著提高了最先进的水平，超越了以前最好的结果。
