### HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting

3D human generation is an important problem with a wide range of applications in computer vision and graphics. Despite recent progress in generative AI such as diffusion models or rendering methods like Neural Radiance Fields or Gaussian Splatting, controlling the generation of accurate 3D humans from text prompts remains an open challenge. Current methods struggle with fine detail, accurate rendering of hands and faces, human realism, and controlability over appearance. The lack of diversity, realism, and annotation in human image data also remains a challenge, hindering the development of a foundational 3D human model. We present a weakly supervised pipeline that tries to address these challenges. In the first step, we generate a photorealistic human image dataset with controllable attributes such as appearance, race, gender, etc using a state-of-the-art image diffusion model. Next, we propose an efficient mapping approach from image features to 3D point clouds using a transformer-based architecture. Finally, we close the loop by training a point-cloud diffusion model that is conditioned on the same text prompts used to generate the original samples. We demonstrate orders-of-magnitude speed-ups in 3D human generation compared to the state-of-the-art approaches, along with significantly improved text-prompt alignment, realism, and rendering quality.

三维人体生成是计算机视觉与图形学中的一项重要课题，拥有广泛的应用场景。尽管近年来生成式 AI（如扩散模型）与渲染方法（如神经辐射场 Neural Radiance Fields 和高斯喷洒 Gaussian Splatting）取得了显著进展，但从文本提示精确生成三维人体仍是一项尚未解决的挑战。现有方法在细节刻画、手部与面部渲染、人体真实感以及外观控制能力方面仍存在显著不足。同时，人体图像数据在多样性、真实性及标注方面的缺失，也阻碍了通用三维人体基础模型的发展。
为应对上述难题，我们提出了一种弱监督生成管线。第一步，我们借助最先进的图像扩散模型，生成具有可控属性（如外观、种族、性别等）的写实人体图像数据集。接着，我们设计了一种基于 Transformer 的高效映射结构，将图像特征转换为三维点云。最后，我们引入一个以文本提示为条件的点云扩散模型，利用与图像生成阶段相同的文本描述进行训练，形成闭环。
该方法相比当前最先进的方案，在三维人体生成速度上实现了数量级的提升，并在文本一致性、图像真实感以及渲染质量方面取得了显著进展。
