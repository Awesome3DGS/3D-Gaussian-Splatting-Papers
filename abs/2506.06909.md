### Gaussian Mapping for Evolving Scenes

Mapping systems with novel view synthesis (NVS) capabilities are widely used in computer vision, with augmented reality, robotics, and autonomous driving applications. Most notably, 3D Gaussian Splatting-based systems show high NVS performance; however, many current approaches are limited to static scenes. While recent works have started addressing short-term dynamics (motion within the view of the camera), long-term dynamics (the scene evolving through changes out of view) remain less explored. To overcome this limitation, we introduce a dynamic scene adaptation mechanism that continuously updates the 3D representation to reflect the latest changes. In addition, since maintaining geometric and semantic consistency remains challenging due to stale observations disrupting the reconstruction process, we propose a novel keyframe management mechanism that discards outdated observations while preserving as much information as possible. We evaluate Gaussian Mapping for Evolving Scenes (GaME) on both synthetic and real-world datasets and find it to be more accurate than the state of the art.

具备新视角合成（Novel View Synthesis, NVS）能力的建图系统在计算机视觉中应用广泛，涵盖增强现实、机器人与自动驾驶等领域。其中，基于三维高斯泼溅（3D Gaussian Splatting, 3DGS）的系统在新视角合成任务中表现尤为出色；然而，目前多数方法仍局限于静态场景。
尽管近期研究开始关注短期动态（即视野内的物体运动），但对长期动态（即场景因视野外变化而演化）问题的探索仍相对不足。为突破这一限制，我们提出了一种动态场景自适应机制，可持续更新三维表示以反映场景的最新变化。
此外，由于旧观测可能干扰重建过程，导致几何与语义一致性难以维持，我们还设计了一种关键帧管理机制，用于在尽量保留信息的同时，剔除过时观测。
我们在合成数据集和真实场景数据集上对所提出的 Gaussian Mapping for Evolving Scenes（GaME） 方法进行了评估，实验结果表明，其精度显著优于现有最先进方法。
