### 4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos

Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.

在相机位姿未知的情况下，从单目视频中对动态场景进行新视角合成，仍然是计算机视觉与计算机图形学中的一项基础性挑战。尽管近年来诸如神经辐射场（Neural Radiance Fields，NeRF）和三维高斯溅射（3D Gaussian Splatting，3DGS）等三维表示方法在静态场景中取得了令人鼓舞的成果，但它们在处理动态内容时表现受限，并且通常依赖于预先计算的相机位姿。本文提出了 4D3R，一种无需相机位姿的动态神经渲染框架，通过两阶段策略对静态与动态成分进行解耦。我们的方法首先利用三维基础模型进行初始相机位姿与几何结构估计，随后引入运动感知的精细化优化。4D3R 包含两项关键技术创新：（1）一种运动感知的束调整（Motion-Aware Bundle Adjustment，MA-BA）模块，将基于 Transformer 的学习先验与 SAM2 相结合，实现对动态物体的鲁棒分割，从而支持更精确的相机位姿优化；（2）一种高效的运动感知高斯溅射（Motion-Aware Gaussian Splatting，MA-GS）表示方法，通过引入控制点、形变场 MLP 以及线性混合蒙皮来建模动态运动，在保持高质量重建的同时显著降低计算开销。在多个真实世界动态数据集上的大量实验表明，与现有最先进方法相比，我们的方法在 PSNR 指标上最高可提升 1.8dB，尤其在包含大尺度动态物体的挑战性场景中表现突出，同时相较于以往的动态场景表示方法，计算需求降低了约 5 倍。
