### CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes with Gaussian Representation

Localizing text descriptions in large-scale 3D scenes is inherently an ambiguous task. This nonetheless arises while describing general concepts, e.g. all traffic lights in a city. To facilitate reasoning based on such concepts, text localization in the form of distribution is required. In this paper, we generate the distribution of the camera poses conditioned upon the textual description.
To facilitate such generation, we propose a diffusion-based architecture that conditionally diffuses the noisy 6DoF camera poses to their plausible locations. The conditional signals are derived from the text descriptions, using the pre-trained text encoders. The connection between text descriptions and pose distribution is established through pretrained Vision-Language-Model, i.e. CLIP. Furthermore, we demonstrate that the candidate poses for the distribution can be further refined by rendering potential poses using 3D Gaussian splatting, guiding incorrectly posed samples towards locations that better align with the textual description, through visual reasoning. We demonstrate the effectiveness of our method by comparing it with both standard retrieval methods and learning-based approaches. Our proposed method consistently outperforms these baselines across all five large-scale datasets.

在大规模三维场景中定位文本描述本质上是一个具有模糊性的任务，尤其是在描述诸如“城市中所有交通信号灯”这样的通用概念时。为便于基于这些概念进行推理，需要以分布形式表达文本定位。在本文中，我们生成了基于文本描述条件的相机位姿分布。
为了实现这种生成，我们提出了一种基于扩散的架构，该架构在文本描述条件下将噪声化的 6 自由度（6DoF）相机位姿扩散到其可能的位置。条件信号通过预训练的文本编码器从文本描述中提取。文本描述与位姿分布之间的连接通过预训练的视觉-语言模型（Vision-Language-Model），即 CLIP 建立。
此外，我们证明了可以通过使用三维高斯散点（3D Gaussian Splatting）渲染潜在位姿，对分布中的候选位姿进行进一步优化。通过视觉推理，引导不正确的样本向更符合文本描述的位置移动。
我们通过与标准检索方法和基于学习的方法进行比较，验证了该方法的有效性。在五个大规模数据集上的实验表明，我们的方法在所有基准上均优于这些基线方法，表现出了一致的优势。
