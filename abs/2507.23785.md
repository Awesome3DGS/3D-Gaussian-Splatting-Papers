### Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis

In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content.

本文提出了一种新颖的视频到 4D 生成框架，可从单个视频输入生成高质量的动态三维内容。直接进行 4D 扩散建模极具挑战性，因为这不仅需要高成本的数据构建，还要应对同时表示三维形状、外观和运动的高维特性。为解决这些问题，我们引入了一种直接的 4DMesh-to-GS 变动场 VAE，该方法可直接从三维动画数据中编码标准高斯泼溅（GS）及其时间变化，而无需针对每个实例进行拟合，并将高维动画压缩到紧凑的潜在空间。在这一高效表示的基础上，我们训练了一个高斯变动场扩散模型，该模型结合了时间感知的扩散 Transformer，并以输入视频和标准 GS 为条件。该模型在精心筛选的 Objaverse 数据集中可动画的三维物体上进行训练，与现有方法相比展现出更高的生成质量。同时，即便仅在合成数据上训练，它在真实视频输入上的泛化能力也十分出色，为生成高质量的动画三维内容开辟了新途径。
