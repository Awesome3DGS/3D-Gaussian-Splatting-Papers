### Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis

We propose GRGS, a generalizable and relightable 3D Gaussian framework for high-fidelity human novel view synthesis under diverse lighting conditions. Unlike existing methods that rely on per-character optimization or ignore physical constraints, GRGS adopts a feed-forward, fully supervised strategy that projects geometry, material, and illumination cues from multi-view 2D observations into 3D Gaussian representations. Specifically, to reconstruct lighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement (LGR) module trained on synthetically relit data to predict accurate depth and surface normals. Based on the high-quality geometry, a Physically Grounded Neural Rendering (PGNR) module is further proposed to integrate neural prediction with physics-based shading, supporting editable relighting with shadows and indirect illumination. Besides, we design a 2D-to-3D projection training scheme that leverages differentiable supervision from ambient occlusion, direct, and indirect lighting maps, which alleviates the computational cost of explicit ray tracing. Extensive experiments demonstrate that GRGS achieves superior visual quality, geometric consistency, and generalization across characters and lighting conditions.


我们提出了 GRGS，一种具备泛化性与可重光照能力的三维高斯框架，能够在多样光照条件下实现高保真的人体新视角合成。不同于现有方法依赖于逐人物优化或忽略物理约束，GRGS 采用前馈式、全监督策略，将几何、材质与光照线索从多视角二维观测投影到三维高斯表示中。具体而言，为了重建具有光照不变性的几何结构，我们引入了一个基于合成重光照数据训练的光照感知几何优化模块（Lighting-aware Geometry Refinement, LGR），以预测准确的深度和表面法向量。在高质量几何基础上，我们进一步提出了物理约束神经渲染模块（Physically Grounded Neural Rendering, PGNR），将神经预测与基于物理的着色相结合，支持带阴影与间接光照的可编辑重光照效果。此外，我们设计了一种二维到三维的投影训练方案，结合环境遮蔽、直接光照和间接光照图的可微监督，避免了显式光线追踪所带来的高计算开销。大量实验证明，GRGS 在不同人物与光照条件下均表现出卓越的视觉质量、几何一致性与泛化能力。
