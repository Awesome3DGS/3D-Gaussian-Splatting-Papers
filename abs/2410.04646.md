### Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for Robust Ground-View Scene Rendering

We present a novel-view rendering algorithm, Mode-GS, for ground-robot trajectory datasets. Our approach is based on using anchored Gaussian splats, which are designed to overcome the limitations of existing 3D Gaussian splatting algorithms. Prior neural rendering methods suffer from severe splat drift due to scene complexity and insufficient multi-view observation, and can fail to fix splats on the true geometry in ground-robot datasets. Our method integrates pixel-aligned anchors from monocular depths and generates Gaussian splats around these anchors using residual-form Gaussian decoders. To address the inherent scale ambiguity of monocular depth, we parameterize anchors with per-view depth-scales and employ scale-consistent depth loss for online scale calibration. Our method results in improved rendering performance, based on PSNR, SSIM, and LPIPS metrics, in ground scenes with free trajectory patterns, and achieves state-of-the-art rendering performance on the R3LIVE odometry dataset and the Tanks and Temples dataset.

我们提出了一种用于地面机器人轨迹数据集的新颖视图渲染算法——Mode-GS。我们的方法基于锚定的高斯点，旨在克服现有3D高斯点算法的局限性。之前的神经渲染方法由于场景复杂性和多视角观测不足，往往会出现严重的点漂移问题，且在地面机器人数据集中无法将点固定在真实几何上。我们的方法结合了来自单目深度的像素对齐锚点，并通过残差形式的高斯解码器在这些锚点周围生成高斯点。为了解决单目深度固有的尺度模糊性，我们通过每视角深度尺度参数化锚点，并采用尺度一致的深度损失进行在线尺度校准。基于PSNR、SSIM和LPIPS指标，我们的方法在具有自由轨迹模式的地面场景中表现出了更好的渲染性能，并在R3LIVE里程计数据集和Tanks and Temples数据集上实现了最先进的渲染效果。
