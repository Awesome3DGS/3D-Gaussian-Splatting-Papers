### UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images

We propose a feed-forward Gaussian Splatting model that unifies 3D scene and semantic field reconstruction. Combining 3D scenes with semantic fields facilitates the perception and understanding of the surrounding environment. However, key challenges include embedding semantics into 3D representations, achieving generalizable real-time reconstruction, and ensuring practical applicability by using only images as input without camera parameters or ground truth depth. To this end, we propose UniForward, a feed-forward model to predict 3D Gaussians with anisotropic semantic features from only uncalibrated and unposed sparse-view images. To enable the unified representation of the 3D scene and semantic field, we embed semantic features into 3D Gaussians and predict them through a dual-branch decoupled decoder. During training, we propose a loss-guided view sampler to sample views from easy to hard, eliminating the need for ground truth depth or masks required by previous methods and stabilizing the training process. The whole model can be trained end-to-end using a photometric loss and a distillation loss that leverages semantic features from a pre-trained 2D semantic model. At the inference stage, our UniForward can reconstruct 3D scenes and the corresponding semantic fields in real time from only sparse-view images. The reconstructed 3D scenes achieve high-quality rendering, and the reconstructed 3D semantic field enables the rendering of view-consistent semantic features from arbitrary views, which can be further decoded into dense segmentation masks in an open-vocabulary manner. Experiments on novel view synthesis and novel view segmentation demonstrate that our method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction.

我们提出了一种前馈式高斯溅射模型，用于统一三维场景与语义场的重建。将三维场景与语义场相结合，有助于感知与理解周围环境。然而，核心挑战在于：(1) 将语义信息嵌入到三维表示中；(2) 实现具有泛化能力的实时重建；(3) 在仅使用图像作为输入的情况下（无需相机参数或真实深度）确保其实用性。为此，我们提出了 **UniForward**，一种前馈式模型，仅基于未经标定且无位姿的稀疏视角图像，即可预测具有各向异性语义特征的三维高斯。为实现三维场景与语义场的统一表示，我们将语义特征嵌入三维高斯中，并通过双分支解耦解码器进行预测。在训练阶段，我们提出了一种基于损失引导的视角采样策略，从简单到困难依次采样视角，从而避免了以往方法所需的真实深度或掩码标注，并能稳定训练过程。整个模型可端到端训练，损失函数包括光度损失和利用预训练二维语义模型特征的蒸馏损失。在推理阶段，**UniForward** 仅依赖稀疏视角图像即可实时重建三维场景及其对应的语义场。重建的三维场景具有高质量渲染效果，而重建的三维语义场则支持从任意视角渲染视角一致的语义特征，并可进一步解码为开放词汇的稠密分割掩码。在新视角合成与新视角分割任务上的实验结果表明，我们的方法在统一三维场景与语义场重建方面达到了当前最优的性能。
