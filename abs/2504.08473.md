### Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data Generation

Generating synthetic images is a useful method for cheaply obtaining labeled data for training computer vision models. However, obtaining accurate 3D models of relevant objects is necessary, and the resulting images often have a gap in realism due to challenges in simulating lighting effects and camera artifacts. We propose using the novel view synthesis method called Gaussian Splatting to address these challenges. We have developed a synthetic data pipeline for generating high-quality context-aware instance segmentation training data for specific objects. This process is fully automated, requiring only a video of the target object. We train a Gaussian Splatting model of the target object and automatically extract the object from the video. Leveraging Gaussian Splatting, we then render the object on a random background image, and monocular depth estimation is employed to place the object in a believable pose. We introduce a novel dataset to validate our approach and show superior performance over other data generation approaches, such as Cut-and-Paste and Diffusion model-based generation.

合成图像是一种低成本获取带标签数据、用于训练计算机视觉模型的有效手段。然而，这一过程通常依赖准确的三维模型，且由于难以真实模拟光照效果与相机成像特性，合成图像常存在逼真度不足的鸿沟。
为解决上述问题，我们提出采用新视角合成方法 Gaussian Splatting。我们构建了一条用于特定物体高质量、具上下文感知能力的实例分割训练数据生成流程，该流程全自动化，仅需输入目标物体的视频。
具体而言，我们首先训练目标物体的 Gaussian Splatting 模型，并自动从视频中完成物体提取。随后，利用 Gaussian Splatting 将该物体渲染到随机背景图像上，并结合单目深度估计以合理地放置物体，生成具有真实空间结构的合成图像。
我们还引入了一个新数据集用于验证所提方法的有效性。实验表明，相较于 Cut-and-Paste 方法与基于扩散模型的生成方法，我们的方法在合成质量与下游任务性能上均表现出更优结果。
