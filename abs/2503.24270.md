### Visual Acoustic Fields

Objects produce different sounds when hit, and humans can intuitively infer how an object might sound based on its appearance and material properties. Inspired by this intuition, we propose Visual Acoustic Fields, a framework that bridges hitting sounds and visual signals within a 3D space using 3D Gaussian Splatting (3DGS). Our approach features two key modules: sound generation and sound localization. The sound generation module leverages a conditional diffusion model, which takes multiscale features rendered from a feature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the sound localization module enables querying the 3D scene, represented by the feature-augmented 3DGS, to localize hitting positions based on the sound sources. To support this framework, we introduce a novel pipeline for collecting scene-level visual-sound sample pairs, achieving alignment between captured images, impact locations, and corresponding sounds. To the best of our knowledge, this is the first dataset to connect visual and acoustic signals in a 3D context. Extensive experiments on our dataset demonstrate the effectiveness of Visual Acoustic Fields in generating plausible impact sounds and accurately localizing impact sources.

物体在受到撞击时会产生不同的声音，人类可以根据物体的外观和材质直观地推断其可能发出的声音。受此直觉启发，本文提出了Visual Acoustic Fields，一个在三维空间中利用三维高斯泼洒（3D Gaussian Splatting, 3DGS）将撞击声音与视觉信号关联的框架。
我们的方法包含两个核心模块：声音生成与声音定位。声音生成模块采用条件扩散模型（conditional diffusion model），输入由特征增强的3DGS渲染出的多尺度特征，生成真实的撞击声音。同时，声音定位模块支持在由特征增强的3DGS表示的三维场景中进行查询，根据声音源定位撞击位置。
为支撑这一框架，我们提出了一条新颖的数据采集流程，用于收集场景级的视觉-声音样本对，实现图像、撞击位置与对应声音之间的对齐。据我们所知，这是第一个在三维环境中连接视觉与声学信号的数据集。
在我们构建的数据集上进行的大量实验表明，Visual Acoustic Fields能够生成合理可信的撞击声音，并准确定位撞击源位置，验证了方法的有效性。
