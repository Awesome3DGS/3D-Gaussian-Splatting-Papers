### Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields

3D reconstruction of highly deformable surfaces (e.g. cloths) from monocular RGB videos is a challenging problem, and no solution provides a consistent and accurate recovery of fine-grained surface details. To account for the ill-posed nature of the setting, existing methods use deformation models with statistical, neural, or physical priors. They also predominantly rely on nonadaptive discrete surface representations (e.g. polygonal meshes), perform frame-by-frame optimisation leading to error propagation, and suffer from poor gradients of the mesh-based differentiable renderers. Consequently, fine surface details such as cloth wrinkles are often not recovered with the desired accuracy. In response to these limitations, we propose ThinShell-SfT, a new method for non-rigid 3D tracking that represents a surface as an implicit and continuous spatiotemporal neural field. We incorporate continuous thin shell physics prior based on the Kirchhoff-Love model for spatial regularisation, which starkly contrasts the discretised alternatives of earlier works. Lastly, we leverage 3D Gaussian splatting to differentiably render the surface into image space and optimise the deformations based on analysis-bysynthesis principles. Our Thin-Shell-SfT outperforms prior works qualitatively and quantitatively thanks to our continuous surface formulation in conjunction with a specially tailored simulation prior and surface-induced 3D Gaussians.

从单目 RGB 视频中重建高度可变形表面（如布料）的三维形状是一项具有挑战性的任务，目前尚无方法能够一致且精确地恢复细粒度的表面细节。由于该问题本质上是病态的，现有方法通常引入统计、神经或物理先验的变形模型。然而，这些方法大多依赖非自适应的离散表面表示（例如多边形网格），进行逐帧优化，导致误差累积，并且受到基于网格的可微渲染器梯度质量差的限制。因此，诸如布料褶皱等细节通常无法以理想精度恢复。
针对上述限制，我们提出了 ThinShell-SfT，这是一种用于非刚性三维追踪的新方法，将表面表示为隐式且连续的时空神经场。我们引入了基于 Kirchhoff-Love 模型的连续薄壳物理先验，用于空间正则化，这与以往工作中的离散化替代方案形成鲜明对比。最后，我们利用三维高斯投影将表面可微渲染到图像空间，并基于“分析-合成”原则优化变形。
得益于我们连续的表面建模方式、专门设计的仿真先验以及由表面诱导的三维高斯表示，ThinShell-SfT 在定性与定量评估中均优于现有方法。
