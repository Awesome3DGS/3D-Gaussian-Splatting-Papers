### Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling

Photorealistic and animatable human avatars are a key enabler for virtual/augmented reality, telepresence, and digital entertainment. While recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering quality and efficiency, existing methods still face fundamental challenges, including time-consuming per-subject optimization and poor generalization under sparse monocular inputs. In this work, we present the Parametric Gaussian Human Model (PGHM), a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos. PGHM introduces two core components: (1) a UV-aligned latent identity map that compactly encodes subject-specific geometry and appearance into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that predicts Gaussian attributes by decomposing static, pose-dependent, and view-dependent components via conditioned decoders. This design enables robust rendering quality under challenging poses and viewpoints, while allowing efficient subject adaptation without requiring multi-view capture or long optimization time. Experiments show that PGHM is significantly more efficient than optimization-from-scratch methods, requiring only approximately 20 minutes per subject to produce avatars with comparable visual quality, thereby demonstrating its practical applicability for real-world monocular avatar creation.

逼真且可动画的人体数字化身是虚拟/增强现实、远程呈现以及数字娱乐等应用的关键支撑。尽管近年来三维高斯泼溅（3D Gaussian Splatting, 3DGS）在渲染质量和效率方面取得了显著进展，现有方法仍面临诸多核心挑战，例如每位角色需进行耗时的个体优化，以及在稀疏单目输入下泛化能力不足。
为此，我们提出了参数化高斯人体模型（Parametric Gaussian Human Model, PGHM），这是一种通用高效的框架，通过将人体先验融入 3DGS，实现从单目视频中快速且高保真的数字人重建。PGHM 包含两个核心组件：（1）UV 对齐的潜在身份特征图（latent identity map），将个体的几何与外观紧凑编码为可学习的特征张量；（2）解耦的多头 U-Net 网络（Multi-Head U-Net），通过条件解码器将高斯属性分解为静态、姿态相关与视角相关三个子空间进行预测。
该设计不仅在复杂姿态与视角条件下实现了稳健的渲染效果，还能高效地适配新主体，无需多视角捕捉或长时间优化。实验结果表明，PGHM 相较于从零优化的方法效率大幅提升，每个主体仅需约 20 分钟即可生成具有可比视觉质量的数字化身，展现出在现实场景中创建单目数字人的实际应用潜力。
