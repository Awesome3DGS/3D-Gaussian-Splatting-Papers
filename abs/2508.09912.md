### E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras

Novel view synthesis and 4D reconstruction techniques predominantly rely on RGB cameras, thereby inheriting inherent limitations such as the dependence on adequate lighting, susceptibility to motion blur, and a limited dynamic range. Event cameras, offering advantages of low power, high temporal resolution and high dynamic range, have brought a new perspective to addressing the scene reconstruction challenges in high-speed motion and low-light scenes. To this end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting approach, for novel view synthesis from multi-view event streams with fast-moving cameras. Specifically, we introduce an event-based initialization scheme to ensure stable training and propose event-adaptive slicing splatting for time-aware reconstruction. Additionally, we employ intensity importance pruning to eliminate floating artifacts and enhance 3D consistency, while incorporating an adaptive contrast threshold for more precise optimization. We design a synthetic multi-view camera setup with six moving event cameras surrounding the object in a 360-degree configuration and provide a benchmark multi-view event stream dataset that captures challenging motion scenarios. Our approach outperforms both event-only and event-RGB fusion baselines and paves the way for the exploration of multi-view event-based reconstruction as a novel approach for rapid scene capture.

新颖视图合成和四维重建技术主要依赖于 RGB 相机，因此不可避免地受到一些固有限制，如对充足光照的依赖、易受运动模糊影响以及有限的动态范围。事件相机具有低功耗、高时间分辨率和高动态范围的优势，为解决高速运动和低光场景下的场景重建挑战带来了新的视角。为此，我们提出了 E-4DGS，这是首个基于事件驱动的动态高斯泼溅方法，用于在快速运动相机的多视角事件流中进行新颖视图合成。具体而言，我们引入了一种基于事件的初始化方案以确保训练稳定，并提出了事件自适应切片泼溅用于时间感知的重建。此外，我们采用强度重要性剪枝来消除漂浮伪影并增强三维一致性，同时结合自适应对比度阈值以实现更精确的优化。我们设计了一个合成的多视角相机系统，由六个运动事件相机以 360 度方式环绕目标，并提供了一个基准多视角事件流数据集，以捕捉具有挑战性的运动场景。实验结果表明，我们的方法在性能上优于事件单独和事件-RGB 融合的基线方法，并为多视角事件驱动的重建作为一种快速场景捕获的新方法开辟了道路。
