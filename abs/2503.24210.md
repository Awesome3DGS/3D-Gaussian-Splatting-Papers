### DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting

Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines.

从模糊的多视角图像中重建清晰的三维表示一直是计算机视觉领域的长期挑战。近年来，相关研究尝试利用事件相机（event-based cameras）提升运动模糊情况下的新视角合成质量，得益于事件相机的高动态范围和微秒级时间分辨率。然而，这些方法在恢复颜色准确性或保持细粒度细节方面往往表现欠佳，导致视觉质量次优。
本文提出了DiET-GS，一种结合扩散先验与事件流辅助的运动去模糊三维高斯泼洒（3DGS）方法。我们的框架在两阶段训练策略中有效利用无模糊的事件流和扩散先验。具体来说，我们引入了一种新颖的框架，通过**事件双重积分（event double integral）**约束3DGS，从而同时实现准确的颜色还原和清晰的细节恢复。此外，我们提出了一种简单的技术，利用扩散先验进一步增强边缘细节。
在合成数据和真实数据上的定性与定量实验结果表明，与现有基线方法相比，DiET-GS能够生成质量显著更高的新视角图像。
