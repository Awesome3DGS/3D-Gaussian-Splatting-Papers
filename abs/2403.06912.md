### DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization

Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a 25× reduction in training time, and over 3000× faster rendering speed.

辐射场在从稀疏输入视图合成新视图方面展示了令人印象深刻的性能，然而，现有方法受到高训练成本和慢推理速度的困扰。本文介绍了 DNGaussian，一个基于 3D 高斯辐射场的深度正则化框架，提供实时、高质量的少样本新视角合成，且成本低。我们的动机源自最近 3D 高斯喷溅的高效表示和令人惊讶的质量，尽管在输入视图减少时会遇到几何退化。在高斯辐射场中，我们发现场景几何的这种退化主要与高斯原语的定位有关，可以通过深度约束来缓解。因此，我们提出了硬性和软性深度正则化，以在粗糙的单目深度监督下恢复精确的场景几何，同时保持细腻的颜色外观。为了进一步细化几何形状的重塑，我们引入了全局-局部深度归一化，增强了对小的局部深度变化的关注。在 LLFF、DTU 和 Blender 数据集上的广泛实验表明，DNGaussian 超越了最先进的方法，以显著降低的内存成本、训练时间的 25 倍减少和渲染速度的 3000 倍以上提升，实现了可比或更好的结果。
