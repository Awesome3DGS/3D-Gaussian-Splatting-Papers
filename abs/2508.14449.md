### D^3-Talker: Dual-Branch Decoupled Deformation Fields for Few-Shot 3D Talking Head Synthesis

A key challenge in 3D talking head synthesis lies in the reliance on a long-duration talking head video to train a new model for each target identity from scratch. Recent methods have attempted to address this issue by extracting general features from audio through pre-training models. However, since audio contains information irrelevant to lip motion, existing approaches typically struggle to map the given audio to realistic lip behaviors in the target face when trained on only a few frames, causing poor lip synchronization and talking head image quality. This paper proposes D^3-Talker, a novel approach that constructs a static 3D Gaussian attribute field and employs audio and Facial Motion signals to independently control two distinct Gaussian attribute deformation fields, effectively decoupling the predictions of general and personalized deformations. We design a novel similarity contrastive loss function during pre-training to achieve more thorough decoupling. Furthermore, we integrate a Coarse-to-Fine module to refine the rendered images, alleviating blurriness caused by head movements and enhancing overall image quality. Extensive experiments demonstrate that D^3-Talker outperforms state-of-the-art methods in both high-fidelity rendering and accurate audio-lip synchronization with limited training data.

三维说话人头像合成的关键挑战在于需要依赖长时长的说话人视频，为每个目标身份从零开始训练新模型。近年来的一些方法尝试通过预训练模型从音频中提取通用特征来缓解这一问题。然而，由于音频中包含与唇部运动无关的信息，现有方法在仅用少量帧训练时，往往难以将给定音频映射到目标人脸的逼真唇部运动，导致唇形同步和头像图像质量较差。本文提出 D^3-Talker，一种新颖的方法，通过构建静态三维高斯属性场，并分别利用音频和面部运动信号独立控制两个不同的高斯属性形变场，从而有效解耦通用和个性化形变的预测。我们设计了一种新的相似性对比损失函数用于预训练，以实现更彻底的解耦。此外，我们集成了一个由粗到细的模块来优化渲染图像，缓解由头部运动引起的模糊，并提升整体图像质量。大量实验表明，D^3-Talker 在有限训练数据下实现了更高保真的渲染和更准确的音频-唇形同步，显著优于当前最先进的方法。
