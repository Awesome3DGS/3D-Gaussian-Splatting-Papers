### VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis

Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars.

真实且高保真的三维面部动画对于在人机交互和无障碍应用中的富有表现力的虚拟形象系统至关重要。尽管已有方法在质量上取得了可喜的成果，但其对网格域的依赖限制了其充分利用二维计算机视觉与图形学中快速发展的视觉创新能力。我们提出了 **VisualSpeaker**，一种利用光真实可微渲染并结合视觉语音识别监督的新方法，以提升三维面部动画效果。我们的方法核心贡献是一种感知型唇读损失（perceptual lip-reading loss），该损失在训练过程中通过将光真实的三维高斯溅射虚拟形象渲染结果输入至预训练的视觉自动语音识别模型（Visual ASR）获得。在 MEAD 数据集上的评估表明，VisualSpeaker 在标准唇部顶点误差（Lip Vertex Error）指标上提升了 56.1%，并显著改善了生成动画的感知质量，同时保留了网格驱动动画的可控性。这种感知导向的设计天然支持精确的口型表达，这对于在手语虚拟形象中区分相似手势至关重要。
