### Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware

Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations.

扩展机器人学习的能力需要大量且多样化的数据。然而，当前主流的数据采集方式——人工远程操作——成本高昂，受限于人为操作和对物理机器人设备的依赖。
我们提出了 Real2Render2Real（R2R2R），一种无需依赖物体动力学模拟或机器人硬件远程操作的全新机器人训练数据生成方法。该方法的输入仅包括：一段由智能手机拍摄的一个或多个物体的扫描，以及一段人类演示的视频。
R2R2R 能够渲染出成千上万条具有高视觉保真度、与机器人平台无关的演示数据。其关键在于重建高精度的三维物体几何与外观，并对物体的 6 自由度（6-DoF）运动进行跟踪。该方法采用 三维高斯投影（3D Gaussian Splatting, 3DGS） 实现对刚体与关节物体的灵活资产生成与轨迹合成，并将这些表示转换为网格（meshes），以兼容如 IsaacLab 等可扩展渲染引擎（但关闭碰撞建模功能）。
由 R2R2R 生成的机器人演示数据可直接用于处理机器人本体状态和图像观测的模型，如视觉-语言-动作（Vision-Language-Action, VLA）模型与模仿学习策略。
实物实验表明，仅基于一段人类演示所生成的 R2R2R 数据即可训练出在表现上媲美于 150 条人工远程操作演示数据 的机器人模型。
