### Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs

Despite recent successes in novel view synthesis using 3D Gaussian Splatting (3DGS), modeling scenes with sparse inputs remains a challenge. In this work, we address two critical yet overlooked issues in real-world sparse-input modeling: extrapolation and occlusion. To tackle these issues, we propose to use a reconstruction by generation pipeline that leverages learned priors from video diffusion models to provide plausible interpretations for regions outside the field of view or occluded. However, the generated sequences exhibit inconsistencies that do not fully benefit subsequent 3DGS modeling. To address the challenge of inconsistencies, we introduce a novel scene-grounding guidance based on rendered sequences from an optimized 3DGS, which tames the diffusion model to generate consistent sequences. This guidance is training-free and does not require any fine-tuning of the diffusion model. To facilitate holistic scene modeling, we also propose a trajectory initialization method. It effectively identifies regions that are outside the field of view and occluded. We further design a scheme tailored for 3DGS optimization with generated sequences. Experiments demonstrate that our method significantly improves upon the baseline and achieves state-of-the-art performance on challenging benchmarks.

尽管 3D Gaussian Splatting (3DGS) 在新视角合成方面取得了显著成功，但在稀疏输入的场景建模中仍然面临挑战。在本文中，我们聚焦于真实世界稀疏输入建模中两个关键但被忽视的问题：外推（extrapolation）和遮挡（occlusion）。
为了解决这些问题，我们提出了一种基于生成的重建（reconstruction by generation）方法，利用视频扩散模型（video diffusion models） 的学习先验，为视野之外或被遮挡区域提供合理的解释。然而，直接生成的序列往往存在不一致性，难以充分促进后续 3DGS 建模。
针对这一问题，我们提出了一种基于优化 3DGS 渲染序列的场景引导（scene-grounding guidance）方法，以约束扩散模型生成一致的序列。该引导方法无需额外训练，且不需要对扩散模型进行微调。
此外，为了实现整体场景建模，我们提出了一种轨迹初始化（trajectory initialization）方法，用于有效识别视野之外和被遮挡区域。我们还设计了一种适用于 3DGS 优化的生成序列策略，以进一步提高建模质量。
实验表明，我们的方法在多个具有挑战性的基准测试上显著超越基线方法，并在稀疏输入场景建模中达到了最先进（SOTA）性能。
