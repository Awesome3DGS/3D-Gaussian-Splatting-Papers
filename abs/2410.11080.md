### Few-shot Novel View Synthesis using Depth Aware 3D Gaussian Splatting

3D Gaussian splatting has surpassed neural radiance field methods in novel view synthesis by achieving lower computational costs and real-time high-quality rendering. Although it produces a high-quality rendering with a lot of input views, its performance drops significantly when only a few views are available. In this work, we address this by proposing a depth-aware Gaussian splatting method for few-shot novel view synthesis. We use monocular depth prediction as a prior, along with a scale-invariant depth loss, to constrain the 3D shape under just a few input views. We also model color using lower-order spherical harmonics to avoid overfitting. Further, we observe that removing splats with lower opacity periodically, as performed in the original work, leads to a very sparse point cloud and, hence, a lower-quality rendering. To mitigate this, we retain all the splats, leading to a better reconstruction in a few view settings. Experimental results show that our method outperforms the traditional 3D Gaussian splatting methods by achieving improvements of 10.5% in peak signal-to-noise ratio, 6% in structural similarity index, and 14.1% in perceptual similarity, thereby validating the effectiveness of our approach.

3D高斯散射在新视角合成中已超越神经辐射场方法，实现了更低的计算成本和实时高质量渲染。尽管在大量输入视角下能生成高质量的渲染，但当仅有少量视角时，其性能会显著下降。在本工作中，我们提出了一种深度感知的高斯散射方法，专门用于少样本的新视角合成。我们使用单目深度预测作为先验，并结合尺度不变的深度损失来约束在少量输入视角下的3D形状。此外，我们采用低阶球谐函数来建模颜色，以避免过拟合。此外，我们观察到在原始方法中定期移除低不透明度的散点会导致点云过于稀疏，从而降低渲染质量。为了解决这一问题，我们保留了所有的散点，从而在少视角设置下实现了更好的重建。实验结果表明，我们的方法在峰值信噪比（PSNR）上提高了10.5%，结构相似性指数（SSIM）上提高了6%，感知相似性上提高了14.1%，验证了我们方法的有效性。
