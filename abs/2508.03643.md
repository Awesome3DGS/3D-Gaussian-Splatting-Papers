### Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images

Reconstructing and semantically interpreting 3D scenes from sparse 2D views remains a fundamental challenge in computer vision. Conventional methods often decouple semantic understanding from reconstruction or necessitate costly per-scene optimization, thereby restricting their scalability and generalizability. In this paper, we introduce Uni3R, a novel feed-forward framework that jointly reconstructs a unified 3D scene representation enriched with open-vocabulary semantics, directly from unposed multi-view images. Our approach leverages a Cross-View Transformer to robustly integrate information across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian primitives endowed with semantic feature fields. This unified representation facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic segmentation, and depth prediction, all within a single, feed-forward pass. Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D scene reconstruction and understanding.

从稀疏的二维视图中重建并语义解析三维场景仍然是计算机视觉中的一项基本挑战。传统方法往往将语义理解与重建解耦，或需要代价高昂的逐场景优化，从而限制了其可扩展性与泛化能力。本文提出了 Uni3R，一种新颖的前向框架，可直接从无位姿的多视图图像中联合重建具备开放词汇语义的统一三维场景表示。我们的方法利用跨视图 Transformer（Cross-View Transformer）稳健整合任意多视图输入的信息，并回归一组带有语义特征场的三维高斯基元。这一统一表示能够在单次前向推理中同时实现高保真新视角合成、开放词汇三维语义分割以及深度预测。大量实验证明，Uni3R 在多个基准数据集上均建立了新的最先进水平，包括在 RE10K 上达到 25.07 PSNR、在 ScanNet 上达到 55.84 mIoU。本工作为通用化、统一的三维场景重建与理解开辟了新范式。
