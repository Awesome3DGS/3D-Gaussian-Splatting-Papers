### From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting

3D Gaussian Splatting has emerged as a powerful approach in novel view synthesis, delivering rapid training and rendering but at the cost of an ever-growing set of Gaussian primitives that strains memory and bandwidth. We introduce AutoOpti3DGS, a training-time framework that automatically restrains Gaussian proliferation without sacrificing visual fidelity. The key idea is to feed the input images to a sequence of learnable Forward and Inverse Discrete Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters are learnable and initialized to zero, and an auxiliary orthogonality loss gradually activates fine frequencies. This wavelet-driven, coarse-to-fine process delays the formation of redundant fine Gaussians, allowing 3DGS to capture global structure first and refine detail only when necessary. Through extensive experiments, AutoOpti3DGS requires just a single filter learning-rate hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks, and consistently produces sparser scene representations more compatible with memory or storage-constrained hardware.

三维高斯投影（3D Gaussian Splatting）作为一种新视角合成的强大方法，能够实现快速训练与渲染，但代价是高斯基元数量不断膨胀，从而对内存与带宽造成压力。为了解决这一问题，我们提出了 **AutoOpti3DGS** —— 一个训练阶段框架，可在不降低视觉保真的前提下自动抑制高斯基元的过度增长。其核心思想是将输入图像输入到一组可学习的正向与逆向离散小波变换（Discrete Wavelet Transforms）中，其中低通滤波器保持固定，高通滤波器则可学习并初始化为零，通过辅助正交损失逐步激活细节频率。该基于小波驱动的由粗到细训练策略，延迟了冗余精细高斯的生成，使 3DGS 能够优先捕捉全局结构，仅在必要时才细化局部细节。大量实验表明，AutoOpti3DGS 仅需一个滤波器学习率的超参数，能够无缝集成到现有高效 3DGS 框架中，始终生成更稀疏的场景表示，更加适用于内存或存储受限的硬件环境。
