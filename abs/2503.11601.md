### Advancing 3D Gaussian Splatting Editing with Complementary and Consensus Information

We present a novel framework for enhancing the visual fidelity and consistency of text-guided 3D Gaussian Splatting (3DGS) editing. Existing editing approaches face two critical challenges: inconsistent geometric reconstructions across multiple viewpoints, particularly in challenging camera positions, and ineffective utilization of depth information during image manipulation, resulting in over-texture artifacts and degraded object boundaries. To address these limitations, we introduce: 1) A complementary information mutual learning network that enhances depth map estimation from 3DGS, enabling precise depth-conditioned 3D editing while preserving geometric structures. 2) A wavelet consensus attention mechanism that effectively aligns latent codes during the diffusion denoising process, ensuring multi-view consistency in the edited results. Through extensive experimentation, our method demonstrates superior performance in rendering quality and view consistency compared to state-of-the-art approaches. The results validate our framework as an effective solution for text-guided editing of 3D scenes.

我们提出了一个新的框架，用于增强文本引导的 3D 高斯溅射（3DGS）编辑的视觉真实感和一致性。现有的编辑方法面临两个关键挑战：在多个视角下几何重建的不一致，特别是在具有挑战性的摄像机位置下；以及在图像操作过程中深度信息的无效利用，导致过度纹理伪影和物体边界的退化。为了解决这些局限性，我们引入了：1）一个互补信息互学习网络，增强了从 3DGS 获得的深度图估计，使得在保持几何结构的同时能够进行精确的深度条件 3D 编辑；2）一种小波共识注意机制，在扩散去噪过程中有效地对齐潜在代码，确保编辑结果在多视角下的一致性。通过广泛的实验，我们的方法在渲染质量和视角一致性方面，相较于最先进的方法，展现了更优的性能。实验结果验证了我们的框架作为文本引导 3D 场景编辑的有效解决方案。
