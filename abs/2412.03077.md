### RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos

Dynamic view synthesis (DVS) has advanced remarkably in recent years, achieving high-fidelity rendering while reducing computational costs. Despite the progress, optimizing dynamic neural fields from casual videos remains challenging, as these videos do not provide direct 3D information, such as camera trajectories or the underlying scene geometry. In this work, we present RoDyGS, an optimization pipeline for dynamic Gaussian Splatting from casual videos. It effectively learns motion and underlying geometry of scenes by separating dynamic and static primitives, and ensures that the learned motion and geometry are physically plausible by incorporating motion and geometric regularization terms. We also introduce a comprehensive benchmark, Kubric-MRig, that provides extensive camera and object motion along with simultaneous multi-view captures, features that are absent in previous benchmarks. Experimental results demonstrate that the proposed method significantly outperforms previous pose-free dynamic neural fields and achieves competitive rendering quality compared to existing pose-free static neural fields.

动态视图合成（Dynamic View Synthesis, DVS）近年来取得了显著进展，不仅实现了高保真渲染，还降低了计算成本。然而，从普通视频中优化动态神经场仍然具有挑战性，因为这类视频通常缺乏直接的三维信息，例如相机轨迹或场景几何。
在本文中，我们提出了RoDyGS，一种从普通视频中优化动态高斯散点（Dynamic Gaussian Splatting）的管道。该方法通过分离动态和静态原语，学习场景的运动和底层几何，同时通过引入运动和几何正则化项，确保学习到的运动和几何符合物理可行性。此外，我们构建了一个全面的基准数据集Kubric-MRig，该数据集提供了丰富的相机和物体运动，以及同步的多视角捕捉，这些特性在现有基准中尚未实现。
实验结果表明，所提出的方法在无位姿动态神经场任务中显著优于现有方法，并在渲染质量上与现有的无位姿静态神经场方法具有竞争力，为动态场景的高效建模提供了新思路。
