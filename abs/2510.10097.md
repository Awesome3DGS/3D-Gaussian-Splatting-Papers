### Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting

Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced 3D reconstruction and novel view synthesis, but remain heavily dependent on accurate camera poses and dense viewpoint coverage. These requirements limit their applicability in sparse-view settings, where pose estimation becomes unreliable and supervision is insufficient. To overcome these challenges, we introduce Gesplat, a 3DGS-based framework that enables robust novel view synthesis and geometrically consistent reconstruction from unposed sparse images. Unlike prior works that rely on COLMAP for sparse point cloud initialization, we leverage the VGGT foundation model to obtain more reliable initial poses and dense point clouds. Our approach integrates several key innovations: 1) a hybrid Gaussian representation with dual position-shape optimization enhanced by inter-view matching consistency; 2) a graph-guided attribute refinement module to enhance scene details; and 3) flow-based depth regularization that improves depth estimation accuracy for more effective supervision. Comprehensive quantitative and qualitative experiments demonstrate that our approach achieves more robust performance on both forward-facing and large-scale complex datasets compared to other pose-free methods.

神经辐射场（Neural Radiance Fields, NeRF）与三维高斯泼溅（3D Gaussian Splatting, 3DGS）在三维重建与新视角合成方面取得了重要进展，但仍严重依赖于准确的相机位姿和密集的视角覆盖。这些要求限制了其在稀疏视角设置下的适用性，因为在此条件下位姿估计变得不可靠，监督信息也十分有限。为应对这一挑战，我们提出Gesplat，一种基于3DGS的框架，能够从无位姿的稀疏图像中实现鲁棒的新视角合成与几何一致的三维重建。与以往依赖COLMAP进行稀疏点云初始化的方法不同，我们采用VGGT基础模型来获得更可靠的初始位姿与稠密点云。我们的方法融合了多项关键创新：1）融合位置-形状双重优化与视角间匹配一致性增强的混合高斯表示；2）用于提升场景细节的图引导属性细化模块；3）基于光流的深度正则化机制，以提高深度估计精度，实现更有效的监督。大量定量与定性实验表明，与其他无需位姿的方法相比，我们的方法在前向视角和大规模复杂数据集上均展现出更为鲁棒的性能表现。
