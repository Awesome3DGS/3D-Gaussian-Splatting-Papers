### FlowR: Flowing from Sparse to Dense 3D Reconstructions

3D Gaussian splatting enables high-quality novel view synthesis (NVS) at real-time frame rates. However, its quality drops sharply as we depart from the training views. Thus, dense captures are needed to match the high-quality expectations of some applications, e.g. Virtual Reality (VR). However, such dense captures are very laborious and expensive to obtain. Existing works have explored using 2D generative models to alleviate this requirement by distillation or generating additional training views. These methods are often conditioned only on a handful of reference input views and thus do not fully exploit the available 3D information, leading to inconsistent generation results and reconstruction artifacts. To tackle this problem, we propose a multi-view, flow matching model that learns a flow to connect novel view renderings from possibly sparse reconstructions to renderings that we expect from dense reconstructions. This enables augmenting scene captures with novel, generated views to improve reconstruction quality. Our model is trained on a novel dataset of 3.6M image pairs and can process up to 45 views at 540x960 resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline consistently improves NVS in sparse- and dense-view scenarios, leading to higher-quality reconstructions than prior works across multiple, widely-used NVS benchmarks.

三维高斯喷溅（3D Gaussian Splatting）能够以实时帧率实现高质量的新视角合成（Novel View Synthesis, NVS）。然而，当观察角度偏离训练视角时，其渲染质量会显著下降。因此，为满足某些应用（如虚拟现实 VR）对高质量的要求，通常需要密集的数据采集，而这类采集过程往往代价高昂且极为繁琐。
已有研究尝试借助二维生成模型，通过蒸馏或生成额外训练视角来缓解对密集采集的依赖。然而，这些方法通常仅基于少量参考视图进行条件生成，未能充分利用可用的三维信息，导致生成结果存在不一致与重建伪影等问题。
为解决这一问题，我们提出了一种多视角流匹配模型，该模型学习一种映射流（flow），将来自稀疏重建的视角渲染结果对齐至期望的密集重建渲染结果，从而支持利用生成的新视角增强场景采集数据，提升重建质量。
我们的方法基于一个包含 360 万图像对的新数据集进行训练，并能在单张 H100 GPU 上以单次前向过程处理多达 45 个视角、540×960 分辨率（共 91K token）的输入。
该流程在稀疏视角与密集视角场景下均显著提升新视角合成效果，在多个主流 NVS 基准上实现了超过现有方法的高质量重建表现。
