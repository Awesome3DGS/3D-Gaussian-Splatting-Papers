### GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes

The semantic synthesis of unseen scenes from multiple viewpoints is crucial for research in 3D scene understanding. Current methods are capable of rendering novel-view images and semantic maps by reconstructing generalizable Neural Radiance Fields. However, they often suffer from limitations in speed and segmentation performance. We propose a generalizable semantic Gaussian Splatting method (GSsplat) for efficient novel-view synthesis. Our model predicts the positions and attributes of scene-adaptive Gaussian distributions from once input, replacing the densification and pruning processes of traditional scene-specific Gaussian Splatting. In the multi-task framework, a hybrid network is designed to extract color and semantic information and predict Gaussian parameters. To augment the spatial perception of Gaussians for high-quality rendering, we put forward a novel offset learning module through group-based supervision and a point-level interaction module with spatial unit aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat achieves state-of-the-art performance for semantic synthesis at the fastest speed.

从多个视角对未见场景进行语义合成是三维场景理解研究中的关键问题。当前方法通过重建具有泛化能力的神经辐射场（Neural Radiance Fields）可以实现新视角图像与语义图的渲染，但在速度与分割性能方面仍存在局限。
本文提出了一种用于高效新视角合成的可泛化语义高斯泼溅方法 GSsplat。该方法通过一次性输入预测场景自适应高斯分布的位置与属性，取代了传统场景特定高斯泼溅方法中的密化与剪枝流程。在多任务框架下，我们设计了一种混合网络用于提取颜色与语义信息，并预测高斯参数。
为增强高斯的空间感知能力以实现高质量渲染，我们提出了一种基于分组监督的偏移学习模块，以及结合空间单元聚合的点级交互模块。在不同数量的多视角输入下进行评估时，GSsplat 在语义合成任务中实现了当前最快的推理速度与最优性能。
