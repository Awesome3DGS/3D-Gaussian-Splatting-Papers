### Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS

Modern camera pipelines apply extensive on-device processing, such as exposure adjustment, white balance, and color correction, which, while beneficial individually, often introduce photometric inconsistencies across views. These appearance variations violate multi-view consistency and degrade the quality of novel view synthesis. Joint optimization of scene representations and per-image appearance embeddings has been proposed to address this issue, but at the cost of increased computational complexity and slower training. In this work, we propose a transformer-based method that predicts spatially adaptive bilateral grids to correct photometric variations in a multi-view consistent manner, enabling robust cross-scene generalization without the need for scene-specific retraining. By incorporating the learned grids into the 3D Gaussian Splatting pipeline, we improve reconstruction quality while maintaining high training efficiency. Extensive experiments show that our approach outperforms or matches existing scene-specific optimization methods in reconstruction fidelity and convergence speed.

现代相机处理流程通常会进行大量的设备端处理，例如曝光调整、白平衡和颜色校正，这些处理虽在单独应用时各有益处，但往往会在不同视角间引入光度不一致。这类外观变化破坏了多视图一致性，并降低了新视角合成的质量。为解决这一问题，已有研究提出将场景表示与逐图像的外观嵌入进行联合优化，但这会增加计算复杂度并减慢训练速度。在本研究中，我们提出了一种基于 Transformer 的方法，用于预测空间自适应的双边网格，从而以多视图一致的方式校正光度变化，实现无需针对特定场景重新训练的稳健跨场景泛化。通过将所学习的网格融合到 3D 高斯点渲染（3D Gaussian Splatting）流程中，我们在保持高训练效率的同时提升了重建质量。大量实验表明，我们的方法在重建保真度和收敛速度方面优于或可媲美现有的特定场景优化方法。
