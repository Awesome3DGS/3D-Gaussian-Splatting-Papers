### GFlow: Recovering 4D World from Monocular Video

Reconstructing 4D scenes from video inputs is a crucial yet challenging task. Conventional methods usually rely on the assumptions of multi-view video inputs, known camera parameters, or static scenes, all of which are typically absent under in-the-wild scenarios. In this paper, we relax all these constraints and tackle a highly ambitious but practical task, which we termed as AnyV4D: we assume only one monocular video is available without any camera parameters as input, and we aim to recover the dynamic 4D world alongside the camera poses. To this end, we introduce GFlow, a new framework that utilizes only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit representation, entailing a flow of Gaussian splatting through space and time. GFlow first clusters the scene into still and moving parts, then applies a sequential optimization process that optimizes camera poses and the dynamics of 3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity among neighboring points and smooth movement across frames. Since dynamic scenes always introduce new content, we also propose a new pixel-wise densification strategy for Gaussian points to integrate new visual content. Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also enables tracking of any points across frames without the need for prior training and segments moving objects from the scene in an unsupervised way. Additionally, the camera poses of each frame can be derived from GFlow, allowing for rendering novel views of a video scene through changing camera pose. By employing the explicit representation, we may readily conduct scene-level or object-level editing as desired, underscoring its versatility and power.

从视频输入中重建4D场景是一项至关重要但又极具挑战性的任务。传统方法通常依赖于多视角视频输入、已知的相机参数或静态场景的假设，这些在野外环境中通常是不存在的。在这篇论文中，我们放宽了所有这些限制，并解决了一个非常雄心勃勃但实际的任务，我们称之为 AnyV4D：我们假设只有一个单目视频可用，没有任何相机参数作为输入，我们的目标是恢复动态的4D世界以及相机姿态。为此，我们引入了 GFlow，这是一个新框架，仅使用2D先验（深度和光流）将视频（3D）提升到一个4D显式表征，包括通过时空的高斯喷溅流动。GFlow首先将场景聚类为静止部分和移动部分，然后应用一个序列优化过程，该过程基于2D先验和场景聚类优化相机姿态和3D高斯点的动态，确保相邻点之间的保真度和帧间的平滑运动。由于动态场景总是引入新内容，我们还提出了一种新的高斯点的像素级密集化策略，以整合新的视觉内容。此外，GFlow超越了单纯的4D重建的界限；它还可以在无需事先训练的情况下跟踪任何帧之间的点，并以无监督的方式从场景中分割移动对象。此外，可以从GFlow派生每个帧的相机姿态，允许通过改变相机姿态渲染视频场景的新视角。通过采用显式表征，我们可以根据需要进行场景级或对象级编辑，突显其多功能性和强大性。
