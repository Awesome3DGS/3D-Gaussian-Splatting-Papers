### Enhancing Monocular 3D Scene Completion with Diffusion Model

3D scene reconstruction is essential for applications in virtual reality, robotics, and autonomous driving, enabling machines to understand and interact with complex environments. Traditional 3D Gaussian Splatting techniques rely on images captured from multiple viewpoints to achieve optimal performance, but this dependence limits their use in scenarios where only a single image is available. In this work, we introduce FlashDreamer, a novel approach for reconstructing a complete 3D scene from a single image, significantly reducing the need for multi-view inputs. Our approach leverages a pre-trained vision-language model to generate descriptive prompts for the scene, guiding a diffusion model to produce images from various perspectives, which are then fused to form a cohesive 3D reconstruction. Extensive experiments show that our method effectively and robustly expands single-image inputs into a comprehensive 3D scene, extending monocular 3D reconstruction capabilities without further training.


3D 场景重建在虚拟现实（VR）、机器人技术和自动驾驶等应用中至关重要，使机器能够理解并与复杂环境交互。传统的 3D Gaussian Splatting (3DGS) 技术依赖于多视角图像来实现最佳性能，但这种依赖限制了其在单视角图像场景中的应用。
在本研究中，我们提出 FlashDreamer，一种从单张图像重建完整 3D 场景的新方法，大幅减少了对多视角输入的需求。我们的方法利用预训练的视觉-语言模型为场景生成描述性提示（prompts），并以此引导扩散模型（diffusion model）从多个视角生成图像，随后将这些视角图像融合，形成连贯的 3D 重建。
广泛实验表明，我们的方法能够有效且稳健地扩展单视角输入，生成完整的 3D 场景，从而在无需额外训练的情况下提升单目 3D 重建能力。
