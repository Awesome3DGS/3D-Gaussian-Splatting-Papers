### Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians

Editing 4D scenes reconstructed from monocular videos based on text prompts is a valuable yet challenging task with broad applications in content creation and virtual environments. The key difficulty lies in achieving semantically precise edits in localized regions of complex, dynamic scenes, while preserving the integrity of unedited content. To address this, we introduce Mono4DEditor, a novel framework for flexible and accurate text-driven 4D scene editing. Our method augments 3D Gaussians with quantized CLIP features to form a language-embedded dynamic representation, enabling efficient semantic querying of arbitrary spatial regions. We further propose a two-stage point-level localization strategy that first selects candidate Gaussians via CLIP similarity and then refines their spatial extent to improve accuracy. Finally, targeted edits are performed on localized regions using a diffusion-based video editing model, with flow and scribble guidance ensuring spatial fidelity and temporal coherence. Extensive experiments demonstrate that Mono4DEditor enables high-quality, text-driven edits across diverse scenes and object types, while preserving the appearance and geometry of unedited areas and surpassing prior approaches in both flexibility and visual fidelity.

基于文本提示编辑由单目视频重建而成的4D场景是一项具有广泛应用价值但极具挑战性的任务，广泛应用于内容创作与虚拟环境。其核心难点在于，如何在复杂动态场景中的局部区域实现语义精确的编辑，同时保持未编辑内容的完整性。为此，我们提出Mono4DEditor，一个灵活且精确的文本驱动4D场景编辑新框架。该方法通过将3D高斯与量化的CLIP特征相结合，构建出一种融合语言信息的动态表示，使得对任意空间区域的语义查询更加高效。我们进一步提出一种两阶段的点级定位策略，首先通过CLIP相似度筛选候选高斯点，然后细化其空间范围以提升定位精度。最后，我们在这些局部区域中使用基于扩散模型的视频编辑方法进行目标编辑，同时引入光流与涂鸦引导，确保空间保真与时间连贯。大量实验证明，Mono4DEditor能够在多种场景与对象类型下实现高质量、文本驱动的编辑，既保留了未编辑区域的外观与几何结构，又在灵活性与视觉保真度方面优于现有方法。
