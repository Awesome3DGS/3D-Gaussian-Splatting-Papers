### EvGGS: A Collaborative Learning Framework for Event-based Generalizable Gaussian Splatting

Event cameras offer promising advantages such as high dynamic range and low latency, making them well-suited for challenging lighting conditions and fast-moving scenarios. However, reconstructing 3D scenes from raw event streams is difficult because event data is sparse and does not carry absolute color information. To release its potential in 3D reconstruction, we propose the first event-based generalizable 3D reconstruction framework, called EvGGS, which reconstructs scenes as 3D Gaussians from only event input in a feedforward manner and can generalize to unseen cases without any retraining. This framework includes a depth estimation module, an intensity reconstruction module, and a Gaussian regression module. These submodules connect in a cascading manner, and we collaboratively train them with a designed joint loss to make them mutually promote. To facilitate related studies, we build a novel event-based 3D dataset with various material objects and calibrated labels of grayscale images, depth maps, camera poses, and silhouettes. Experiments show models that have jointly trained significantly outperform those trained individually. Our approach performs better than all baselines in reconstruction quality, and depth/intensity predictions with satisfactory rendering speed.

事件相机以其高动态范围和低延迟的优势备受推崇，非常适合光照条件复杂和快速移动的场景。然而，从原始事件流重建三维场景非常困难，因为事件数据稀疏且不包含绝对颜色信息。为了释放其在三维重建中的潜力，我们提出了第一个基于事件的可泛化三维重建框架，称为 EvGGS，它可以仅从事件输入中以前馈方式重建场景为三维高斯模型，并能在未见过的案例中泛化，无需重新训练。该框架包括深度估计模块、强度重建模块和高斯回归模块。这些子模块以级联方式连接，并通过设计的联合损失函数进行协同训练，以促进它们的相互提升。为了促进相关研究，我们构建了一个新颖的基于事件的三维数据集，包括各种材料的对象和经过校准的灰度图像、深度图、相机姿态和剪影标签。实验显示，联合训练的模型显著优于单独训练的模型。我们的方法在重建质量、深度/强度预测方面均优于所有基线，并且渲染速度令人满意。
