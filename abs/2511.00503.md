### Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models

We introduce Diff4Splat, a feed-forward method that synthesizes controllable and explicit 4D scenes from a single image. Our approach unifies the generative priors of video diffusion models with geometry and motion constraints learned from large-scale 4D datasets. Given a single input image, a camera trajectory, and an optional text prompt, Diff4Splat directly predicts a deformable 3D Gaussian field that encodes appearance, geometry, and motion, all in a single forward pass, without test-time optimization or post-hoc refinement. At the core of our framework lies a video latent transformer, which augments video diffusion models to jointly capture spatio-temporal dependencies and predict time-varying 3D Gaussian primitives. Training is guided by objectives on appearance fidelity, geometric accuracy, and motion consistency, enabling Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate the effectiveness of Diff4Splatacross video generation, novel view synthesis, and geometry extraction, where it matches or surpasses optimization-based methods for dynamic scene synthesis while being significantly more efficient.

我们提出了 Diff4Splat，一种前馈式方法，可从单张图像合成可控且显式的四维场景。该方法融合了视频扩散模型的生成先验与从大规模四维数据集中学习到的几何与运动约束。给定一张输入图像、一个相机轨迹以及可选的文本提示，Diff4Splat 能够在一次前向推理中直接预测一个可变形的三维高斯场，编码外观、几何和运动信息，无需测试时优化或后处理精修。我们框架的核心是一个视频潜变量变换器，它增强了视频扩散模型，使其能够联合捕捉时空依赖关系，并预测时变的三维高斯基元。训练过程中通过外观保真度、几何精度和运动一致性等目标进行引导，使 Diff4Splat 能在 30 秒内合成高质量的四维场景。我们在视频生成、新视角合成和几何提取等任务中展示了 Diff4Splat 的有效性，其性能与基于优化的方法相当甚至优于它们，同时具备显著更高的效率。
