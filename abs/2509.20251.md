### 4D Driving Scene Generation With Stereo Forcing

Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations.

当前的生成模型在合成动态四维驾驶场景时仍存在显著挑战，尤其是在无需针对每个场景进行单独优化的情况下，同时实现时间外推与空间新视角合成（NVS）仍然困难。生成与新视角合成之间的桥接问题尚未得到充分解决。本文提出了 PhiGenesis，一个统一的四维场景生成框架，将视频生成技术扩展至具备几何与时间一致性。给定多视角图像序列及相机参数，PhiGenesis 可沿目标三维轨迹生成时间连续的四维高斯溅射（4D Gaussian Splatting）表示。在第一阶段，PhiGenesis 利用预训练的视频 VAE，并引入新颖的视域适配器（range-view adapter），实现从多视角图像到四维场景的前向重建。该架构同时支持单帧或视频输入，输出包含几何、语义与运动信息的完整四维场景。在第二阶段，PhiGenesis 引入几何引导的视频扩散模型，利用历史渲染的四维场景作为先验，基于轨迹生成未来视图。为应对新视角中的几何曝光偏差，我们提出了一种名为立体约束（Stereo Forcing）的新型条件策略，在去噪过程中融合几何不确定性。该方法通过基于不确定性感知扰动动态调整生成影响，从而增强时间一致性。实验结果表明，我们的方法在外观与几何重建、时间生成以及新视角合成（NVS）任务上均达到了当前最先进的性能，同时在下游评测中也展现出优异的综合表现。
