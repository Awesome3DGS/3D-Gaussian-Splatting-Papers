### HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars

We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of hair and hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a few-shot manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.

我们提出了一种具有显式头发组合性的三维头部虚拟形象通用先验模型。现有用于构建可泛化三维头部虚拟形象先验的方法通常采用整体建模，将面部与头发视为不可分割的整体。这种方法忽视了人类头部固有的组合性，使得模型难以自然地解耦面部与头发的表示，尤其是在数据集有限的情况下。此外，这类整体模型在支持三维面部与发型交换等需要灵活可控的应用时也存在困难。为了解决这些问题，我们引入了一种显式建模面部与头发组合性的先验模型，分别学习它们的潜在空间。实现这一方法的关键是我们提出的合成无发数据生成流程，该流程利用来自扩散先验估计的无发几何与纹理，从影棚采集的数据集中去除头发。通过利用成对的有发与无发数据集，我们为面部与头发训练了解耦的先验模型，并将组合性作为归纳偏置以促进有效分离。我们模型固有的组合性使得在保持身份一致性的前提下，实现面部与头发组件在虚拟形象之间的无缝迁移。此外，我们还展示了该模型能够通过单目采集的少量样本进行快速微调，为未见过的对象生成高保真、具备头发组合性的三维头部虚拟形象。这些能力突显了我们方法在真实场景中的实用性，为灵活且富有表现力的三维虚拟形象生成铺平了道路。
