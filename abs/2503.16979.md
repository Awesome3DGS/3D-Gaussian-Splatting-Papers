### Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting

Building Free-Viewpoint Videos in a streaming manner offers the advantage of rapid responsiveness compared to offline training methods, greatly enhancing user experience. However, current streaming approaches face challenges of high per-frame reconstruction time (10s+) and error accumulation, limiting their broader application. In this paper, we propose Instant Gaussian Stream (IGS), a fast and generalizable streaming framework, to address these issues. First, we introduce a generalized Anchor-driven Gaussian Motion Network, which projects multi-view 2D motion features into 3D space, using anchor points to drive the motion of all Gaussians. This generalized Network generates the motion of Gaussians for each target frame in the time required for a single inference. Second, we propose a Key-frame-guided Streaming Strategy that refines each key frame, enabling accurate reconstruction of temporally complex scenes while mitigating error accumulation. We conducted extensive in-domain and cross-domain evaluations, demonstrating that our approach can achieve streaming with a average per-frame reconstruction time of 2s+, alongside a enhancement in view synthesis quality.

流式生成自由视角视频 相较于离线训练方法具备更快的响应速度，显著提升用户体验。然而，现有流式方法面临 单帧重建时间过长（10 秒以上） 以及 误差累积 的问题，限制了其广泛应用。为了解决这些问题，我们提出 Instant Gaussian Stream (IGS)，一个快速且具备泛化能力的流式框架。
首先，我们引入了广义的锚点驱动高斯运动网络 (Anchor-driven Gaussian Motion Network)，该网络将多视角 2D 运动特征投影到 3D 空间，并利用锚点 (anchor points) 驱动所有高斯点的运动。这一通用网络能够在单次推理的时间内预测每个目标帧的高斯运动。
其次，我们提出关键帧引导的流式策略 (Key-frame-guided Streaming Strategy)，通过精细化处理关键帧，精准重建时间复杂场景，同时缓解误差累积问题。
我们进行了大规模的域内和跨域评估，结果表明 IGS 能够实现流式处理，平均单帧重建时间降低至 2 秒级，同时提升新视角合成质量。
