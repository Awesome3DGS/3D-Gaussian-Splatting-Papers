### WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving

Recent advances in driving-scene generation and reconstruction have demonstrated significant potential for enhancing autonomous driving systems by producing scalable and controllable training data. Existing generation methods primarily focus on synthesizing diverse and high-fidelity driving videos; however, due to limited 3D consistency and sparse viewpoint coverage, they struggle to support convenient and high-quality novel-view synthesis (NVS). Conversely, recent 3D/4D reconstruction approaches have significantly improved NVS for real-world driving scenes, yet inherently lack generative capabilities. To overcome this dilemma between scene generation and reconstruction, we propose WorldSplat, a novel feed-forward framework for 4D driving-scene generation. Our approach effectively generates consistent multi-track videos through two key steps: (i) We introduce a 4D-aware latent diffusion model integrating multi-modal information to produce pixel-aligned 4D Gaussians in a feed-forward manner. (ii) Subsequently, we refine the novel view videos rendered from these Gaussians using a enhanced video diffusion model. Extensive experiments conducted on benchmark datasets demonstrate that WorldSplat effectively generates high-fidelity, temporally and spatially consistent multi-track novel view driving videos.

近年来，驾驶场景的生成与重建取得了显著进展，为自动驾驶系统提供可扩展、可控的训练数据展现出巨大潜力。现有的生成方法主要致力于合成多样化且高保真的驾驶视频，但由于缺乏充分的三维一致性与稀疏的视角覆盖，这些方法难以支持便捷且高质量的新视图合成（Novel View Synthesis, NVS）。相反，近期的三维/四维重建方法显著提升了真实驾驶场景下的 NVS 表现，但其本质上缺乏生成能力。为打破场景生成与重建之间的这一困境，我们提出了 **WorldSplat**，一种用于四维驾驶场景生成的新型前馈框架。该方法通过两个关键步骤有效生成时空一致的多轨道视频：（i）我们提出了一个具备四维感知能力的潜空间扩散模型（4D-aware latent diffusion model），融合多模态信息，以前馈方式生成像素对齐的四维高斯；（ii）随后，我们利用增强型视频扩散模型对由这些高斯渲染的新视角视频进行精细化优化。大量在基准数据集上的实验结果表明，WorldSplat 能够高效生成具有高保真度、时空一致性和多轨道特征的新视角驾驶视频。
