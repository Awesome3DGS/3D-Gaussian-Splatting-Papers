### EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis

Novel view synthesis of urban scenes is essential for autonomous driving-related applications. Existing NeRF and 3DGS-based methods show promising results in achieving photorealistic renderings but require slow, per-scene optimization. We introduce EVolSplat, an efficient 3D Gaussian Splatting model for urban scenes that works in a feed-forward manner. Unlike existing feed-forward, pixelaligned 3DGS methods, which often suffer from issues like multi-view inconsistencies and duplicated content, our approach predicts 3D Gaussians across multiple frames within a unified volume using a 3D convolutional network. This is achieved by initializing 3D Gaussians with noisy depth predictions, and then refining their geometric properties in 3D space and predicting color based on 2D textures. Our model also handles distant views and the sky with a flexible hemisphere background model. This enables us to perform fast, feed-forward reconstruction while achieving real-time rendering. Experimental evaluations on the KITTI-360 and Waymo datasets show that our method achieves state-of-the-art quality compared to existing feedforward 3DGS- and NeRF-based methods.

城市场景的新视角合成对于自动驾驶相关应用至关重要。尽管现有基于 NeRF 和 3D Gaussian Splatting（3DGS）的方法在实现真实感渲染方面表现出色，但它们通常依赖于缓慢的逐场景优化过程。我们提出了 EVolSplat，一种高效的城市场景 3D 高斯溅射模型，能够以前馈方式运行。不同于现有的前馈式、像素对齐的 3DGS 方法常常面临多视图不一致与内容重复等问题，EVolSplat 通过一个三维卷积网络，在统一体积内预测多个帧的 3D 高斯，从而避免这些问题。具体而言，我们首先利用带噪声的深度预测初始化 3D 高斯，随后在三维空间中对其几何属性进行精细调整，并根据二维纹理预测颜色。此外，我们还设计了灵活的半球背景建模机制，用于处理远距离视角和天空区域，使得系统能够在实现实时渲染的同时，完成快速的前馈式重建。
在 KITTI-360 和 Waymo 数据集上的实验评估表明，与现有前馈式 3DGS 和 NeRF 方法相比，EVolSplat 在图像质量方面达到了当前最优水平。
