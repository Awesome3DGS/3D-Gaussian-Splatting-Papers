### Video4DGen: Enhancing Video and 4D Generation through Mutual Optimization

The advancement of 4D (i.e., sequential 3D) generation opens up new possibilities for lifelike experiences in various applications, where users can explore dynamic objects or characters from any viewpoint. Meanwhile, video generative models are receiving particular attention given their ability to produce realistic and imaginative frames. These models are also observed to exhibit strong 3D consistency, indicating the potential to act as world simulators. In this work, we present Video4DGen, a novel framework that excels in generating 4D representations from single or multiple generated videos as well as generating 4D-guided videos. This framework is pivotal for creating high-fidelity virtual contents that maintain both spatial and temporal coherence. The 4D outputs generated by Video4DGen are represented using our proposed Dynamic Gaussian Surfels (DGS), which optimizes time-varying warping functions to transform Gaussian surfels (surface elements) from a static state to a dynamically warped state. We design warped-state geometric regularization and refinements on Gaussian surfels, to preserve the structural integrity and fine-grained appearance details. To perform 4D generation from multiple videos and capture representation across spatial, temporal, and pose dimensions, we design multi-video alignment, root pose optimization, and pose-guided frame sampling strategies. The leveraging of continuous warping fields also enables a precise depiction of pose, motion, and deformation over per-video frames. Further, to improve the overall fidelity from the observation of all camera poses, Video4DGen performs novel-view video generation guided by the 4D content, with the proposed confidence-filtered DGS to enhance the quality of generated sequences. With the ability of 4D and video generation, Video4DGen offers a powerful tool for applications in virtual reality, animation, and beyond.

随着 4D（即时序三维）生成技术的发展，人们在各类应用中得以实现更加真实的沉浸式体验，用户可以从任意视角探索动态物体或角色。同时，视频生成模型因其生成真实且富有想象力的画面能力而受到高度关注，这类模型也被观察到展现出良好的三维一致性，具备充当“世界模拟器”的潜力。
在本研究中，我们提出了 Video4DGen，这是一个全新框架，能够从单个或多个生成视频中构建 4D 表示，也可以用于生成受 4D 内容引导的视频。该框架对于创建在时空维度上均保持高度一致性的高保真虚拟内容具有关键意义。
Video4DGen 生成的 4D 输出采用我们提出的 动态高斯面元（Dynamic Gaussian Surfels, DGS） 表示形式。通过优化时变形变函数，DGS 将静态状态下的高斯面元转换为动态变形状态。我们设计了针对变形状态的几何正则化与外观细节优化机制，以保持结构完整性和高质量纹理表现。
为实现多视频驱动的 4D 生成，并捕捉跨空间、时间与姿态维度的一致表示，我们进一步提出了多视频对齐机制、根姿态优化策略以及基于姿态的帧采样方法。通过连续形变场的引入，系统可对每个视频中的姿态、运动与形变实现精细表达。
此外，为了提升从各视角观察下的整体真实感，Video4DGen 还支持基于 4D 内容的新视角视频生成，并引入 置信度过滤的 DGS（confidence-filtered DGS） 机制来提升合成序列的质量。
凭借其在 4D 与视频生成方面的能力，Video4DGen 为虚拟现实、动画等领域提供了一个功能强大的创作工具。
