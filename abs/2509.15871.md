### Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval

3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text prompts, which is essential for applications such as robotics. However, existing 3DVG methods encounter two main challenges: first, they struggle to handle the implicit representation of spatial textures in 3D Gaussian Splatting (3DGS), making per-scene training indispensable; second, they typically require larges amounts of labeled data for effective training. To this end, we propose Grounding via View Retrieval (GVR), a novel zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D retrieval task that leverages object-level view retrieval to collect grounding clues from multiple views, which not only avoids the costly process of 3D annotation, but also eliminates the need for per-scene training. Extensive experiments demonstrate that our method achieves state-of-the-art visual grounding performance while avoiding per-scene training, providing a solid foundation for zero-shot 3DVG research.

三维视觉指代（3D Visual Grounding, 3DVG）旨在根据文本提示在三维场景中定位目标物体，这对于机器人等应用具有重要意义。然而，现有的 3DVG 方法面临两大挑战：其一，难以处理三维高斯溅射（3D Gaussian Splatting, 3DGS）中的空间纹理隐式表示，导致必须进行逐场景训练；其二，通常需要大量标注数据才能实现有效训练。为此，我们提出了 **Grounding via View Retrieval (GVR)**，一种面向 3DGS 的零样本视觉指代新框架。该方法将 3DVG 转化为二维检索任务，通过基于对象级视图检索从多个视角收集指代线索，从而既避免了昂贵的三维标注过程，也无需逐场景训练。大量实验结果表明，我们的方法在无需逐场景训练的情况下仍能实现当前最优的视觉指代性能，为零样本 3DVG 研究提供了坚实基础。
