### TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos

In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural nets, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. We propose a new framework named TRACE to model the motion physics of complex dynamic 3D scenes. The key novelty of our method is that, by formulating each 3D point as a rigid particle with size and orientation in space, we directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particle's motion over time. Extensive experiments on three existing dynamic datasets and one newly created challenging synthetic datasets demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters.

本文旨在在没有任何人工标注的情况下，仅通过动态多视角视频来建模三维场景的几何、外观和物理信息。现有方法通常通过将物理约束损失作为软约束，或将简单物理模型集成到神经网络中，但往往无法有效学习复杂的运动物理，或者需要额外的标注（如物体类型或掩码）。我们提出了一个新的框架 TRACE，用于建模复杂动态三维场景的运动物理。该方法的关键创新在于：将每个三维点视为空间中具有大小和方向的刚体粒子，直接为每个粒子学习一个平移-旋转动力学系统，并显式估计一整套物理参数，以控制粒子随时间的运动。我们在三个现有的动态数据集和一个新构建的具有挑战性的合成数据集上进行了大量实验，结果表明该方法在未来帧外推任务中表现远超基线方法。该框架的一个良好特性是：只需对学习到的物理参数进行聚类，就能轻松地实现对多个物体或部分的分割。
