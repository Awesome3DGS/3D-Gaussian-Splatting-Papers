### RI3D: Few-Shot Gaussian Splatting With Repair and Inpainting Diffusion Priors

In this paper, we propose RI3D, a novel 3DGS-based approach that harnesses the power of diffusion models to reconstruct high-quality novel views given a sparse set of input images. Our key contribution is separating the view synthesis process into two tasks of reconstructing visible regions and hallucinating missing regions, and introducing two personalized diffusion models, each tailored to one of these tasks. Specifically, one model ('repair') takes a rendered image as input and predicts the corresponding high-quality image, which in turn is used as a pseudo ground truth image to constrain the optimization. The other model ('inpainting') primarily focuses on hallucinating details in unobserved areas. To integrate these models effectively, we introduce a two-stage optimization strategy: the first stage reconstructs visible areas using the repair model, and the second stage reconstructs missing regions with the inpainting model while ensuring coherence through further optimization. Moreover, we augment the optimization with a novel Gaussian initialization method that obtains per-image depth by combining 3D-consistent and smooth depth with highly detailed relative depth. We demonstrate that by separating the process into two tasks and addressing them with the repair and inpainting models, we produce results with detailed textures in both visible and missing regions that outperform state-of-the-art approaches on a diverse set of scenes with extremely sparse inputs.

在本文中，我们提出了 RI3D，这是一种基于 3D 高斯溅射（3DGS）的方法，利用扩散模型的力量，在给定稀疏输入图像的情况下重建高质量的新视角。我们的关键贡献是将视角合成过程分为重建可见区域和虚拟缺失区域两个任务，并引入了两个个性化的扩散模型，每个模型针对其中一个任务进行了优化。具体来说，一个模型（“修复”）以渲染图像为输入，预测相应的高质量图像，该图像进一步作为伪地面真实图像用于约束优化。另一个模型（“修复”）主要集中在虚拟未观察到区域的细节。为了有效地集成这些模型，我们引入了两阶段优化策略：第一阶段使用修复模型重建可见区域，第二阶段使用修复模型重建缺失区域，并确保通过进一步优化保持一致性。此外，我们通过一种新的高斯初始化方法增强了优化过程，该方法通过将 3D 一致性和平滑深度与高度详细的相对深度结合来获取每个图像的深度。我们证明，通过将过程分为两个任务，并通过修复和修复模型解决它们，我们可以在可见和缺失区域中生成细致纹理的结果，在极其稀疏输入的多样场景上超越了最先进的技术。
