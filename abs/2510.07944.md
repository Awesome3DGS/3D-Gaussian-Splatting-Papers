### CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving

Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.

生成模型已被广泛应用于世界建模任务，如环境模拟与未来状态预测。随着自动驾驶技术的发展，系统不仅需要在多种控制条件下生成高保真视频，还需产出如深度估计等多样且有意义的信息。为此，我们提出了 CVD-STORM，一种基于跨视角视频扩散的模型，结合时空重建变分自编码器（VAE），能够在多种控制输入下生成具有四维重建能力的长时多视角视频。我们的方法首先通过辅助的四维重建任务对 VAE 进行微调，从而增强其对三维结构与时间动态的编码能力；随后，将该 VAE 融入视频扩散生成流程中，显著提升视频生成质量。实验结果表明，CVD-STORM 在 FID 与 FVD 指标上均取得了显著提升。此外，联合训练的高斯泼溅解码器可有效重建动态场景，提供有价值的几何信息，助力全面的场景理解。
