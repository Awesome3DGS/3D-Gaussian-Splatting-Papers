### No Parameters, No Problem: 3D Gaussian Splatting without Camera Intrinsics and Extrinsics

While 3D Gaussian Splatting (3DGS) has made significant progress in scene reconstruction and novel view synthesis, it still heavily relies on accurately pre-computed camera intrinsics and extrinsics, such as focal length and camera poses. In order to mitigate this dependency, the previous efforts have focused on optimizing 3DGS without the need for camera poses, yet camera intrinsics remain necessary. To further loose the requirement, we propose a joint optimization method to train 3DGS from an image collection without requiring either camera intrinsics or extrinsics. To achieve this goal, we introduce several key improvements during the joint training of 3DGS. We theoretically derive the gradient of the camera intrinsics, allowing the camera intrinsics to be optimized simultaneously during training. Moreover, we integrate global track information and select the Gaussian kernels associated with each track, which will be trained and automatically rescaled to an infinitesimally small size, closely approximating surface points, and focusing on enforcing multi-view consistency and minimizing reprojection errors, while the remaining kernels continue to serve their original roles. This hybrid training strategy nicely unifies the camera parameters estimation and 3DGS training. Extensive evaluations demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on both public and synthetic datasets.

尽管 3D Gaussian Splatting (3DGS) 在新视角合成方面表现出色，但它仍然依赖于精确的预计算相机参数，而这些参数往往难以获取且容易受到噪声影响。此前的无 COLMAP（COLMAP-Free） 方法通过局部约束优化相机位姿，但在复杂场景中常常表现不佳。
为了解决这一问题，我们提出 TrackGS，通过特征轨迹（feature tracks） 对多视角几何关系施加全局约束。我们选择与每条轨迹关联的高斯分布，并在训练过程中缩放至无限小尺寸，以确保空间精度。此外，我们同时最小化重投影误差（reprojection error）和反投影误差（backprojection error），以增强几何一致性。
此外，我们推导了相机内参的梯度计算，从而在 3DGS 训练过程中对相机参数进行联合优化，构建了一个端到端的优化框架。在包含剧烈相机运动的挑战性数据集上，我们的方法达到了**最先进（SOTA）**的性能。
