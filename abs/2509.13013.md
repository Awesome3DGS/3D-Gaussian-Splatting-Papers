### Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image

With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.

随着三维表示技术与生成模型的快速发展，从单张图像重建全身三维头像的研究取得了显著进展。然而，由于单目输入所提供的信息有限，该任务在本质上仍是不适定问题，使得在生成过程中难以有效控制被遮挡区域的几何形状与纹理外观。为应对这一挑战，我们重新设计了重建流程，提出了 **Dream3DAvatar**——一个高效且可文本控制的两阶段三维头像生成框架。第一阶段中，我们构建了一个轻量化、适配器增强的多视角生成模型。具体而言，我们引入 **Pose-Adapter**，将 SMPL-X 渲染与骨骼信息注入到 SDXL 模型中，以确保多视角间的几何与姿态一致性。为了保持面部身份特征，我们进一步设计 **ID-Adapter-G**，在生成过程中注入高分辨率的面部特征。此外，我们利用 **BLIP2** 自动生成多视角图像的高质量文本描述，从而增强在被遮挡区域的文本驱动可控性。在第二阶段，我们设计了一个具备多视角特征融合模块的前馈式 Transformer 模型，以从生成图像中重建高保真的三维高斯溅射表示（3DGS）。此外，我们提出 **ID-Adapter-R**，通过门控机制将面部特征有效融合到重建过程中，从而提升高频细节的恢复能力。大量实验表明，我们的方法能够在无需后处理的情况下生成逼真且可直接用于动画制作的三维头像，并在多项评测指标上持续优于现有基线方法。
