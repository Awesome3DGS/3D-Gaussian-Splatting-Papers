### RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable 3D Gaussian Avatars

Modeling animatable human avatars from monocular or multi-view videos has been widely studied, with recent approaches leveraging neural radiance fields (NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in novel-view and novel-pose synthesis. However, existing methods often struggle to accurately capture the dynamics of loose clothing, as they primarily rely on global pose conditioning or static per-frame representations, leading to oversmoothing and temporal inconsistencies in non-rigid regions. To address this, We propose RealityAvatar, an efficient framework for high-fidelity digital human modeling, specifically targeting loosely dressed avatars. Our method leverages 3D Gaussian Splatting to capture complex clothing deformations and motion dynamics while ensuring geometric consistency. By incorporating a motion trend module and a latentbone encoder, we explicitly model pose-dependent deformations and temporal variations in clothing behavior. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach in capturing fine-grained clothing deformations and motion-driven shape variations. Our method significantly enhances structural fidelity and perceptual quality in dynamic human reconstruction, particularly in non-rigid regions, while achieving better consistency across temporal frames.

从单目或多视角视频中建模可动画的人体角色是一个被广泛研究的问题，近期的一些方法借助神经辐射场（Neural Radiance Fields, NeRF）或三维高斯喷洒（3D Gaussian Splatting, 3DGS）在新视角与新姿态合成方面取得了显著成果。然而，现有方法在建模宽松衣物的动态变化方面仍面临挑战：它们主要依赖全局姿态条件或逐帧静态表示，难以准确捕捉非刚性区域的细节，容易导致过度平滑和时间不一致问题。
为此，我们提出了 RealityAvatar，一种高效、高保真的数字人建模框架，专为穿着宽松衣物的角色设计。该方法基于三维高斯喷洒，有效捕捉复杂的衣物变形与运动动态，同时保持几何一致性。
我们引入了 **运动趋势模块（motion trend module）**与 隐骨编码器（latentbone encoder），显式建模姿态驱动下的衣物形变与时间变化行为，从而增强对动态特征的建模能力。
在多个基准数据集上的大量实验证明，我们的方法在捕捉细粒度衣物变形与运动驱动的形态变化方面表现出色，显著提升了动态人体重建中非刚性区域的结构保真度与感知质量，同时具备更好的时间一致性。
