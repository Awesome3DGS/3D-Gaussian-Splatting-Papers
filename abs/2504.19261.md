### Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting

Scene view synthesis, which generates novel views from limited perspectives, is increasingly vital for applications like virtual reality, augmented reality, and robotics. Unlike object-based tasks, such as generating 360° views of a car, scene view synthesis handles entire environments where non-uniform observations pose unique challenges for stable rendering quality. To address this issue, we propose a novel approach: renderability field-guided gaussian splatting (RF-GS). This method quantifies input inhomogeneity through a renderability field, guiding pseudo-view sampling to enhanced visual consistency. To ensure the quality of wide-baseline pseudo-views, we train an image restoration model to map point projections to visible-light styles. Additionally, our validated hybrid data optimization strategy effectively fuses information of pseudo-view angles and source view textures. Comparative experiments on simulated and real-world data show that our method outperforms existing approaches in rendering stability.

场景视图合成（Scene View Synthesis）旨在从有限视角生成新视图，对于虚拟现实、增强现实和机器人等应用日益重要。与“物体生成”任务（如生成汽车的 360° 视图）不同，场景视图合成处理的是整个环境，其非均匀的观测数据带来了渲染质量稳定性方面的独特挑战。
为解决这一问题，我们提出一种新方法：可渲染性场引导的高斯投影（Renderability Field-Guided Gaussian Splatting, RF-GS）。该方法通过构建可渲染性场对输入数据的非均匀性进行量化，从而引导伪视图采样，提升视觉一致性。为了保证宽基线伪视图的图像质量，我们训练了一个图像恢复模型，将点投影映射为可见光风格的图像。此外，我们提出并验证了一种混合数据优化策略，能够有效融合伪视角与源视图纹理信息。
在模拟与真实数据集上的对比实验表明，该方法在渲染稳定性方面优于现有技术。
