### BulletGen: Improving 4D Reconstruction with Bullet-Time Generation

Transforming casually captured, monocular videos into fully immersive dynamic experiences is a highly ill-posed task, and comes with significant challenges, e.g., reconstructing unseen regions, and dealing with the ambiguity in monocular depth estimation. In this work we introduce BulletGen, an approach that takes advantage of generative models to correct errors and complete missing information in a Gaussian-based dynamic scene representation. This is done by aligning the output of a diffusion-based video generation model with the 4D reconstruction at a single frozen "bullet-time" step. The generated frames are then used to supervise the optimization of the 4D Gaussian model. Our method seamlessly blends generative content with both static and dynamic scene components, achieving state-of-the-art results on both novel-view synthesis, and 2D/3D tracking tasks.

将随意拍摄的单目视频转换为完全沉浸式的动态体验是一项高度病态的问题，并面临诸多挑战，例如重建不可见区域以及处理单目深度估计中的歧义。本文提出 **BulletGen**，一种利用生成模型来纠正误差并补全基于高斯的动态场景表示中缺失信息的方法。具体而言，我们将扩散式视频生成模型的输出与四维重建在某个冻结的“子弹时间”帧对齐，然后利用生成的帧来监督四维高斯模型的优化。我们的方法能够将生成内容与静态和动态场景组件无缝融合，在新视角合成以及二维/三维跟踪任务中均取得了当前最优的结果。
