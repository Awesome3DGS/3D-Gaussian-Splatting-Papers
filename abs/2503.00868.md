### Vid2Fluid: 3D Dynamic Fluid Assets from Single-View Videos with Generative Gaussian Splatting

The generation of 3D content from single-view images has been extensively studied, but 3D dynamic scene generation with physical consistency from videos remains in its early stages. We propose a novel framework leveraging generative 3D Gaussian Splatting (3DGS) models to extract 3D dynamic fluid objects from single-view videos. The fluid geometry represented by 3DGS is initially generated from single-frame images, then denoised, densified, and aligned across frames. We estimate the fluid surface velocity using optical flow and compute the mainstream of the fluid to refine it. The 3D volumetric velocity field is then derived from the enclosed surface. The velocity field is then converted into a divergence-free, grid-based representation, enabling the optimization of simulation parameters through its differentiability across frames. This process results in simulation-ready fluid assets with physical dynamics closely matching those observed in the source video. Our approach is applicable to various fluid types, including gas, liquid, and viscous fluids, and allows users to edit the output geometry or extend movement durations seamlessly. Our automatic method for creating 3D dynamic fluid assets from single-view videos, easily obtainable from the internet, shows great potential for generating large-scale 3D fluid assets at a low cost.

从单视角图像生成 3D 内容已被广泛研究，但具有物理一致性的 3D 动态场景生成仍处于早期阶段。在本文中，我们提出了一种新颖的框架，利用生成式 3D Gaussian Splatting (3DGS) 模型，从单视角视频中提取3D 动态流体对象。
首先，我们通过 3DGS 生成流体几何表示，初始形态由单帧图像生成，随后进行去噪、密集化和跨帧对齐。接着，我们利用光流（optical flow） 估算流体表面速度，并计算流体的主流方向（mainstream） 以优化流体形态。随后，我们基于封闭的流体表面推导3D 体速度场（3D volumetric velocity field），并将其转换为无散度的网格化表示（divergence-free, grid-based representation），从而在跨帧优化过程中保持可微分性，实现仿真参数优化。
这一过程生成的流体资产具备可模拟的物理动态，并与源视频中的流体运动高度匹配。我们的方法适用于气体、液体和粘性流体等多种流体类型，同时支持用户编辑几何形态或无缝扩展运动持续时间。该方法可自动从互联网获取单视角视频，生成大规模 3D 动态流体资产，极大降低了成本，展现出广阔的应用前景。
