### GaussianFormer3D: Multi-Modal Gaussian-based Semantic Occupancy Prediction with 3D Deformable Attention

3D semantic occupancy prediction is critical for achieving safe and reliable autonomous driving. Compared to camera-only perception systems, multi-modal pipelines, especially LiDAR-camera fusion methods, can produce more accurate and detailed predictions. Although most existing works utilize a dense grid-based representation, in which the entire 3D space is uniformly divided into discrete voxels, the emergence of 3D Gaussians provides a compact and continuous object-centric representation. In this work, we propose a multi-modal Gaussian-based semantic occupancy prediction framework utilizing 3D deformable attention, named as GaussianFormer3D. We introduce a voxel-to-Gaussian initialization strategy to provide 3D Gaussians with geometry priors from LiDAR data, and design a LiDAR-guided 3D deformable attention mechanism for refining 3D Gaussians with LiDAR-camera fusion features in a lifted 3D space. We conducted extensive experiments on both on-road and off-road datasets, demonstrating that our GaussianFormer3D achieves high prediction accuracy that is comparable to state-of-the-art multi-modal fusion-based methods with reduced memory consumption and improved efficiency.

三维语义占据预测对于实现安全可靠的自动驾驶至关重要。相比于仅使用摄像头的感知系统，多模态管线，特别是激光雷达-摄像头融合方法，能够生成更准确且更具细节的预测结果。尽管现有大多数方法采用稠密的网格表示方式，将整个三维空间均匀划分为离散体素，但随着三维高斯的出现，提供了一种紧凑、连续、以目标为中心的表示方式。
在本研究中，我们提出了一种基于多模态高斯表示的语义占据预测框架，命名为 GaussianFormer3D，该框架结合了三维可变形注意力机制。我们引入了一种从体素到高斯的初始化策略，使三维高斯能够从激光雷达数据中获得几何先验信息；并设计了一种激光雷达引导的三维可变形注意力机制，用于在提升至三维空间后融合激光雷达与摄像头的特征，从而精细化高斯表示。
我们在道路场景和越野场景的多个数据集上进行了大量实验，结果表明 GaussianFormer3D 在保持与现有最先进多模态融合方法相当的预测精度的同时，显著降低了内存占用，并提升了整体效率。
