### Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings

Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive "bags of embeddings" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise.

三维新视角合成（Novel View Synthesis）随着三维高斯溅射（3D Gaussian Splatting, 3DGS）的发展取得了显著突破，实现了实时的照片级渲染。然而，高斯溅射固有的模糊性对三维场景理解带来了挑战，从而限制了其在增强现实（AR）、虚拟现实（VR）以及机器人等领域的广泛应用。尽管近期一些工作尝试通过二维基础模型蒸馏（2D foundation model distillation）来学习语义信息，但这类方法存在根本性缺陷：α混合会在多个物体间平均语义，使得实现三维层面的语义理解成为不可能。为此，我们提出了一种**全新的范式转变式方法**，完全绕过语义的可微渲染过程。我们的核心思想是利用预分解的物体级高斯表示（object-level Gaussians），并通过多视角CLIP特征聚合构建出全面的“特征嵌入包（bags of embeddings）”，从整体上描述每个物体。这一设计带来了两大能力：（1）通过将文本查询与物体级（而非高斯级）嵌入进行匹配，实现精确的开放词汇物体检索；（2）实现任务的无缝适配：可将物体ID传播到像素层用于二维分割，或传播到高斯层用于三维提取。实验结果表明，我们的方法有效克服了三维开放词汇物体提取的核心难题，同时在二维开放词汇分割任务上保持与当前最先进方法相当的性能，实现了性能与灵活性的双重平衡。
