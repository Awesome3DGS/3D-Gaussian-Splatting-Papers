### FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using Gaussian Splatting with Depth-Feature Consistency

Recently, Gaussian Splatting has sparked a new trend in the field of computer vision. Apart from novel view synthesis, it has also been extended to the area of multi-view reconstruction. The latest methods facilitate complete, detailed surface reconstruction while ensuring fast training speed. However, these methods still require dense input views, and their output quality significantly degrades with sparse views. We observed that the Gaussian primitives tend to overfit the few training views, leading to noisy floaters and incomplete reconstruction surfaces. In this paper, we present an innovative sparse-view reconstruction framework that leverages intra-view depth and multi-view feature consistency to achieve remarkably accurate surface reconstruction. Specifically, we utilize monocular depth ranking information to supervise the consistency of depth distribution within patches and employ a smoothness loss to enhance the continuity of the distribution. To achieve finer surface reconstruction, we optimize the absolute position of depth through multi-view projection features. Extensive experiments on DTU and BlendedMVS demonstrate that our method outperforms state-of-the-art methods with a speedup of 60x to 200x, achieving swift and fine-grained mesh reconstruction without the need for costly pre-training.

最近，高斯点绘制（Gaussian Splatting）在计算机视觉领域掀起了一股新潮流。除了用于新视图合成外，它还被扩展到多视角重建领域。最新的方法能够实现完整且细致的表面重建，同时保证较快的训练速度。然而，这些方法仍然需要密集的输入视图，当视图稀疏时，输出质量会显著下降。我们观察到，高斯原语在稀疏视图训练时容易过拟合，导致噪声浮点和不完整的重建表面。
本文提出了一种创新的稀疏视图重建框架，通过利用视内深度和多视角特征一致性，实现了高度精确的表面重建。具体而言，我们利用单目深度排序信息监督补丁内的深度分布一致性，并引入平滑损失以增强分布的连续性。为了实现更精细的表面重建，我们通过多视角投影特征优化深度的绝对位置。
在 DTU 和 BlendedMVS 数据集上的大量实验表明，所提出的方法在速度上比当前最先进的方法快 60 倍至 200 倍，同时无需昂贵的预训练即可实现快速且细粒度的网格重建。这一方法显著提升了稀疏视图重建的精度和效率。
