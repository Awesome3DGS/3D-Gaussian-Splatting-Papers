### HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation

The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.

扩散模型的迅猛发展为虚拟现实（VR）和增强现实（AR）技术的应用带来了变革性潜力，而这类应用通常依赖于场景级的四维（4D）资产以实现沉浸式体验。然而，现有扩散模型大多聚焦于静态三维场景或物体级动态建模，限制了其在构建真正沉浸式体验方面的能力。
为解决这一问题，我们提出 HoloTime 框架，该框架融合视频扩散模型，从单一提示或参考图像生成全景视频，并配套一套 360 度 4D 场景重建方法，可将生成的全景视频无缝转换为 4D 资产，从而为用户带来完整沉浸式的 4D 体验。
具体而言，为了使视频扩散模型能够生成高保真全景视频，我们引入了 360World 数据集，这是首个适用于下游 4D 场景重建任务的全景视频综合数据集。在该数据集基础上，我们提出了 Panoramic Animator，一种两阶段的图像到视频扩散模型，能够将全景图像转换为高质量的全景视频。
随后，我们提出 Panoramic Space-Time Reconstruction，该方法利用时空深度估计技术将生成的全景视频转换为 4D 点云，并进一步优化得到时空一致的整体 4D Gaussian Splatting 表示，从而实现空间与时间维度一致的 4D 场景重建。
为验证方法有效性，我们与现有方法进行了对比分析，结果表明无论是在全景视频生成还是在 4D 场景重建方面，HoloTime 均展现出显著优势。这一成果证明了我们的方法能够创造更具吸引力和真实感的沉浸式环境，进而提升 VR 与 AR 应用中的用户体验。
