### Bootstrap 3D Reconstructed Scenes from 3D Gaussian Splatting

Recent developments in neural rendering techniques have greatly enhanced the rendering of photo-realistic 3D scenes across both academic and commercial fields. The latest method, known as 3D Gaussian Splatting (3D-GS), has set new benchmarks for rendering quality and speed. Nevertheless, the limitations of 3D-GS become pronounced in synthesizing new viewpoints, especially for views that greatly deviate from those seen during training. Additionally, issues such as dilation and aliasing arise when zooming in or out. These challenges can all be traced back to a single underlying issue: insufficient sampling. In our paper, we present a bootstrapping method that significantly addresses this problem. This approach employs a diffusion model to enhance the rendering of novel views using trained 3D-GS, thereby streamlining the training process. Our results indicate that bootstrapping effectively reduces artifacts, as well as clear enhancements on the evaluation metrics. Furthermore, we show that our method is versatile and can be easily integrated, allowing various 3D reconstruction projects to benefit from our approach.

近期在神经渲染技术方面的发展显著提升了学术和商业领域中对于逼真3D场景的渲染效果。最新的方法，称为三维高斯喷溅（3D-GS），已经为渲染质量和速度树立了新的标准。然而，在合成新视点时，特别是对于在训练期间很少见到的视点，3D-GS的限制变得明显。此外，在放大或缩小时还会出现膨胀和混叠的问题。这些挑战都可以追溯到一个基本问题：采样不足。在我们的论文中，我们提出了一种显著解决这个问题的引导方法。这种方法采用扩散模型来增强使用训练过的3D-GS渲染新视角，从而简化了训练过程。我们的结果表明，引导方法有效减少了伪影，并在评估指标上取得了明显的提升。此外，我们还展示了我们的方法具有多功能性，可以轻松集成，使各种3D重建项目都能从我们的方法中受益。
