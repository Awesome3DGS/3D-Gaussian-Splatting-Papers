### GASPACHO: Gaussian Splatting for Controllable Humans and Objects

We present GASPACHO: a method for generating photorealistic controllable renderings of human-object interactions. Given a set of multi-view RGB images of human-object interactions, our method reconstructs animatable templates of the human and object as separate sets of Gaussians simultaneously. Different from existing work, which focuses on human reconstruction and ignores objects as background, our method explicitly reconstructs both humans and objects, thereby allowing for controllable renderings of novel human object interactions in different poses from novel-camera viewpoints. During reconstruction, we constrain the Gaussians that generate rendered images to be a linear function of a set of canonical Gaussians. By simply changing the parameters of the linear deformation functions after training, our method can generate renderings of novel human-object interaction in novel poses from novel camera viewpoints. We learn the 3D Gaussian properties of the canonical Gaussians on the underlying 2D manifold of the canonical human and object templates. This in turns requires a canonical object template with a fixed UV unwrapping. To define such an object template, we use a feature based representation to track the object across the multi-view sequence. We further propose an occlusion aware photometric loss that allows for reconstructions under significant occlusions. Several experiments on two human-object datasets - BEHAVE and DNA-Rendering - demonstrate that our method allows for high-quality reconstruction of human and object templates under significant occlusion and the synthesis of controllable renderings of novel human-object interactions in novel human poses from novel camera views.

我们提出 GASPACHO，一种用于生成逼真且可控的人-物交互渲染的方法。给定一组多视角 RGB 图像，我们的方法能够同时重建可动画的人体和物体模板，将它们作为两个独立的高斯集合进行建模。不同于现有方法仅关注人体重建并将物体视为背景，我们的方法显式重建人和物体，从而实现从新视角、不同姿态下的人-物交互可控渲染。
在重建过程中，我们约束用于渲染图像的高斯点，使其是一组标准高斯点（canonical Gaussians）的线性函数。在训练完成后，仅需调整线性变形函数的参数，我们的方法即可在新视角、不同人体姿态下生成新的人-物交互渲染。我们在标准人体与物体模板的二维流形上学习标准高斯点的 3D 属性。这一过程需要定义一个固定 UV 展开的标准物体模板，为此，我们采用基于特征的表示来跟踪物体在多视角序列中的变化。此外，我们提出了一种遮挡感知的光度损失（occlusion-aware photometric loss），使得方法能够在严重遮挡的情况下进行重建。
在 BEHAVE 和 DNA-Rendering 两个人-物交互数据集上的实验表明，我们的方法能够在显著遮挡条件下实现高质量的人体与物体模板重建，并能生成可控的人-物交互渲染，支持新的人体姿态和新视角。
