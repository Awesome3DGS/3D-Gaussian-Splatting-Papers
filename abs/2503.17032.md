### TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting

Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we "bake" the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro.

逼真的 3D 全身语音驱动虚拟人 在增强现实 (AR) 领域具有广阔的应用前景，涵盖 电商直播、全息通信 等场景。尽管 3D 高斯散点 (3DGS) 在逼真化虚拟人生成方面取得了进展，但现有方法在全身语音驱动任务中仍面临 面部表情与身体动作的精细控制难题，并且细节不足，难以在移动设备上实时运行。
我们提出 TaoAvatar，一种基于 3DGS 的高保真、轻量化全身语音驱动虚拟人，能够由多种信号驱动。我们的方法首先创建一个个性化的着衣人体参数化模板，将高斯点绑定至该模板以表示外观。随后，我们预训练一个基于 StyleUnet 的网络 来处理复杂的依赖姿态的非刚性形变，该方法能够捕捉高频外观细节，但计算资源需求较高，不适用于移动设备。
为了解决这一问题，我们利用蒸馏技术 将非刚性形变“烘焙”到一个轻量级的 MLP 网络 中，并开发混合变形 (blend shapes) 机制以补偿细节损失。大量实验表明，TaoAvatar 在保持最先进渲染质量的同时，能够在多种设备上实时运行，并在 Apple Vision Pro 等高分辨率双目设备上达到 90 FPS。
