### LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds

Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits their generalization ability. Conversely, optimization-based video methods achieve higher fidelity but demand controlled capture conditions and computationally intensive refinement processes. Motivated by the emergence of large reconstruction models for efficient static reconstruction, we propose LHM (Large Animatable Human Reconstruction Model) to infer high-fidelity avatars represented as 3D Gaussian splatting in a feed-forward pass. Our model leverages a multimodal transformer architecture to effectively encode the human body positional features and image features with attention mechanism, enabling detailed preservation of clothing geometry and texture. To further boost the face identity preservation and fine detail recovery, we propose a head feature pyramid encoding scheme to aggregate multi-scale features of the head regions. Extensive experiments demonstrate that our LHM generates plausible animatable human in seconds without post-processing for face and hands, outperforming existing methods in both reconstruction accuracy and generalization ability.

从单张图像重建可动画的 3D 人体是一个具有挑战性的问题，因为在解耦几何形状、外观和变形时存在模糊性。近年来，3D 人体重建的进展主要集中在静态人体建模上，并且依赖使用合成 3D 扫描进行训练，限制了其泛化能力。相反，基于优化的视频方法能够实现更高的逼真度，但需要受控的拍摄条件并且计算上非常密集。受高效静态重建的大型重建模型的启发，我们提出了 LHM（大型可动画人体重建模型），通过前馈传递推断出以 3D 高斯溅射表示的高保真化身。我们的模型利用多模态 Transformer 架构，有效地编码人体的位置信息特征和图像特征，并结合注意力机制，能够细致地保留服装几何形状和纹理。为了进一步增强面部身份的保留和细节恢复，我们提出了一种头部特征金字塔编码方案，聚合头部区域的多尺度特征。大量实验表明，我们的 LHM 在没有后处理的情况下能够在几秒钟内生成可信的可动画人体，包括面部和手部，且在重建精度和泛化能力上均优于现有方法。
