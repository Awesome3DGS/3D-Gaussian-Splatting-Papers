### SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields

Holistic 3D scene understanding, which jointly models geometry, appearance, and semantics, is crucial for applications like augmented reality and robotic interaction. Existing feed-forward 3D scene understanding methods (e.g., LSM) are limited to extracting language-based semantics from scenes, failing to achieve holistic scene comprehension. Additionally, they suffer from low-quality geometry reconstruction and noisy artifacts. In contrast, per-scene optimization methods rely on dense input views, which reduces practicality and increases complexity during deployment. In this paper, we propose SemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which unifies 3D Gaussians with latent semantic attributes for joint geometry-appearance-semantics modeling. To predict the semantic anisotropic Gaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a cost volume representation that stores cross-view feature similarities, enhancing coherent and accurate scene comprehension. Leveraging a two-stage distillation framework, SemanticSplat reconstructs a holistic multi-modal semantic feature field from sparse-view images. Experiments demonstrate the effectiveness of our method for 3D scene understanding tasks like promptable and open-vocabulary segmentation.

整体三维场景理解（Holistic 3D Scene Understanding）需要在统一框架下同时建模几何、外观与语义，对于增强现实、机器人交互等应用至关重要。现有前馈式三维场景理解方法（如 LSM）仅限于从场景中提取基于语言的语义信息，无法实现真正的整体场景理解；同时，其几何重建质量较低且存在噪声伪影。相比之下，逐场景优化方法依赖稠密输入视角，降低了实用性并增加了部署复杂度。为此，本文提出 **SemanticSplat**，一种前馈式语义感知三维重建方法，将三维高斯与潜在语义属性结合，实现几何—外观—语义的联合建模。为了预测语义各向异性高斯，**SemanticSplat** 融合了多种特征场（如 LSeg、SAM）与存储跨视角特征相似性的代价体表示（Cost Volume Representation），从而提升场景理解的一致性与准确性。借助双阶段蒸馏框架，**SemanticSplat** 能够从稀疏视角图像中重建完整的多模态语义特征场。实验结果表明，我们的方法在可提示分割（Promptable Segmentation）与开放词汇分割（Open-vocabulary Segmentation）等三维场景理解任务中表现优异。
