### VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors

Neural rendering-based urban scene reconstruction methods commonly rely on images collected from driving vehicles with cameras facing and moving forward. Although these methods can successfully synthesize from views similar to training camera trajectory, directing the novel view outside the training camera distribution does not guarantee on-par performance. In this paper, we tackle the Extrapolated View Synthesis (EVS) problem by evaluating the reconstructions on views such as looking left, right or downwards with respect to training camera distributions. To improve rendering quality for EVS, we initialize our model by constructing dense LiDAR map, and propose to leverage prior scene knowledge such as surface normal estimator and large-scale diffusion model. Qualitative and quantitative comparisons demonstrate the effectiveness of our methods on EVS. To the best of our knowledge, we are the first to address the EVS problem in urban scene reconstruction.

基于神经渲染的城市场景重建方法通常依赖于从驾驶车辆上采集的图像，摄像头面向前方移动。虽然这些方法能够成功地合成与训练相似视角的图像，但是指向训练摄像头分布之外的新视角，并不能保证同等水平的性能。在本文中，我们解决了“外推视角合成（EVS）”问题，通过评估在不同于训练摄像头分布的视角下的重建效果，例如向左、向右或向下查看。为了改善EVS的渲染质量，我们通过构建密集的激光雷达地图来初始化模型，并提出利用场景先验知识，如表面法线估计器和大规模扩散模型。定性和定量比较显示了我们方法在EVS上的有效性。据我们所知，我们是首个解决城市场景重建中EVS问题的研究工作。

