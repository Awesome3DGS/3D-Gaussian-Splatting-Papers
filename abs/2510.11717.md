### Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams

Event cameras offer various advantages for novel view rendering compared to synchronously operating RGB cameras, and efficient event-based techniques supporting rigid scenes have been recently demonstrated in the literature. In the case of non-rigid objects, however, existing approaches additionally require sparse RGB inputs, which can be a substantial practical limitation; it remains unknown if similar models could be learned from event streams only. This paper sheds light on this challenging open question and introduces Ev4DGS, i.e., the first approach for novel view rendering of non-rigidly deforming objects in the explicit observation space (i.e., as RGB or greyscale images) from monocular event streams. Our method regresses a deformable 3D Gaussian Splatting representation through 1) a loss relating the outputs of the estimated model with the 2D event observation space, and 2) a coarse 3D deformation model trained from binary masks generated from events. We perform experimental comparisons on existing synthetic and newly recorded real datasets with non-rigid objects. The results demonstrate the validity of Ev4DGS and its superior performance compared to multiple naive baselines that can be applied in our setting.

事件相机在新视图合成方面相比同步工作的RGB相机具有多种优势，并且近期已有文献展示了在刚性场景中高效的事件驱动技术。然而，对于非刚性物体，现有方法通常还需要稀疏的RGB输入，这在实际应用中可能构成显著限制；目前仍不清楚是否可以仅从事件流中学习出类似的模型。本文探讨了这一具有挑战性的开放问题，并提出了Ev4DGS，即首个能够基于单目事件流，在显式观测空间（即RGB或灰度图像）中对非刚性变形物体实现新视图合成的方法。我们的方法通过以下两点回归可变形的三维高斯溅射表示：1）一种将估计模型输出与二维事件观测空间关联的损失函数，2）一种利用事件生成的二值掩码训练的粗略三维变形模型。我们在现有的合成数据集以及新采集的非刚性物体真实数据集上进行了实验对比。结果表明，Ev4DGS在本设定下不仅是可行的，而且在性能上优于多种可用的朴素基线方法。
