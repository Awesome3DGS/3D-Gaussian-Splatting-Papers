### RDG-GS: Relative Depth Guidance with Gaussian Splatting for Real-time Sparse-View 3D Rendering

Efficiently synthesizing novel views from sparse inputs while maintaining accuracy remains a critical challenge in 3D reconstruction. While advanced techniques like radiance fields and 3D Gaussian Splatting achieve rendering quality and impressive efficiency with dense view inputs, they suffer from significant geometric reconstruction errors when applied to sparse input views. Moreover, although recent methods leverage monocular depth estimation to enhance geometric learning, their dependence on single-view estimated depth often leads to view inconsistency issues across different viewpoints. Consequently, this reliance on absolute depth can introduce inaccuracies in geometric information, ultimately compromising the quality of scene reconstruction with Gaussian splats. In this paper, we present RDG-GS, a novel sparse-view 3D rendering framework with Relative Depth Guidance based on 3D Gaussian Splatting. The core innovation lies in utilizing relative depth guidance to refine the Gaussian field, steering it towards view-consistent spatial geometric representations, thereby enabling the reconstruction of accurate geometric structures and capturing intricate textures. First, we devise refined depth priors to rectify the coarse estimated depth and insert global and fine-grained scene information to regular Gaussians. Building on this, to address spatial geometric inaccuracies from absolute depth, we propose relative depth guidance by optimizing the similarity between spatially correlated patches of depth and images. Additionally, we also directly deal with the sparse areas challenging to converge by the adaptive sampling for quick densification. Across extensive experiments on Mip-NeRF360, LLFF, DTU, and Blender, RDG-GS demonstrates state-of-the-art rendering quality and efficiency, making a significant advancement for real-world application.

高效地从稀疏输入中合成新视角，同时保持准确性，仍然是 3D 重建中的关键挑战。尽管辐射场和 3D 高斯点渲染等先进技术在密集视角输入下能够实现高质量渲染和出色效率，但在应用于稀疏输入视角时，它们会出现显著的几何重建误差。此外，尽管近期方法利用单目深度估计来增强几何学习，但对单视角估计深度的依赖常导致不同视点间的视图不一致性。这种对绝对深度的依赖可能引入几何信息的不准确性，从而削弱高斯点场景重建的质量。
在本文中，我们提出了 RDG-GS，一种基于相对深度引导和 3D 高斯点渲染的稀疏视角 3D 渲染框架。其核心创新在于利用相对深度引导优化高斯场，推动其生成视图一致的空间几何表示，从而实现精准的几何结构重建并捕捉复杂的纹理。
首先，我们设计了精细化的深度先验，用以修正粗略估计的深度，并将全局和细粒度的场景信息融入常规高斯点中。在此基础上，为解决绝对深度引发的空间几何误差，我们提出了相对深度引导，通过优化深度与图像中空间相关的区域块之间的相似性，提升几何一致性。此外，对于稀疏区域的收敛问题，我们通过自适应采样快速实现点的密集化。
在 Mip-NeRF360、LLFF、DTU 和 Blender 数据集上的大量实验表明，RDG-GS 实现了最先进的渲染质量和效率，为真实场景应用带来了重要进展。
