### DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction

Reconstructing clean, distractor-free 3D scenes from real-world captures remains a significant challenge, particularly in highly dynamic and cluttered settings such as egocentric videos. To tackle this problem, we introduce DeGauss, a simple and robust self-supervised framework for dynamic scene reconstruction based on a decoupled dynamic-static Gaussian Splatting design. DeGauss models dynamic elements with foreground Gaussians and static content with background Gaussians, using a probabilistic mask to coordinate their composition and enable independent yet complementary optimization. DeGauss generalizes robustly across a wide range of real-world scenarios, from casual image collections to long, dynamic egocentric videos, without relying on complex heuristics or extensive supervision. Experiments on benchmarks including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that DeGauss consistently outperforms existing methods, establishing a strong baseline for generalizable, distractor-free 3D reconstructionin highly dynamic, interaction-rich environments.

从真实世界捕捉中重建干净且无干扰的三维场景仍然是一个重大挑战，特别是在高度动态和杂乱的环境中，如自我中心的视频。为了解决这个问题，我们提出了DeGauss，这是一个简单且稳健的自监督框架，基于解耦的动态-静态高斯点云渲染设计进行动态场景重建。DeGauss使用前景高斯点来建模动态元素，使用背景高斯点来建模静态内容，采用概率掩膜来协调它们的组合，使得动态和静态部分能够独立但互补地优化。DeGauss在广泛的真实场景中表现出稳健的泛化能力，从随意的图像集合到长时间、动态的自我中心视频，均无需依赖复杂的启发式方法或大量的监督。我们在多个基准数据集（包括NeRF-on-the-go、ADT、AEA、Hot3D和EPIC-Fields）上的实验表明，DeGauss始终优于现有方法，为在高度动态且富有互动的环境中进行通用、无干扰的三维重建奠定了坚实的基准。
