### ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian Splatting

We present ODE-GS, a novel method that unifies 3D Gaussian Splatting with latent neural ordinary differential equations (ODEs) to forecast dynamic 3D scenes far beyond the time span seen during training. Existing neural rendering systems - whether NeRF- or 3DGS-based - embed time directly in a deformation network and therefore excel at interpolation but collapse when asked to predict the future, where timestamps are strictly out-of-distribution. ODE-GS eliminates this dependency: after learning a high-fidelity, time-conditioned deformation model for the training window, we freeze it and train a Transformer encoder that summarizes past Gaussian trajectories into a latent state whose continuous evolution is governed by a neural ODE. Numerical integration of this latent flow yields smooth, physically plausible Gaussian trajectories that can be queried at any future instant and rendered in real time. Coupled with a variational objective and a lightweight second-derivative regularizer, ODE-GS attains state-of-the-art extrapolation on D-NeRF and NVFI benchmarks, improving PSNR by up to 10 dB and halving perceptual error (LPIPS) relative to the strongest baselines. Our results demonstrate that continuous-time latent dynamics are a powerful, practical route to photorealistic prediction of complex 3D scenes.

我们提出了 ODE-GS，一种将三维高斯泼洒（3D Gaussian Splatting）与潜在神经常微分方程（latent neural ODEs）相结合的新方法，用于对动态三维场景进行远超训练时间范围的预测。
现有的神经渲染系统——无论基于 NeRF 还是 3DGS——通常将时间直接嵌入形变网络中，因此在时间插值任务中表现出色，但在面临时间戳超出训练分布的预测任务时却容易失效。ODE-GS 消除了这种依赖：在学习训练时间窗口内的高保真时间条件形变模型后，我们将其参数冻结，并训练一个 Transformer 编码器，该编码器能够将过去的高斯轨迹摘要为一个潜在状态，其连续演化由一个神经 ODE 所控制。
通过对该潜在轨迹进行数值积分，可生成平滑、物理合理的高斯轨迹，支持任意未来时间点的查询与实时渲染。结合变分目标函数与轻量的二阶导数正则项，ODE-GS 在 D-NeRF 与 NVFI 等基准上实现了最先进的外推性能，PSNR 提升可达 10 dB，LPIPS 感知误差减少一半，相较于当前最强基线表现出显著优势。
我们的研究结果表明，基于连续时间的潜在动态建模是实现高质量三维场景预测的强大且实用的路径。
