### OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding

Recent advancements in 3D scene understanding have made significant strides in enabling interaction with scenes using open-vocabulary queries, particularly for VR/AR and robotic applications. Nevertheless, existing methods are hindered by rigid offline pipelines and the inability to provide precise 3D object-level understanding given open-ended queries. In this paper, we present OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that improves semantic modeling and refines object-level understanding. OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed Distance Field to facilitate lossless fusion of semantic features on-the-fly. Furthermore, we introduce a novel multimodal language-guided approach named MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D objects by adaptively adjusting similarity thresholds, achieving an improvement 17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments demonstrate that our method outperforms existing methods in 3D object understanding and scene reconstruction quality, as well as showcasing its effectiveness in language-guided scene interaction.

近年来，三维场景理解取得了显著进展，使得在 VR/AR 和机器人等应用中能够利用开放词汇查询与场景进行交互。然而，现有方法受制于僵化的离线流程，并且在开放式查询条件下无法提供精确的三维物体级理解。本文提出了 OpenGS-Fusion，这是一种创新的开放词汇稠密建图框架，可提升语义建模能力并优化物体级理解。OpenGS-Fusion 将三维高斯表示与截断符号距离场（TSDF）结合，实现了语义特征的无损在线融合。此外，我们引入了一种新颖的多模态语言引导方法——MLLM 辅助自适应阈值（MLLM-Assisted Adaptive Thresholding），通过自适应调整相似度阈值来优化三维物体的分割效果，与固定阈值策略相比，3D mIoU 提升了 17%。大量实验表明，我们的方法在三维物体理解和场景重建质量上均优于现有方法，同时展现了其在语言引导的场景交互中的有效性。
