### Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion

Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on sparse-input novel view synthesis, not only as filling spatial gaps between widely spaced views, but also as completing a natural video unfolding through space.
We recast the task as test-time natural video completion, using powerful priors from pretrained video diffusion models to hallucinate plausible in-between views. Our zero-shot, generation-guided framework produces pseudo views at novel camera poses, modulated by an uncertainty-aware mechanism for spatial coherence. These synthesized frames densify supervision for 3D Gaussian Splatting (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.
The result is coherent, high-fidelity renderings from sparse inputs without any scene-specific training or fine-tuning. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.

仅凭对场景的一些片段性观察，你能想象摄像机穿梭其中所呈现出的完整电影画面吗？这正是我们看待稀疏输入的新视角合成任务的方式：不仅是填补稀疏视角之间的空间空白，更是重建一段自然流动的视频。我们将该任务重新定义为测试时的自然视频补全，利用预训练视频扩散模型中蕴含的强大先验，以生成可信的中间视角画面。我们提出了一种零样本的生成引导框架，可在新颖视角下合成伪视图，并通过不确定性感知机制调控空间一致性。这些合成帧用于增强3D高斯溅射（3D-GS）场景重建中的监督信号，尤其在观测稀缺区域表现显著。我们还引入了一个迭代反馈机制，使三维几何与二维视图合成相互作用，协同提升场景重建与视图生成质量。最终，系统在无任何特定场景训练或微调的前提下，便能从极少量输入中生成连贯、高保真的渲染结果。在 LLFF、DTU、DL3DV 和 MipNeRF-360 数据集上，我们的方法在极度稀疏场景下显著优于现有强大的 3D-GS 基线方法。
