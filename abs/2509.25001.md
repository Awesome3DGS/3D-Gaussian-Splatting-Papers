### LVT: Large-Scale Scene Reconstruction via Local View Transformers

Large transformer models are proving to be a powerful tool for 3D vision and novel view synthesis. However, the standard Transformer's well-known quadratic complexity makes it difficult to scale these methods to large scenes. To address this challenge, we propose the Local View Transformer (LVT), a large-scale scene reconstruction and novel view synthesis architecture that circumvents the need for the quadratic attention operation. Motivated by the insight that spatially nearby views provide more useful signal about the local scene composition than distant views, our model processes all information in a local neighborhood around each view. To attend to tokens in nearby views, we leverage a novel positional encoding that conditions on the relative geometric transformation between the query and nearby views. We decode the output of our model into a 3D Gaussian Splat scene representation that includes both color and opacity view-dependence. Taken together, the Local View Transformer enables reconstruction of arbitrarily large, high-resolution scenes in a single forward pass.

大型 Transformer 模型已被证明是三维视觉与新视图合成中的强大工具。然而，标准 Transformer 所具有的著名二次复杂度（quadratic complexity）使得其难以在大规模场景中扩展。为应对这一挑战，我们提出了 **局部视图 Transformer（Local View Transformer, LVT）**，一种面向大规模场景重建与新视图合成的架构，能够绕过传统二次注意力操作的限制。受到这样一个关键洞察的启发——空间上相邻的视角比远处视角能提供更多关于局部场景组成的有用信息——我们的模型在每个视角的局部邻域内处理所有相关信息。为了高效地关注相邻视角中的特征标记（tokens），我们设计了一种新颖的位置编码方式，该编码基于查询视角与相邻视角之间的相对几何变换进行条件化建模。模型的输出被解码为包含颜色与不透明度视角依赖性的三维高斯溅射（3D Gaussian Splat）场景表示。综上所述，**Local View Transformer** 能够在一次前向推理中重建任意大规模、高分辨率的三维场景。
