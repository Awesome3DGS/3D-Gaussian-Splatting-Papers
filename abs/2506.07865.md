### FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity

In this paper, we aim to model 3D scene geometry, appearance, and the underlying physics purely from multi-view videos. By applying various governing PDEs as PINN losses or incorporating physics simulation into neural networks, existing works often fail to learn complex physical motions at boundaries or require object priors such as masks or types. In this paper, we propose FreeGave to learn the physics of complex dynamic 3D scenes without needing any object priors. The key to our approach is to introduce a physics code followed by a carefully designed divergence-free module for estimating a per-Gaussian velocity field, without relying on the inefficient PINN losses. Extensive experiments on three public datasets and a newly collected challenging real-world dataset demonstrate the superior performance of our method for future frame extrapolation and motion segmentation. Most notably, our investigation into the learned physics codes reveals that they truly learn meaningful 3D physical motion patterns in the absence of any human labels in training.

本文旨在从多视角视频中纯粹地建模三维场景的几何、外观及其底层物理属性。现有方法通常通过将各种控制偏微分方程（PDEs）作为物理引导神经网络（PINN）损失，或将物理仿真融入神经网络来建模物理过程，但这些方法往往无法有效学习物体边界处复杂的物理运动，或依赖于如掩码、物体类别等先验知识。
为此，本文提出 FreeGave：一种无需任何物体先验即可学习复杂动态三维场景物理的全新方法。该方法的核心在于引入一种物理编码（physics code），并设计了一个无散度（divergence-free）的模块，以估计每个高斯基元的速度场，从而避免了低效的 PINN 损失函数。
我们在三个公开数据集以及一个新采集的具有挑战性的真实世界数据集上进行了大量实验，结果显示该方法在未来帧预测和运动分割任务中具有显著性能优势。尤其值得注意的是，我们对学习到的物理编码进行了深入分析，发现即便在完全无人工标注的训练条件下，模型仍能够习得有意义的三维物理运动模式。
