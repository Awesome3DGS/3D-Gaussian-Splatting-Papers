### Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting without Accurate Pose Initialization

3D Gaussian Splatting has recently emerged as a powerful tool for fast and accurate novel-view synthesis from a set of posed input images. However, like most novel-view synthesis approaches, it relies on accurate camera pose information, limiting its applicability in real-world scenarios where acquiring accurate camera poses can be challenging or even impossible. We propose an extension to the 3D Gaussian Splatting framework by optimizing the extrinsic camera parameters with respect to photometric residuals. We derive the analytical gradients and integrate their computation with the existing high-performance CUDA implementation. This enables downstream tasks such as 6-DoF camera pose estimation as well as joint reconstruction and camera refinement. In particular, we achieve rapid convergence and high accuracy for pose estimation on real-world scenes. Our method enables fast reconstruction of 3D scenes without requiring accurate pose information by jointly optimizing geometry and camera poses, while achieving state-of-the-art results in novel-view synthesis. Our approach is considerably faster to optimize than most competing methods, and several times faster in rendering. We show results on real-world scenes and complex trajectories through simulated environments, achieving state-of-the-art results on LLFF while reducing runtime by two to four times compared to the most efficient competing method.

3D Gaussian Splatting 最近作为一种快速且精确的新视角合成工具崭露头角，利用一组具有姿态的输入图像生成新视角。然而，与大多数新视角合成方法类似，它依赖于精确的相机姿态信息，这在现实场景中可能难以获取，甚至不可能实现。为此，我们提出了对3D Gaussian Splatting框架的扩展，通过优化与光度残差相关的外部相机参数来克服这一限制。我们推导了解析梯度，并将其计算与现有的高性能CUDA实现集成。这使得后续任务如六自由度（6-DoF）相机姿态估计以及联合重建和相机优化成为可能。特别是，我们在真实场景中实现了快速收敛和高精度的姿态估计。该方法能够在无需精确姿态信息的情况下，通过联合优化几何和相机姿态，快速重建3D场景，同时在新视角合成中达到最新的技术水平。与大多数竞争方法相比，我们的方法优化速度显著加快，渲染速度也提升了数倍。我们在真实场景和模拟环境中的复杂轨迹上展示了实验结果，在LLFF数据集上实现了最新的技术水平，并将运行时间减少了两到四倍，优于当前最有效的竞争方法。
