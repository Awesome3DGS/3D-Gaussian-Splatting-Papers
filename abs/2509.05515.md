### Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting

Recently, distilling open-vocabulary language features from 2D images into 3D Gaussians has attracted significant attention. Although existing methods achieve impressive language-based interactions of 3D scenes, we observe two fundamental issues: background Gaussians contributing negligibly to a rendered pixel get the same feature as the dominant foreground ones, and multi-view inconsistencies due to view-specific noise in language embeddings. We introduce Visibility-Aware Language Aggregation (VALA), a lightweight yet effective method that computes marginal contributions for each ray and applies a visibility-aware gate to retain only visible Gaussians. Moreover, we propose a streaming weighted geometric median in cosine space to merge noisy multi-view features. Our method yields a robust, view-consistent language feature embedding in a fast and memory-efficient manner. VALA improves open-vocabulary localization and segmentation across reference datasets, consistently surpassing existing works.

近年来，将开放词汇语言特征从二维图像蒸馏到三维高斯表示中受到了广泛关注。虽然现有方法在三维场景的语言交互方面取得了令人印象深刻的成果，但我们观察到其中存在两个根本性问题：其一，背景高斯对渲染像素的贡献几乎可以忽略，却被赋予与主导前景相同的特征；其二，多视图语言嵌入中存在视角特定噪声，导致跨视图不一致性。为此，我们提出了可见性感知语言聚合方法（Visibility-Aware Language Aggregation, VALA），这是一种轻量但高效的方案。VALA通过为每条光线计算边际贡献，并引入可见性门控机制，仅保留可见的高斯。此外，我们还提出了一种在余弦空间中进行流式加权几何中值融合的方法，用于整合多视图噪声特征。该方法能够快速、内存高效地生成稳健且跨视图一致的语言特征嵌入。实验结果表明，VALA在多个参考数据集上的开放词汇定位和分割任务中均取得了显著提升，并持续超越现有方法。
