### Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting

3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis. However, existing methods struggle to adaptively optimize the distribution of Gaussian primitives based on scene characteristics, making it challenging to balance reconstruction quality and efficiency. Inspired by human perception, we propose scene-adaptive perceptual densification for Gaussian Splatting (Perceptual-GS), a novel framework that integrates perceptual sensitivity into the 3DGS training process to address this challenge. We first introduce a perception-aware representation that models human visual sensitivity while constraining the number of Gaussian primitives. Building on this foundation, we develop a perceptual sensitivity-adaptive distribution to allocate finer Gaussian granularity to visually critical regions, enhancing reconstruction quality and robustness. Extensive evaluations on multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate that Perceptual-GS achieves state-of-the-art performance in reconstruction quality, efficiency, and robustness.

三维高斯溅射（3DGS）已成为新视角合成的强大技术。然而，现有方法难以根据场景特征自适应地优化高斯基元的分布，从而难以在重建质量与效率之间取得平衡。受人类感知机制的启发，我们提出**感知驱动的场景自适应高斯加密（Perceptual-GS）**，一种将感知敏感性融入 3DGS 训练过程的新框架，以应对这一挑战。我们首先引入一种**感知感知表示**（Perception-aware Representation），在约束高斯基元数量的同时建模人类视觉敏感性。在此基础上，我们设计了**感知敏感性自适应分布策略**，将更精细的高斯粒度分配给视觉关键区域，从而提升重建质量与鲁棒性。在多个数据集上的大量评估（包括用于大规模场景的 BungeeNeRF）表明，Perceptual-GS 在重建质量、效率和鲁棒性方面均达到了当前最优性能。
