### VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling

We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video generation model. Our core idea is a dual-stream architecture that attaches a dedicated pose generation model alongside a pre-trained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement.

我们提出了 VideoRFSplat，一种直接从文本生成三维模型的方法，利用视频生成模型来生成真实感的 3D 高斯散点 (3DGS)，适用于无界真实世界场景。为了生成具有多样化相机位姿和无界空间范围的真实场景，同时确保对任意文本提示的泛化能力，现有方法通常微调 2D 生成模型，以同时建模相机位姿和多视角图像。然而，由于模态差异，这些方法在扩展 2D 生成模型以联合建模时容易出现不稳定性，进而需要额外的模型来稳定训练和推理。
在本研究中，我们提出了一种新的 架构 和 采样策略，在微调视频生成模型时能够联合建模多视角图像和相机位姿。我们核心的想法是 双流架构，该架构通过通信模块将 专用的相机位姿生成模型 附加到 预训练视频生成模型 之上，从而在独立的流中生成 多视角图像 和 相机位姿。这一设计有效减少了 位姿与图像模态之间的相互干扰。
此外，我们提出了一种 异步采样策略，使相机位姿的去噪速度快于多视角图像。这样，快速去噪的相机位姿能够更早地对多视角图像生成进行条件约束，从而减少跨模态的不确定性，并增强一致性。
我们在多个大规模真实世界数据集（RealEstate10K、MVImgNet、DL3DV-10K、ACID）上进行了训练。实验结果表明，VideoRFSplat 优于现有的 直接文本到 3D 生成方法，后者通常依赖 基于分数蒸馏采样 (score distillation sampling, SDS) 进行后处理优化。而 VideoRFSplat 在无需此类后处理的情况下，即可实现更优的生成效果。
