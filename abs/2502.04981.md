### OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and Geometric-Aware Gaussian Splatting

Obtaining semantic 3D occupancy from raw sensor data without manual annotations remains an essential yet challenging task. While prior works have approached this as a perception prediction problem, we formulate it as scene-aware 3D occupancy reconstruction with geometry and semantics. In this work, we propose OccGS, a novel 3D Occupancy reconstruction framework utilizing Semantic and Geometric-Aware Gaussian Splatting in a zero-shot manner. Leveraging semantics extracted from vision-language models and geometry guided by LiDAR points, OccGS constructs Semantic and Geometric-Aware Gaussians from raw multisensor data. We also develop a cumulative Gaussian-to-3D voxel splatting method for reconstructing occupancy from the Gaussians. OccGS performs favorably against self-supervised methods in occupancy prediction, achieving comparable performance to fully supervised approaches and achieving state-of-the-art performance on zero-shot semantic 3D occupancy estimation.

从原始传感器数据中获取语义3D占据信息而无需人工标注，依然是一个至关重要且具有挑战性的任务。尽管先前的研究将其视为感知预测问题，但我们将其表述为具有几何和语义的场景感知3D占据重建。在本研究中，我们提出了OccGS，一种利用语义和几何感知高斯溅射的零样本3D占据重建框架。OccGS通过利用从视觉-语言模型提取的语义信息和通过LiDAR点引导的几何信息，从原始多传感器数据中构建语义和几何感知高斯。我们还开发了一种累积的高斯到3D体素溅射方法，用于从高斯中重建占据信息。与自监督方法相比，OccGS在占据预测方面表现优异，达到了与完全监督方法相当的性能，并在零样本语义3D占据估计任务中实现了最先进的性能。
