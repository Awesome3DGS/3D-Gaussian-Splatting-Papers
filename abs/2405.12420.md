### GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details

Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly. Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos. However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model. In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts. In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis. Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate diverse geometric and texture details. We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives.

传统的3D服装创建是劳动密集型的，涉及素描、建模、UV映射和纹理化等多个步骤，这些步骤既耗时又昂贵。最近，基于扩散的生成模型的进展为从文本提示、图像和视频生成3D服装开辟了新的可能性。然而，现有方法要么在多视图图像中存在不一致，要么需要额外的过程来从底层人体模型中分离出衣物。在本文中，我们提出了一种名为GarmentDreamer的新方法，该方法利用3D高斯喷溅（GS）作为指导，从文本提示生成可穿戴、可模拟的3D服装网格。与直接使用生成模型预测的多视图图像作为指导相比，我们的3DGS指导确保了服装变形和纹理合成的一致优化。我们的方法引入了一个由法线和RGBA信息指导的新颖服装增强模块，并结合了隐式神经纹理场（NeTF）和得分蒸馏采样（SDS）来生成多样化的几何和纹理细节。我们通过全面的定性和定量实验验证了我们方法的有效性，展示了GarmentDreamer相比于现有最先进方法的卓越性能。
