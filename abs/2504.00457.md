### Distilling Multi-view Diffusion Models into 3D Generators

We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacher's probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method.

我们提出了 DD3G，一种将多视图扩散模型（MV-DM）蒸馏为三维生成器的框架，基于高斯喷洒（Gaussian Splatting）实现。DD3G 通过模拟 MV-DM 的常微分方程（ODE）轨迹，压缩并融合其丰富的视觉与空间几何知识，从而使得蒸馏后的生成器相较于仅在三维数据上训练的模型具备更强的泛化能力。
与以往的摊销式优化方法不同，DD3G 对齐了 MV-DM 与三维生成器的表示空间，从而将教师模型的**概率流（probabilistic flow）**有效传递给学生模型，避免了由概率采样引起的目标函数不一致问题。由于概率流的引入及三维高斯中多种属性的耦合，使得生成过程更加复杂。
为应对此挑战，我们提出了 PEPD，一种包含**模式提取（Pattern Extraction）与渐进解码（Progressive Decoding）**两个阶段的生成器架构，能够高效融合概率流，并在 0.06 秒内将单张图像转化为三维高斯表示。
此外，为减少知识损失并克服稀疏视角监督问题，我们设计了一个联合优化目标，结合显式监督与隐式验证，以确保生成样本的质量。
在蒸馏过程中，我们利用现有的 2D 生成模型构建了包含 12 万张高质量 RGBA 图像的数据集。实验在合成与公开数据集上验证了我们方法的有效性。
