### SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition

Reconstructing dynamic 3D scenes from monocular video remains fundamentally challenging due to the need to jointly infer motion, structure, and appearance from limited observations. Existing dynamic scene reconstruction methods based on Gaussian Splatting often entangle static and dynamic elements in a shared representation, leading to motion leakage, geometric distortions, and temporal flickering. We identify that the root cause lies in the coupled modeling of geometry and appearance across time, which hampers both stability and interpretability. To address this, we propose \textbf{SplitGaussian}, a novel framework that explicitly decomposes scene representations into static and dynamic components. By decoupling motion modeling from background geometry and allowing only the dynamic branch to deform over time, our method prevents motion artifacts in static regions while supporting view- and time-dependent appearance refinement. This disentangled design not only enhances temporal consistency and reconstruction fidelity but also accelerates convergence. Extensive experiments demonstrate that SplitGaussian outperforms prior state-of-the-art methods in rendering quality, geometric stability, and motion separation.

从单目视频中重建动态三维场景依然是一个根本性的挑战，因为这需要在有限观测条件下联合推断运动、结构和外观。现有基于高斯溅射的动态场景重建方法通常将静态与动态元素混合在同一表示中，导致运动泄漏、几何失真和时间闪烁。我们发现其根本原因在于跨时间耦合建模几何与外观，这阻碍了稳定性与可解释性。为解决这一问题，我们提出了 \textbf{SplitGaussian}，一种将场景表示显式分解为静态与动态组件的新框架。通过将运动建模与背景几何解耦，并仅允许动态分支随时间发生形变，我们的方法有效防止了静态区域的运动伪影，同时支持视角与时间相关的外观优化。这种解耦设计不仅提升了时间一致性与重建保真度，还加快了收敛速度。大量实验证明，SplitGaussian 在渲染质量、几何稳定性和运动分离方面均优于以往的最先进方法。
