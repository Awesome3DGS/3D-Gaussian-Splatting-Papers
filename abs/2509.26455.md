### Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting

We present Stylos, a single-forward 3D Gaussian framework for 3D style transfer that operates on unposed content, from a single image to a multi-view collection, conditioned on a separate reference style image. Stylos synthesizes a stylized 3D Gaussian scene without per-scene optimization or precomputed poses, achieving geometry-aware, view-consistent stylization that generalizes to unseen categories, scenes, and styles. At its core, Stylos adopts a Transformer backbone with two pathways: geometry predictions retain self-attention to preserve geometric fidelity, while style is injected via global cross-attention to enforce visual consistency across views. With the addition of a voxel-based 3D style loss that aligns aggregated scene features to style statistics, Stylos enforces view-consistent stylization while preserving geometry. Experiments across multiple datasets demonstrate that Stylos delivers high-quality zero-shot stylization, highlighting the effectiveness of global style-content coupling, the proposed 3D style loss, and the scalability of our framework from single view to large-scale multi-view settings.

本文提出了 **Stylos**，一种单前向（single-forward）三维高斯框架，用于基于三维风格迁移的生成任务。Stylos 能够在无姿态标注的输入下工作，从单张图像到多视图集合均可适用，并以独立的参考风格图像作为条件输入。该方法无需逐场景优化或预计算姿态，即可生成具备几何感知与视角一致性的三维高斯风格化场景，并具备对未见类别、场景与风格的良好泛化能力。Stylos 的核心结构基于 Transformer 骨干网络，包含两条路径：几何预测路径通过自注意力机制保持几何保真度，而风格路径则通过全局交叉注意力（global cross-attention）实现多视图间的视觉一致性。进一步地，我们提出了一种基于体素的三维风格损失（voxel-based 3D style loss），通过对聚合的场景特征与风格统计进行对齐，实现几何保持下的视角一致性风格迁移。大量实验表明，Stylos 在多个数据集上均能实现高质量的零样本风格化效果，验证了其 **全局风格-内容耦合机制**、**三维风格损失设计** 以及 **从单视图到大规模多视图场景的可扩展性** 的有效性。
