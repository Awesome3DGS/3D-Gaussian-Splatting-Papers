### Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and Physics-Based Simulation

Neural reconstruction models for autonomous driving simulation have made significant strides in recent years, with dynamic models becoming increasingly prevalent. However, these models are typically limited to handling in-domain objects closely following their original trajectories. We introduce a hybrid approach that combines the strengths of neural reconstruction with physics-based rendering. This method enables the virtual placement of traditional mesh-based dynamic agents at arbitrary locations, adjustments to environmental conditions, and rendering from novel camera viewpoints. Our approach significantly enhances novel view synthesis quality -- especially for road surfaces and lane markings -- while maintaining interactive frame rates through our novel training method, NeRF2GS. This technique leverages the superior generalization capabilities of NeRF-based methods and the real-time rendering speed of 3D Gaussian Splatting (3DGS). We achieve this by training a customized NeRF model on the original images with depth regularization derived from a noisy LiDAR point cloud, then using it as a teacher model for 3DGS training. This process ensures accurate depth, surface normals, and camera appearance modeling as supervision. With our block-based training parallelization, the method can handle large-scale reconstructions (greater than or equal to 100,000 square meters) and predict segmentation masks, surface normals, and depth maps. During simulation, it supports a rasterization-based rendering backend with depth-based composition and multiple camera models for real-time camera simulation, as well as a ray-traced backend for precise LiDAR simulation.

近年来，自动驾驶仿真中的神经重建模型取得了显著进展，动态模型越来越普及。然而，这些模型通常仅限于处理跟随原始轨迹的领域内对象。我们提出了一种混合方法，结合了神经重建和基于物理的渲染的优势。这种方法能够在任意位置虚拟放置传统的基于网格的动态代理，调整环境条件，并从新颖的摄像机视角进行渲染。我们的方法显著提高了新视角合成质量，特别是在路面和车道标线的渲染上，同时通过我们新颖的训练方法 NeRF2GS 保持了交互式帧率。该技术利用了基于 NeRF 方法的优越泛化能力和 3D 高斯溅射（3DGS）的实时渲染速度。我们通过在原始图像上训练一个定制的 NeRF 模型，并利用来自噪声 LiDAR 点云的深度正则化，随后将其作为教师模型进行 3DGS 训练。这个过程确保了深度、表面法线和摄像机外观建模的准确性，并作为监督信号。通过我们基于块的训练并行化方法，该方法能够处理大规模重建（大于或等于 100,000 平方米），并预测分割掩码、表面法线和深度图。在仿真过程中，它支持基于光栅化的渲染后端，结合基于深度的合成和多摄像机模型进行实时摄像机仿真，同时还支持基于光线追踪的后端进行精确的 LiDAR 仿真。
