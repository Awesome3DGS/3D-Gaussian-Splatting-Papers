### OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving

Accurate and realistic 3D scene reconstruction enables the lifelike creation of autonomous driving simulation environments. With advancements in 3D Gaussian Splatting (3DGS), previous studies have applied it to reconstruct complex dynamic driving scenes. These methods typically require expensive LiDAR sensors and pre-annotated datasets of dynamic objects. To address these challenges, we propose OG-Gaussian, a novel approach that replaces LiDAR point clouds with Occupancy Grids (OGs) generated from surround-view camera images using Occupancy Prediction Network (ONet). Our method leverages the semantic information in OGs to separate dynamic vehicles from static street background, converting these grids into two distinct sets of initial point clouds for reconstructing both static and dynamic objects. Additionally, we estimate the trajectories and poses of dynamic objects through a learning-based approach, eliminating the need for complex manual annotations. Experiments on Waymo Open dataset demonstrate that OG-Gaussian is on par with the current state-of-the-art in terms of reconstruction quality and rendering speed, achieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while significantly reducing computational costs and economic overhead.

精准且真实的三维场景重建 对于自动驾驶仿真环境的逼真构建至关重要。随着 3D 高斯溅射（3D Gaussian Splatting, 3DGS） 技术的进步，先前研究已将其应用于复杂动态驾驶场景的重建。然而，这些方法通常需要昂贵的 LiDAR 传感器 以及预标注的动态物体数据集，导致成本较高且难以大规模推广。
为了解决这些问题，我们提出 OG-Gaussian，一种创新方法，用由环视相机图像通过占据预测网络（Occupancy Prediction Network, ONet）生成的占据网格（Occupancy Grids, OGs）替代 LiDAR 点云。我们的方法利用 OGs 中的语义信息，将动态车辆与静态街道背景分离，并将这些网格转换为 两组独立的初始点云，分别用于重建 静态物体 和 动态物体。此外，我们通过 基于学习的方法 估计 动态物体的轨迹和姿态，无需复杂的人工标注。
在 Waymo Open 数据集 上的实验表明，OG-Gaussian 在重建质量和渲染速度方面可媲美当前最先进的方法，平均 PSNR 达 35.13，渲染速度 达到 143 FPS，同时大幅降低了计算成本和经济开销，为高效的自动驾驶场景重建提供了新方向。
