### ODG: Occupancy Prediction Using Dual Gaussians

Occupancy prediction infers fine-grained 3D geometry and semantics from camera images of the surrounding environment, making it a critical perception task for autonomous driving. Existing methods either adopt dense grids as scene representation, which is difficult to scale to high resolution, or learn the entire scene using a single set of sparse queries, which is insufficient to handle the various object characteristics. In this paper, we present ODG, a hierarchical dual sparse Gaussian representation to effectively capture complex scene dynamics. Building upon the observation that driving scenes can be universally decomposed into static and dynamic counterparts, we define dual Gaussian queries to better model the diverse scene objects. We utilize a hierarchical Gaussian transformer to predict the occupied voxel centers and semantic classes along with the Gaussian parameters. Leveraging the real-time rendering capability of 3D Gaussian Splatting, we also impose rendering supervision with available depth and semantic map annotations injecting pixel-level alignment to boost occupancy learning. Extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets new state-of-the-art results while maintaining low inference cost.

占用预测（Occupancy Prediction）通过摄像机图像推断环境的精细三维几何与语义信息，是自动驾驶中的关键感知任务。现有方法要么采用稠密网格作为场景表示，但难以扩展至高分辨率；要么使用单一稀疏查询集来学习整个场景，但不足以应对不同物体的多样特性。本文提出 **ODG**，一种分层双稀疏高斯表示方法，以高效捕获复杂的场景动态。基于驾驶场景可普遍分解为静态部分与动态部分的观察，我们设计了双高斯查询（Dual Gaussian Queries），以更好地建模多样化的场景物体。我们采用分层高斯 Transformer 预测占用体素中心、语义类别及其对应的高斯参数。利用三维高斯溅射（3D Gaussian Splatting）的实时渲染能力，我们结合可用的深度和语义图标注引入渲染监督，在像素级对齐约束下进一步提升占用预测性能。在 Occ3D-nuScenes 与 Occ3D-Waymo 基准测试上的大量实验表明，我们的方法在保持低推理开销的同时，达到了新的最优性能。
