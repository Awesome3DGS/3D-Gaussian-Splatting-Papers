### OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects

Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime.

从单目视频中重建自由运动物体仍是一项具有挑战性的任务，尤其是在缺乏可靠的位姿或深度信息，并且物体运动方式多变的情况下。为应对这一问题，本文提出了OnlineSplatter——一种新颖的在线前馈式重建框架，可直接从RGB图像帧生成高质量、以物体为中心的三维高斯表示，无需相机位姿、深度先验或Bundle Adjustment优化。
该方法以首帧为锚点启动重建流程，并通过密集的高斯原语场对物体表示进行逐步细化，其计算开销在任意视频序列长度下始终保持恒定。我们工作的核心贡献在于提出了一种双键记忆模块，结合了潜在的外观-几何键与显式的方向键，能够稳健地融合当前帧特征与时间聚合的物体状态。该模块通过空间引导的记忆读取与高效稀疏化机制，有效应对自由运动物体的建模需求，实现全面而紧凑的物体表示覆盖。
在多个真实世界数据集上的评估表明，OnlineSplatter在无需位姿条件下显著优于现有最先进的重建方法，且在增加观察次数的同时持续提升重建质量，同时保持恒定的内存占用与运行时间。
