### MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos

Accurately analyzing the motion parts and their motion attributes in dynamic environments is crucial for advancing key areas such as embodied intelligence. Addressing the limitations of existing methods that rely on dense multi-view images or detailed part-level annotations, we propose an innovative framework that can analyze 3D mobility from monocular videos in a zero-shot manner. This framework can precisely parse motion parts and motion attributes only using a monocular video, completely eliminating the need for annotated training data. Specifically, our method first constructs the scene geometry and roughly analyzes the motion parts and their initial motion attributes combining depth estimation, optical flow analysis and point cloud registration method, then employs 2D Gaussian splatting for scene representation. Building on this, we introduce an end-to-end dynamic scene optimization algorithm specifically designed for articulated objects, refining the initial analysis results to ensure the system can handle 'rotation', 'translation', and even complex movements ('rotation+translation'), demonstrating high flexibility and versatility. To validate the robustness and wide applicability of our method, we created a comprehensive dataset comprising both simulated and real-world scenarios. Experimental results show that our framework can effectively analyze articulated object motions in an annotation-free manner, showcasing its significant potential in future embodied intelligence applications.

在动态环境中对运动部件及其运动属性进行精确分析，对于推动具身智能等关键领域的发展至关重要。针对现有方法依赖密集多视角图像或精细零件级标注的局限性，本文提出了一种创新性框架，能够以零样本的方式从单目视频中解析三维运动性（3D mobility）。该框架仅依赖单目视频即可精准解析运动部件及其运动属性，完全不依赖任何带注释的训练数据。
具体而言，我们的方法首先通过深度估计、光流分析和点云配准等技术构建场景几何结构，并粗略分析运动部件及其初始运动属性，随后使用二维高斯投影（2D Gaussian Splatting）进行场景表示。在此基础上，我们提出了一种专为关节型物体设计的端到端动态场景优化算法，用于进一步优化初始分析结果，使系统能够处理“旋转”、“平移”乃至“旋转+平移”等复杂运动，展现出高度的灵活性与通用性。
为验证方法的鲁棒性与广泛适用性，我们构建了一个涵盖模拟与真实场景的综合数据集。实验结果表明，该框架能够在无需人工标注的条件下有效解析关节型物体的运动特性，展现出在未来具身智能应用中的巨大潜力。
