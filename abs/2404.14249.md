### CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and View-consistent 3D Semantic Understanding

The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time synthesis of novel views in 3D scenes. Currently, it primarily focuses on geometry and appearance modeling, while lacking the semantic understanding of scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to efficiently comprehend 3D environments without annotated semantic data. In specific, rather than straightforwardly learning and rendering high-dimensional semantic features of 3D Gaussians, which significantly diminishes the efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC exploits the inherent unified semantics within objects to learn compact yet effective semantic representations of 3D Gaussians, enabling highly efficient rendering (>100 FPS). Additionally, to address the semantic ambiguity, caused by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the multi-view consistency originated from the 3D model. 3DCS imposes cross-view semantic consistency constraints by leveraging refined, self-predicted pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing precise and view-consistent segmentation results. Extensive experiments demonstrate that our method remarkably outperforms existing state-of-the-art approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on Replica and ScanNet datasets, respectively, while maintaining real-time rendering speed. Furthermore, our approach exhibits superior performance even with sparse input data, verifying the robustness of our method.

最近的三维高斯涂抹（GS）展示了在三维场景中高质量和实时合成新视角的能力。目前，它主要关注几何和外观建模，而缺乏对场景的语义理解。为了弥补这一差距，我们提出了CLIP-GS，它将对比语言-图像预训练（CLIP）的语义集成到高斯涂抹中，以有效地理解没有标注语义数据的三维环境。具体来说，我们提出了一种语义属性紧凑性（SAC）方法，而不是直接学习和渲染三维高斯的高维语义特征，这大大降低了效率。SAC利用物体内部固有的统一语义，学习紧凑而有效的三维高斯的语义表示，实现高效渲染（>100 FPS）。此外，为了解决利用视图不一致的二维CLIP语义监督高斯所导致的语义歧义问题，我们引入了一种三维连贯自训练（3DCS）策略，依靠三维模型产生的多视图一致性。3DCS通过利用从训练的三维高斯模型派生的精炼的自预测伪标签，施加跨视图语义一致性约束，从而增强精确且视图一致的分割结果。广泛的实验表明，我们的方法显著优于现有的最先进方法，在Replica和ScanNet数据集上的mIoU指标分别提高了17.29%和20.81%，同时保持实时渲染速度。此外，我们的方法即使在输入数据稀疏的情况下也表现出优越的性能，验证了我们方法的鲁棒性。
