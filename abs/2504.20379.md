### GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting

In this paper, we present a method for localizing a query image with respect to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the method uses 3DGS to render a synthetic RGBD image at some initial pose estimate. Second, it establishes 2D-2D correspondences between the query image and this synthetic image. Third, it uses the depth map to lift the 2D-2D correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP) problem to produce a final pose estimate. Results from evaluation across three existing datasets with 38 scenes and over 2,700 test images show that our method significantly reduces both inference time (by over two orders of magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation error compared to baseline methods that use photometric loss minimization. Results also show that our method tolerates large errors in the initial pose estimate of up to 55° in rotation and 1.1 units in translation (normalized by scene scale), achieving final pose errors of less than 5° in rotation and 0.05 units in translation on 90% of images from the Synthetic NeRF and Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and Temples dataset.

在本文中，我们提出了一种方法，用于将查询图像定位到预先计算的三维高斯泼溅（3D Gaussian Splatting, 3DGS）场景表示中。该方法首先使用 3DGS 在某个初始位姿估计下渲染出一张合成的 RGBD 图像。然后，在查询图像与该合成图像之间建立 2D-2D 对应关系。接着，利用深度图将这些 2D-2D 对应关系提升为 2D-3D 对应关系，并通过求解透视-n-点（PnP）问题得到最终的位姿估计。
在三个已有数据集上对共 38 个场景和超过 2700 张测试图像进行评估的结果表明，与基于光度损失最小化的基线方法相比，我们的方法在推理时间上显著降低（从 10 秒以上缩短至最快 0.1 秒，提升超过两个数量级），同时也大幅减少了估计误差。实验还表明，我们的方法能够容忍初始位姿估计中高达 55° 的旋转误差和 1.1 个单位（按场景尺度归一化）的平移误差，并在 Synthetic NeRF 和 Mip-NeRF360 数据集上的 90% 图像以及更具挑战性的 Tanks and Temples 数据集上的 42% 图像上，将最终位姿误差控制在 5° 的旋转和 0.05 个单位的平移以内。
