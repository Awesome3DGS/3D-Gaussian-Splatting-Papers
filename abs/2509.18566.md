### Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction

Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

从单目视频中同时重建动态人体与静态场景仍然是一项具有挑战性的任务，尤其是在快速运动情况下，RGB 帧容易受到运动模糊的影响。事件相机具有独特优势，例如微秒级时间分辨率，使其成为动态人体重建的理想传感器选择。为此，我们提出了一种基于事件引导的人体-场景联合重建框架，通过三维高斯溅射（3D Gaussian Splatting）在单个事件相机的输入下同时建模人体与场景。具体而言，我们使用一组统一的三维高斯，并为其引入可学习的语义属性；其中仅被分类为人体的高斯会进行形变以实现动画，而场景高斯保持静态。为应对运动模糊问题，我们提出了一种事件引导损失，通过匹配相邻渲染帧之间的模拟亮度变化与事件流，从而提升快速运动区域的局部保真度。该方法无需外部人体掩码，并简化了对独立高斯集合的管理。在 ZJU-MoCap-Blur 与 MMHPSD-Blur 两个基准数据集上的实验表明，我们的方法在人体-场景联合重建方面达到了当前最先进水平，在 PSNR/SSIM 指标上显著优于强基线模型，同时有效降低了 LPIPS，尤其在高速运动场景中表现突出。
