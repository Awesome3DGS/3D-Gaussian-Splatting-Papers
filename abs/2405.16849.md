### Sync4D: Video Guided Controllable Dynamics for Physics-Based 4D Generation

In this work, we introduce a novel approach for creating controllable dynamics in 3D-generated Gaussians using casually captured reference videos. Our method transfers the motion of objects from reference videos to a variety of generated 3D Gaussians across different categories, ensuring precise and customizable motion transfer. We achieve this by employing blend skinning-based non-parametric shape reconstruction to extract the shape and motion of reference objects. This process involves segmenting the reference objects into motion-related parts based on skinning weights and establishing shape correspondences with generated target shapes. To address shape and temporal inconsistencies prevalent in existing methods, we integrate physical simulation, driving the target shapes with matched motion. This integration is optimized through a displacement loss to ensure reliable and genuine dynamics. Our approach supports diverse reference inputs, including humans, quadrupeds, and articulated objects, and can generate dynamics of arbitrary length, providing enhanced fidelity and applicability. Unlike methods heavily reliant on diffusion video generation models, our technique offers specific and high-quality motion transfer, maintaining both shape integrity and temporal consistency.

在这项工作中，我们介绍了一种使用随意捕获的参考视频在三维生成的高斯中创造可控动态的新方法。我们的方法将参考视频中的物体运动转移到不同类别的生成三维高斯上，确保精确和可定制的运动传递。我们通过采用基于混合蒙皮的非参数形状重建来提取参考对象的形状和运动来实现这一点。这个过程涉及根据蒙皮权重将参考对象分割成与运动相关的部分，并与生成的目标形状建立形状对应关系。为了解决现有方法中普遍存在的形状和时间不一致性，我们整合了物理模拟，驱动目标形状与匹配的运动。这种整合通过位移损失优化，以确保可靠和真实的动态。我们的方法支持多样的参考输入，包括人类、四足动物和关节对象，并可以生成任意长度的动态，提供了更高的保真度和适用性。与严重依赖扩散视频生成模型的方法不同，我们的技术提供了特定的、高质量的运动转移，保持了形状完整性和时间连贯性。
