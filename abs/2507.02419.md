### AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars

Similar to facial beautification in real life, 3D virtual avatars require personalized customization to enhance their visual appeal, yet this area remains insufficiently explored. Although current 3D Gaussian editing methods can be adapted for facial makeup purposes, these methods fail to meet the fundamental requirements for achieving realistic makeup effects: 1) ensuring a consistent appearance during drivable expressions, 2) preserving the identity throughout the makeup process, and 3) enabling precise control over fine details. To address these, we propose a specialized 3D makeup method named AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup patterns from a single reference photo of any individual. We adopt a coarse-to-fine idea to first maintain the consistent appearance and identity, and then to refine the details. In particular, the diffusion model is employed to generate makeup images as supervision. Due to the uncertainties in diffusion process, the generated images are inconsistent across different viewpoints and expressions. Therefore, we propose a Coherent Duplication method to coarsely apply makeup to the target while ensuring consistency across dynamic and multiview effects. Coherent Duplication optimizes a global UV map by recoding the averaged facial attributes among the generated makeup images. By querying the global UV map, it easily synthesizes coherent makeup guidance from arbitrary views and expressions to optimize the target avatar. Given the coarse makeup avatar, we further enhance the makeup by incorporating a Refinement Module into the diffusion model to achieve high makeup quality. Experiments demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation.

与现实生活中的面部美化类似，三维虚拟头像同样需要个性化定制来提升其视觉吸引力，但这一领域仍然研究不足。尽管现有的三维高斯编辑方法可以被改造用于面部化妆，但它们无法满足实现真实化妆效果的基本要求：1）在可驱动表情下保持外观一致性；2）在化妆过程中保留身份特征；3）能够对细节进行精确控制。为此，我们提出了一种专门的三维化妆方法——AvatarMakeup，该方法利用预训练的扩散模型，从任意个体的一张参考照片中迁移化妆样式。我们采用由粗到细的思路，先确保外观和身份一致性，再逐步细化细节。具体而言，扩散模型用于生成化妆图像作为监督。然而，由于扩散过程的不确定性，生成的化妆图像在不同视角和表情下存在不一致。为解决这一问题，我们提出了一种一致性复制（Coherent Duplication）方法，对目标进行粗化妆的同时，确保动态与多视角效果的一致性。一致性复制通过记录生成化妆图像的平均面部属性来优化全局 UV 映射。通过查询该全局 UV 映射，可以轻松从任意视角和表情生成一致的化妆引导，从而优化目标头像。在得到粗化妆头像后，我们进一步将精细化模块（Refinement Module）引入扩散模型，以实现高质量的化妆效果。实验结果表明，AvatarMakeup 在动画全程中实现了化妆迁移质量与一致性的最先进水平。
