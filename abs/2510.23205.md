### VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting

End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm that unifies perception, prediction, and planning into a holistic, data-driven framework. However, achieving robustness to varying camera viewpoints, a common real-world challenge due to diverse vehicle configurations, remains an open problem. In this work, we propose VR-Drive, a novel E2E-AD framework that addresses viewpoint generalization by jointly learning 3D scene reconstruction as an auxiliary task to enable planning-aware view synthesis. Unlike prior scene-specific synthesis approaches, VR-Drive adopts a feed-forward inference strategy that supports online training-time augmentation from sparse views without additional annotations. To further improve viewpoint consistency, we introduce a viewpoint-mixed memory bank that facilitates temporal interaction across multiple viewpoints and a viewpoint-consistent distillation strategy that transfers knowledge from original to synthesized views. Trained in a fully end-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and improves planning under viewpoint shifts. In addition, we release a new benchmark dataset to evaluate E2E-AD performance under novel camera viewpoints, enabling comprehensive analysis. Our results demonstrate that VR-Drive is a scalable and robust solution for the real-world deployment of end-to-end autonomous driving systems.

端到端自动驾驶（E2E-AD）作为一种有前景的新范式，将感知、预测与规划整合为一个整体的数据驱动框架。然而，在应对不同摄像头视角这一由于车辆配置多样性而在真实世界中普遍存在的挑战方面，仍存在显著困难。为此，我们提出了 VR-Drive，一个新颖的 E2E-AD 框架，通过联合学习三维场景重建作为辅助任务，实现面向规划的视图合成，从而增强视角泛化能力。与以往依赖特定场景合成的方法不同，VR-Drive 采用前馈式推理策略，支持从稀疏视图进行在线训练时增强，无需额外注释。为了进一步提升视角一致性，我们引入了视角混合记忆库，用于促进跨多个视角的时序交互，并提出了一种视角一致的蒸馏策略，将原始视图中的知识迁移至合成视图。在完全端到端的训练范式下，VR-Drive 有效缓解了由合成引入的噪声，并提升了在视角变换下的规划性能。此外，我们还发布了一个全新的基准数据集，用于评估在新颖摄像头视角下的 E2E-AD 表现，从而支持更全面的分析。实验结果表明，VR-Drive 是一个具备可扩展性与鲁棒性的解决方案，适用于真实环境中的端到端自动驾驶系统部署。
