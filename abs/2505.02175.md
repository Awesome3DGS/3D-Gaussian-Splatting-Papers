### SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting

Recovering 3D information from scenes via multi-view stereo reconstruction (MVS) and novel view synthesis (NVS) is inherently challenging, particularly in scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting (3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3D scene reconstruction while maintaining real-time performance. Recent approaches have tackled the problem of sparse real-time NVS using 3DGS within a generalizable, MVS-based learning framework to regress 3D Gaussian parameters. Our work extends this line of research by addressing the challenge of generalizable sparse 3D reconstruction and NVS jointly, and manages to perform successfully at both tasks. We propose an MVS-based learning pipeline that regresses 2DGS surface element parameters in a feed-forward fashion to perform 3D shape reconstruction and NVS from sparse-view images. We further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. The resulting model attains the state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also demonstrates strong generalization on the BlendedMVS and Tanks and Temples datasets. We note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed.

通过多视图立体重建（Multi-View Stereo, MVS）与新视角合成（Novel View Synthesis, NVS）恢复场景中的三维信息本质上具有挑战性，尤其是在稀疏视角设置下更为困难。三维高斯泼溅（3D Gaussian Splatting, 3DGS）的出现实现了实时、写实的新视角合成。随后，二维高斯泼溅（2D Gaussian Splatting, 2DGS）通过透视精确的二维高斯图元光栅化，在保持实时性能的同时提升了渲染过程中的几何表达能力，从而改善了三维场景重建质量。
近期的一些方法已在可泛化的、基于 MVS 的学习框架中利用 3DGS 解决稀疏视角下的实时 NVS 问题，通过回归 3D 高斯参数实现重建与渲染。本文在此研究方向基础上进一步拓展，提出了一种同时解决可泛化稀疏视角三维重建与新视角合成的联合方法，并在两项任务中均取得优异表现。
我们提出了一种基于 MVS 的学习流水线，采用前馈方式回归 2DGS 表面元素参数，从稀疏视角图像中实现三维形状重建与新视角图像合成。此外，我们进一步表明该可泛化流水线能够从已有的多视角基础视觉特征中受益。
实验表明，我们的模型在 DTU 稀疏三维重建基准上达到了当前最优的 Chamfer 距离指标，同时在新视角合成任务中也实现了最先进性能。在 BlendedMVS 与 Tanks and Temples 数据集上亦展现出良好的泛化能力。值得注意的是，与基于隐式表示体渲染的前馈稀疏视角重建方法相比，我们的模型不仅在精度上超越现有最优方法，且推理速度提升近两个数量级。
