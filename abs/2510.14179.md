### Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures

We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production.

我们提出了一个新框架，通过创新的数据定制流程，使视频扩散模型同时具备多视角角色一致性与三维相机控制能力。我们利用4D高斯溅射（4D Gaussian Splatting, 4DGS）对录制的体积捕捉表演进行多相机轨迹重渲染，并结合视频重光照模型引入光照变化，用以训练角色一致性模块。在此基础上，我们对开源的先进视频扩散模型进行微调，使其在多视角身份保留、精确相机控制与光照适应性方面具备强大能力。该框架还支持虚拟制作中的核心能力，包括多角色生成（采用联合训练与噪声融合两种方式，其中噪声融合支持在推理阶段高效组合独立定制模型）；此外，还支持场景与真实视频定制，以及对运动轨迹与空间布局的控制。大量实验结果表明，该方法在视频质量、个性化准确性、相机控制与光照适应性方面均有显著提升，推动了视频生成技术在虚拟制作中的深度融合。
