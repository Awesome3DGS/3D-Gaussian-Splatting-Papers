### SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior

Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views.

街景的新视角合成（NVS）在自动驾驶模拟中扮演着至关重要的角色。目前实现它的主流技术是神经渲染，例如神经辐射场（NeRF）和三维高斯喷涂（3DGS）。尽管取得了令人振奋的进展，但在处理街景时，当前方法在远离训练视点的视角维持渲染质量方面存在困难。这一问题源于由移动车辆上的固定摄像头捕获的稀疏训练视图。为了解决这个问题，我们提出了一种新颖的方法，通过利用扩散模型的先验以及补充的多模态数据来增强3DGS的能力。具体来说，我们首先通过添加来自相邻帧的图像作为条件来微调扩散模型，同时利用来自激光雷达点云的深度数据提供额外的空间信息。然后我们在训练中将扩散模型应用于未见视图的3DGS正则化。实验结果验证了我们方法与当前最先进模型相比的有效性，并展示了其在从更广视角渲染图像方面的进步。
