### GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting

Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution generalization, and 3D scene inpainting and completion frameworks that suffer from cross-view inconsistency and suboptimal error handling, as they depend exclusively on depth data or 3D smoothness, which ultimately degrades output quality and computational performance. Building upon these approaches, we present GaussVideoDreamer, which advances generative multimedia approaches by bridging the gap between image, video, and 3D generation, integrating their strengths through two key innovations: (1) A progressive video inpainting strategy that harnesses temporal coherence for improved multiview consistency and faster convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video diffusion with 3D consistent multiview evidence. Our pipeline combines three core components: a geometry-aware initialization protocol, Inconsistency-Aware Gaussian Splatting, and a progressive video inpainting strategy. Experimental results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and at least 2x speedup compared to existing methods while maintaining robust performance across diverse scenes.

单张图像的三维场景重建由于其本质上的不适定性和输入约束有限，始终面临巨大挑战。近期研究主要沿着两个方向展开探索：一类是基于三维一致性数据集训练的多视角生成模型，但其泛化能力在面对分布外样本时表现不佳；另一类是三维场景的补全与修复方法，但这类方法高度依赖深度信息或三维几何光滑性，易导致视角间不一致、误差处理能力不足，从而影响输出质量和计算性能。
在此基础上，我们提出 GaussVideoDreamer，通过整合图像生成、视频生成与三维生成三者的优势，推进生成式多媒体技术的发展。该方法包含两项关键创新：（1）一种渐进式的视频修复策略，利用时间一致性提升多视角一致性并加速收敛；（2）一种三维高斯投影一致性掩码，用以为视频扩散过程提供三维一致的多视角证据。
我们的流程整合了三项核心组件：几何感知的初始化策略、感知不一致性的高斯投影机制（Inconsistency-Aware Gaussian Splatting），以及渐进式的视频修复策略。实验结果表明，我们的方法在多个场景中均表现稳健，在保持生成质量的同时，LLaVA-IQA 分数提升了 32%，并在运行速度上至少实现了 2 倍加速，相较现有方法具有显著优势。
