### GRASPLAT: Enabling dexterous grasping through novel view synthesis

Achieving dexterous robotic grasping with multi-fingered hands remains a significant challenge. While existing methods rely on complete 3D scans to predict grasp poses, these approaches face limitations due to the difficulty of acquiring high-quality 3D data in real-world scenarios. In this paper, we introduce GRASPLAT, a novel grasping framework that leverages consistent 3D information while being trained solely on RGB images. Our key insight is that by synthesizing physically plausible images of a hand grasping an object, we can regress the corresponding hand joints for a successful grasp. To achieve this, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of real hand-object interactions, enabling end-to-end training with RGB data. Unlike prior methods, our approach incorporates a photometric loss that refines grasp predictions by minimizing discrepancies between rendered and real images. We conduct extensive experiments on both synthetic and real-world grasping datasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9% over existing image-based methods.

实现多指灵巧手的机器人抓取仍然是一个重大挑战。现有方法通常依赖完整的三维扫描数据来预测抓取姿态，但在现实环境中获取高质量三维数据具有较大困难，限制了这些方法的应用。本文提出了一种新颖的抓取框架——GRASPLAT，它在训练过程中仅使用RGB图像，同时利用一致的三维信息来辅助学习。我们核心的洞见在于：通过合成物理上合理的手部抓取物体图像，可以反向推理出实现成功抓取所需的手部关节位置。为此，我们采用三维高斯投影（3D Gaussian Splatting）技术生成真实手物交互的高保真新视角图像，从而实现基于RGB数据的端到端训练。与以往方法不同，我们引入了一种光度损失项，通过最小化渲染图与真实图像之间的差异来优化抓取预测。我们在合成和真实抓取数据集上进行了大量实验证明，GRASPLAT在抓取成功率方面相比现有基于图像的方法最高可提升36.9%。
