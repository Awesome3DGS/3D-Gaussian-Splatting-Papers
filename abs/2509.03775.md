### ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction

3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.

三维高斯溅射（3DGS）是一种最先进的技术，可用于高质量、实时渲染的真实场景建模。通常，使用更多的三维高斯可以获得更高质量的表示。然而，大量高斯的使用会显著增加用于存储模型参数的GPU显存需求。如此庞大的模型不仅需要大显存的高性能GPU进行训练，还会因内存访问和数据传输效率低下而导致训练与渲染延迟增加。在本研究中，我们提出了ContraGS，这是一种能够直接在压缩的3DGS表示上进行训练的方法，无需减少高斯数量，从而仅带来极小的质量损失。ContraGS利用码本在整个训练过程中紧凑地存储一组高斯参数向量，从而显著降低内存消耗。尽管码本已被证明在压缩已完全训练的3DGS模型中非常有效，但直接基于码本表示进行训练仍是一个未解决的挑战。ContraGS通过将参数估计视为贝叶斯推理问题，解决了在码本压缩表示中学习不可微参数的难题。为此，ContraGS提供了一个框架，有效利用MCMC采样在这些压缩表示的后验分布上进行采样。实验表明，ContraGS在训练过程中显著降低了峰值内存（平均减少3.49倍），并加速了训练和渲染（平均加速1.36倍和1.88倍），同时保持接近最先进的重建质量。
