### PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images

Reconstructing an animatable 3D human from casually captured images of an articulated subject without camera or human pose information is a practical yet challenging task due to view misalignment, occlusions, and the absence of structural priors. While optimization-based methods can produce high-fidelity results from monocular or multi-view videos, they require accurate pose estimation and slow iterative optimization, limiting scalability in unconstrained scenarios. Recent feed-forward approaches enable efficient single-image reconstruction but struggle to effectively leverage multiple input images to reduce ambiguity and improve reconstruction accuracy. To address these challenges, we propose PF-LHM, a large human reconstruction model that generates high-quality 3D avatars in seconds from one or multiple casually captured pose-free images. Our approach introduces an efficient Encoder-Decoder Point-Image Transformer architecture, which fuses hierarchical geometric point features and multi-view image features through multimodal attention. The fused features are decoded to recover detailed geometry and appearance, represented using 3D Gaussian splats. Extensive experiments on both real and synthetic datasets demonstrate that our method unifies single- and multi-image 3D human reconstruction, achieving high-fidelity and animatable 3D human avatars without requiring camera and human pose annotations.

从随意拍摄的关节化人物图像中，在不具备相机或人体姿态信息的情况下重建可动画的三维人体，是一项具有实用价值但极具挑战性的任务，其难点在于视角不对齐、遮挡以及缺乏结构先验。虽然基于优化的方法可以从单目或多视角视频中生成高保真结果，但它们依赖于精确的姿态估计，并需要缓慢的迭代优化，因此在非受限场景下的可扩展性较差。近期的前馈式方法能够高效完成单图重建，但在充分利用多张输入图像以减少歧义、提升重建精度方面表现不足。为解决这些问题，我们提出 **PF-LHM**，一种大型人体重建模型，可在数秒内从一张或多张随意拍摄的无姿态图像生成高质量的三维虚拟人。我们的方法引入了一种高效的**编码器-解码器点-图像 Transformer 架构**，通过多模态注意力融合分层的几何点特征与多视图图像特征。融合后的特征被解码以恢复细致的几何与外观，并采用三维高斯溅射进行表示。在真实与合成数据集上的大量实验表明，我们的方法统一了单图与多图三维人体重建，实现了无需相机与人体姿态标注的高保真可动画三维虚拟人。
