### 4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models

Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries.

学习 4D 语言场以支持动态场景中的时间敏感和开放词汇查询对于许多现实世界的应用至关重要。虽然 LangSplat 成功地将 CLIP 特征与 3D 高斯表征结合，从而在 3D 静态场景中实现了精确性和效率，但它缺乏处理动态 4D 场的能力，因为 CLIP 主要为静态图像-文本任务设计，无法捕捉视频中的时间动态。现实世界中的环境本质上是动态的，物体语义随时间变化。构建精确的 4D 语言场需要获得像素对齐的、面向物体的视频特征，而当前的视觉模型在这方面存在困难。为了解决这些挑战，我们提出了 4D LangSplat，它通过学习 4D 语言场高效处理动态场景中的时间无关或时间敏感的开放词汇查询。4D LangSplat 避免了从视觉特征中学习语言场的过程，而是直接从通过多模态大语言模型（MLLMs）生成的物体视频字幕中的文本学习。具体来说，我们提出了一种多模态物体视频提示方法，包括视觉和文本提示，引导 MLLMs 生成视频中物体的详细、时间一致的高质量字幕。这些字幕通过大语言模型编码成高质量的句子嵌入，然后作为像素对齐的、面向物体的特征监督，为共享嵌入空间中的开放词汇文本查询提供支持。鉴于 4D 场景中的物体状态展现出平滑的状态过渡，我们进一步提出了一种状态可变形网络，有效地建模这些随时间变化的连续变化。我们在多个基准测试上的结果表明，4D LangSplat 在处理时间敏感和时间无关的开放词汇查询时，均能够实现精确且高效的结果。
