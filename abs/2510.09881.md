### LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates

Recent advances in novel-view synthesis can create the photo-realistic visualization of real-world environments from conventional camera captures. However, acquiring everyday environments from casual captures faces challenges due to frequent scene changes, which require dense observations both spatially and temporally. We propose long-term Gaussian scene chronology from sparse-view updates, coined LTGS, an efficient scene representation that can embrace everyday changes from highly under-constrained casual captures. Given an incomplete and unstructured Gaussian splatting representation obtained from an initial set of input images, we robustly model the long-term chronology of the scene despite abrupt movements and subtle environmental variations. We construct objects as template Gaussians, which serve as structural, reusable priors for shared object tracks. Then, the object templates undergo a further refinement pipeline that modulates the priors to adapt to temporally varying environments based on few-shot observations. Once trained, our framework is generalizable across multiple time steps through simple transformations, significantly enhancing the scalability for a temporal evolution of 3D environments. As existing datasets do not explicitly represent the long-term real-world changes with a sparse capture setup, we collect real-world datasets to evaluate the practicality of our pipeline. Experiments demonstrate that our framework achieves superior reconstruction quality compared to other baselines while enabling fast and light-weight updates.

新视角合成技术的最新进展使得可以利用常规相机捕捉数据生成真实环境的照片级可视化效果。然而，日常环境的随手拍摄常面临频繁场景变化所带来的挑战，这类变化要求在空间和时间上都具备高密度的观测。为应对这一问题，我们提出LTGS（long-term Gaussian scene chronology from sparse-view updates），一种可从极度欠约束的随手拍摄中适应日常变化的高效场景表示方式。面对仅由初始图像集获得的不完整、非结构化高斯泼溅表示，我们能够稳健建模场景的长期时间序列，即便在存在剧烈运动和微妙环境变化的情况下亦如此。我们将场景中的对象构建为模板高斯，这些模板作为可共享的结构性先验用于跟踪对象轨迹。随后，这些对象模板将进入进一步的优化流程，根据少量观测对先验进行调节，以适应时间变化的环境。一旦训练完成，我们的框架便可通过简单的变换泛化到多个时间步，大幅提升了3D环境时间演化建模的可扩展性。鉴于现有数据集并未显式覆盖稀疏拍摄条件下的长期现实变化，我们采集了真实世界的数据集用于评估本方法的实用性。实验结果表明，我们的框架不仅在重建质量上优于现有基线方法，同时支持快速且轻量的更新。
