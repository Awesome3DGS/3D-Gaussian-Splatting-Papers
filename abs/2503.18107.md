### PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding

Recently, 3D Gaussian Splatting (3DGS) has shown encouraging performance for open vocabulary scene understanding tasks. However, previous methods cannot distinguish 3D instance-level information, which usually predicts a heatmap between the scene feature and text query. In this paper, we propose PanoGS, a novel and effective 3D panoptic open vocabulary scene understanding approach. Technically, to learn accurate 3D language features that can scale to large indoor scenarios, we adopt the pyramid tri-plane to model the latent continuous parametric feature space and use a 3D feature decoder to regress the multi-view fused 2D feature cloud. Besides, we propose language-guided graph cuts that synergistically leverage reconstructed geometry and learned language cues to group 3D Gaussian primitives into a set of super-primitives. To obtain 3D consistent instance, we perform graph clustering based segmentation with SAM-guided edge affinity computation between different super-primitives. Extensive experiments on widely used datasets show better or more competitive performance on 3D panoptic open vocabulary scene understanding.

近年来，3D Gaussian Splatting（3DGS）在开放词汇场景理解任务中展现出令人鼓舞的性能。然而，现有方法通常仅通过场景特征与文本查询之间的热力图来建立关联，难以实现对三维实例级信息的区分。
为此，本文提出了一种新颖且高效的三维全景式开放词汇场景理解方法——PanoGS。在技术上，为了学习可扩展至大规模室内场景的高质量三维语言特征，我们采用 金字塔三平面（pyramid tri-plane）结构来建模潜在的连续参数特征空间，并通过一个 三维特征解码器回归多视图融合后的二维特征点云。
此外，我们提出了语言引导的图割算法（language-guided graph cuts），结合重建几何信息与学习到的语言线索，将三维高斯基元划分为一组超基元（super-primitives）。为了实现三维一致性的实例分割，我们在超基元之间计算由 SAM（Segment Anything Model）引导的边界亲和度，并执行图聚类分割。
我们在多个主流数据集上进行了大量实验，结果表明，PanoGS 在三维全景开放词汇场景理解任务中实现了优于或具有竞争力的性能。
