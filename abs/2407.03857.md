### PFGS: High Fidelity Point Cloud Rendering via Feature Splatting

Rendering high-fidelity images from sparse point clouds is still challenging. Existing learning-based approaches suffer from either hole artifacts, missing details, or expensive computations. In this paper, we propose a novel framework to render high-quality images from sparse points. This method first attempts to bridge the 3D Gaussian Splatting and point cloud rendering, which includes several cascaded modules. We first use a regressor to estimate Gaussian properties in a point-wise manner, the estimated properties are used to rasterize neural feature descriptors into 2D planes which are extracted from a multiscale extractor. The projected feature volume is gradually decoded toward the final prediction via a multiscale and progressive decoder. The whole pipeline experiences a two-stage training and is driven by our well-designed progressive and multiscale reconstruction loss. Experiments on different benchmarks show the superiority of our method in terms of rendering qualities and the necessities of our main components.

目前，从稀疏点云生成高保真度图像仍然具有挑战性。现有的基于学习的方法往往存在孔洞伪影、细节缺失或计算复杂度高的问题。本文提出了一种新的框架，用于从稀疏点生成高质量图像。该方法首先尝试将3D高斯光滑和点云渲染进行桥接，其中包括多个级联模块。我们首先使用回归器以点云方式估计高斯属性，这些估计的属性用于将神经特征描述符栅格化到从多尺度提取的2D平面中。投影的特征体积通过多尺度和渐进解码器逐步解码至最终预测。整个流程经历了两阶段训练，并受我们设计良好的渐进和多尺度重建损失驱动。在不同基准测试中的实验显示了我们方法在渲染质量上的优越性以及我们主要组成部分的必要性。
