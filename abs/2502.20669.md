### EndoPBR: Material and Lighting Estimation for Photorealistic Surgical Simulations via Physically-based Rendering

The lack of labeled datasets in 3D vision for surgical scenes inhibits the development of robust 3D reconstruction algorithms in the medical domain. Despite the popularity of Neural Radiance Fields and 3D Gaussian Splatting in the general computer vision community, these systems have yet to find consistent success in surgical scenes due to challenges such as non-stationary lighting and non-Lambertian surfaces. As a result, the need for labeled surgical datasets continues to grow. In this work, we introduce a differentiable rendering framework for material and lighting estimation from endoscopic images and known geometry. Compared to previous approaches that model lighting and material jointly as radiance, we explicitly disentangle these scene properties for robust and photorealistic novel view synthesis. To disambiguate the training process, we formulate domain-specific properties inherent in surgical scenes. Specifically, we model the scene lighting as a simple spotlight and material properties as a bidirectional reflectance distribution function, parameterized by a neural network. By grounding color predictions in the rendering equation, we can generate photorealistic images at arbitrary camera poses. We evaluate our method with various sequences from the Colonoscopy 3D Video Dataset and show that our method produces competitive novel view synthesis results compared with other approaches. Furthermore, we demonstrate that synthetic data can be used to develop 3D vision algorithms by finetuning a depth estimation model with our rendered outputs. Overall, we see that the depth estimation performance is on par with fine-tuning with the original real images.

3D 视觉在手术场景中的标注数据集缺乏，严重限制了医学领域中鲁棒 3D 重建算法的发展。尽管 Neural Radiance Fields (NeRF) 和 3D Gaussian Splatting (3DGS) 在计算机视觉社区中广受关注，但由于非固定光照和非朗伯（non-Lambertian）表面等挑战，这些方法在手术场景中尚未取得稳定成功。因此，对标注的手术数据集的需求仍在增长。
在本研究中，我们提出了一个可微分渲染框架，用于从内窥镜图像和已知几何信息中估计材质和光照。与以往将光照和材质联合建模为辐射度（radiance）的方法不同，我们显式地解耦这些场景属性，以实现更鲁棒且具备高真实感的新视角合成。
为了消除训练过程中的歧义，我们针对手术场景固有的特性进行了建模。具体而言，我们将场景光照建模为单一聚光灯（spotlight），并使用双向反射分布函数（BRDF） 来表征材质属性，该函数由神经网络参数化。通过将颜色预测严格约束在渲染方程内，我们能够在任意相机位姿下生成高逼真的图像。
我们在 Colonoscopy 3D Video Dataset（结肠镜 3D 视频数据集）的多个序列上进行了评估，结果表明，我们的方法在新视角合成任务上与现有方法相比具有竞争力。此外，我们进一步证明，合成数据可用于 3D 视觉算法的开发——具体而言，我们使用渲染输出对深度估计模型进行微调，最终的深度估计性能与使用原始真实图像微调的结果相当。
