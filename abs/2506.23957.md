### GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering

Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce **GaVS**, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent "local reconstruction and rendering" paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study.

视频稳定化在视频处理领域至关重要，它能够消除不必要的抖动，同时保留用户原本的运动意图。现有方法根据其应用领域的不同，存在多种问题（如几何失真、过度裁剪、泛化能力差），这些问题会降低用户体验。为了解决这些问题，我们提出了 **GaVS**，一种基于三维的全新方法，将视频稳定化重新定义为一种时间一致的“局部重建与渲染”范式。在已知三维相机位姿的前提下，我们增强重建模型以预测高斯投影（Gaussian Splatting）基元，并在测试阶段利用多视角、动态感知的光度监督以及跨帧正则化对模型进行微调，从而生成时间一致的局部重建结果。随后使用该模型渲染每一帧稳定化后的画面。我们还引入了场景外推模块以避免画面裁剪。我们的方法在一个经过重新构建的数据集上进行了评估，该数据集包含了丰富的三维先验信息，覆盖了多种相机运动模式和场景动态。在定量评估中，我们的方法在传统任务指标和新的几何一致性指标上均能与最先进的二维和 2.5D 方法竞争甚至超越它们。在定性评估中，通过用户研究验证，我们的方法相比现有替代方案能够产生显著更优的效果。
