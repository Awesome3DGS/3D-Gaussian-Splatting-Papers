### DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting

The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360∘ scene generation pipeline that facilitates the creation of comprehensive 360∘ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary "flat" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360∘ perspective, providing an enhanced immersive experience over existing techniques.

随着虚拟现实应用需求的增加，制作沉浸式3D资产的重要性日益凸显。我们提出了一种文本到3D全景360度场景生成流程，该流程能在几分钟内为野外环境创建全面的360度场景。我们的方法利用了2D扩散模型的生成能力和自我完善提示来创建高质量且全局一致的全景图像。这个图像作为初步的“平面”（2D）场景表示。随后，它被提升到3D高斯体，使用喷涂技术以实现实时探索。为了产生一致的3D几何结构，我们的流程通过将2D单眼深度对齐到全局优化的点云中来构建空间连贯的结构。这个点云作为3D高斯体的质心的初始状态。为了解决单视角输入固有的不可见问题，我们在合成和输入相机视图上施加语义和几何约束作为规范。这些约束指导高斯体的优化，帮助重建未见区域。总之，我们的方法提供了一个全局一致的3D场景，具有360度的视角，相较现有技术提供了更优的沉浸式体验。
