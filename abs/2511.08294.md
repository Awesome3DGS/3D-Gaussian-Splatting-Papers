### SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering

Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning.

精确的三维人体姿态估计是增强现实和人机交互等应用的基础。当前最先进的多视角方法通常依赖大规模标注数据进行训练，通过学习跨视角融合预测来实现性能提升，但当测试场景与训练分布不一致时，其泛化能力往往较差。
为克服这些局限性，我们提出了 **SkelSplat**，一种基于**可微高斯渲染**的多视角三维人体姿态估计新框架。该方法将人体姿态建模为由三维高斯组成的骨架结构，每个关节对应一个高斯，并通过可微渲染进行优化，从而在无需三维真实标注监督的情况下，实现对任意相机视角的无缝融合。
鉴于高斯溅射最初是为稠密场景重建而设计的，我们提出了一种新的 **one-hot 编码方案**，使得各个人体关节能够被独立优化。SkelSplat 在 **Human3.6M** 和 **CMU** 数据集上优于不依赖三维真实标注的方法，并且相比基于学习的方法，跨数据集误差最多降低 **47.8%**。在 **Human3.6M-Occ** 和 **Occlusion-Person** 数据集上的实验进一步表明，该方法在无需针对特定场景进行微调的情况下，依然对遮挡具有良好的鲁棒性。
