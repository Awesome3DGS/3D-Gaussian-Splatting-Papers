### LightHeadEd: Relightable & Editable Head Avatars from a Smartphone

Creating photorealistic, animatable, and relightable 3D head avatars traditionally requires expensive Lightstage with multiple calibrated cameras, making it inaccessible for widespread adoption. To bridge this gap, we present a novel, cost-effective approach for creating high-quality relightable head avatars using only a smartphone equipped with polaroid filters. Our approach involves simultaneously capturing cross-polarized and parallel-polarized video streams in a dark room with a single point-light source, separating the skin's diffuse and specular components during dynamic facial performances. We introduce a hybrid representation that embeds 2D Gaussians in the UV space of a parametric head model, facilitating efficient real-time rendering while preserving high-fidelity geometric details. Our learning-based neural analysis-by-synthesis pipeline decouples pose and expression-dependent geometrical offsets from appearance, decomposing the surface into albedo, normal, and specular UV texture maps, along with the environment maps. We collect a unique dataset of various subjects performing diverse facial expressions and head movements.

传统上，构建逼真、可驱动、可重光照的三维头部虚拟化身通常依赖于配备多台标定相机的昂贵 Lightstage 系统，使其难以被广泛应用。为弥合这一技术鸿沟，本文提出一种仅使用搭载偏振滤光片的智能手机即可实现的高质量、低成本重光照头像建模方法。
该方法在暗室中设置单一光源，同时采集交叉偏振与平行偏振的视频流，从而在动态面部表演过程中分离出皮肤的漫反射与镜面反射成分。在此基础上，我们引入了一种混合表示形式，将二维高斯嵌入到参数化头部模型的 UV 空间中，实现了高保真的几何细节保留与高效实时渲染之间的良好平衡。
我们还设计了一个基于神经分析-合成的学习框架，将姿态与表情相关的几何偏移与外观特征进行解耦，将表面外观进一步分解为反照率（albedo）、法线（normal）、镜面反射（specular）UV 纹理图以及环境贴图，从而实现更具物理一致性的渲染能力。
此外，我们构建了一个包含多位参与者的独特数据集，涵盖丰富的面部表情与头部运动，为后续研究与应用提供了坚实的数据基础。
