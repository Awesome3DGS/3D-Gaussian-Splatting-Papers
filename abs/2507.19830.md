### Taking Language Embedded 3D Gaussian Splatting into the Wild

Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide. However, little attention has been given to the immersive understanding of architectural styles and structural knowledge, which remains largely confined to browsing static text-image pairs. Therefore, can we draw inspiration from 3D in-the-wild reconstruction techniques and use unconstrained photo collections to create an immersive approach for understanding the 3D structure of architectural components? To this end, we extend language embedded 3D Gaussian splatting (3DGS) and propose a novel framework for open-vocabulary scene understanding from unconstrained photo collections. Specifically, we first render multiple appearance images from the same viewpoint as the unconstrained image with the reconstructed radiance field, then extract multi-appearance CLIP features and two types of language feature uncertainty maps-transient and appearance uncertainty-derived from the multi-appearance features to guide the subsequent optimization process. Next, we propose a transient uncertainty-aware autoencoder, a multi-appearance language field 3DGS representation, and a post-ensemble strategy to effectively compress, learn, and fuse language features from multiple appearances. Finally, to quantitatively evaluate our method, we introduce PT-OVS, a new benchmark dataset for assessing open-vocabulary segmentation performance on unconstrained photo collections. Experimental results show that our method outperforms existing methods, delivering accurate open-vocabulary segmentation and enabling applications such as interactive roaming with open-vocabulary queries, architectural style pattern recognition, and 3D scene editing.

近年来，利用大规模互联网照片集进行三维重建的研究取得了显著进展，使得全球范围内的地标与历史遗迹能够以沉浸式的方式进行虚拟探索。然而，对于建筑风格与结构知识的沉浸式理解却鲜有关注，这类内容仍主要局限于浏览静态的图文配对信息。那么，我们是否可以借鉴真实环境下的三维重建技术，利用非受限照片集来构建一种沉浸式方法，从而理解建筑构件的三维结构？为此，我们扩展了语言嵌入的三维高斯点渲染（3D Gaussian Splatting, 3DGS），提出了一种面向非受限照片集的开放词汇场景理解新框架。具体而言，我们首先利用重建的辐射场，从与非受限图像相同的视角渲染多种外观图像，然后提取多外观 CLIP 特征，并基于这些特征计算两类语言特征不确定性图——瞬态不确定性与外观不确定性，用于指导后续优化过程。接着，我们提出了瞬态不确定性感知自编码器、多外观语言场 3DGS 表示以及后融合策略，以高效压缩、学习并融合多外观的语言特征。最后，为了定量评估我们的方法，我们引入了 PT-OVS 数据集，这是一个用于评测非受限照片集上开放词汇分割性能的新基准。实验结果表明，我们的方法优于现有方法，能够实现精确的开放词汇分割，并支持开放词汇查询的交互漫游、建筑风格模式识别以及三维场景编辑等应用。
