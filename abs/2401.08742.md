### Fast Dynamic 3D Object Generation from a Single-view Video

Generating dynamic three-dimensional (3D) object from a single-view video is challenging due to the lack of 4D labeled data. Existing methods extend text-to-3D pipelines by transferring off-the-shelf image generation models such as score distillation sampling, but they are slow and expensive to scale (e.g., 150 minutes per object) due to the need for back-propagating the information-limited supervision signals through a large pretrained model. To address this limitation, we propose an efficient video-to-4D object generation framework called Efficient4D. It generates high-quality spacetime-consistent images under different camera views, and then uses them as labeled data to directly train a novel 4D Gaussian splatting model with explicit point cloud geometry, enabling real-time rendering under continuous camera trajectories. Extensive experiments on synthetic and real videos show that Efficient4D offers a remarkable 10-fold increase in speed when compared to prior art alternatives while preserving the same level of innovative view synthesis quality. For example, Efficient4D takes only 14 minutes to model a dynamic object.

从单视角视频生成动态三维（3D）对象是具有挑战性的，因为缺乏四维（4D）标注数据。现有方法通过转移现成的图像生成模型（如分数蒸馏采样）来扩展文本至3D的流程，但由于需要通过大型预训练模型反向传播信息有限的监督信号，这些方法速度慢且难以扩展（例如，每个对象需要150分钟）。为了解决这一限制，我们提出了一种高效的视频至4D对象生成框架，名为Efficient4D。它生成高质量、时空一致的图像，并在不同的相机视角下使用这些图像作为标注数据，直接训练一个具有明确点云几何形状的新颖4D高斯喷溅模型，从而实现实时渲染连续的相机轨迹。在合成和真实视频上的大量实验表明，与先前的技术相比，Efficient4D在速度上实现了显著的10倍提升，同时保持了相同水平的创新视图合成质量。例如，Efficient4D仅需14分钟就能对一个动态对象进行建模。
