### 4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation

We propose the first framework capable of computing a 4D spatio-temporal grid of video frames and 3D Gaussian particles for each time step using a feed-forward architecture. Our architecture has two main components, a 4D video model and a 4D reconstruction model. In the first part, we analyze current 4D video diffusion architectures that perform spatial and temporal attention either sequentially or in parallel within a two-stream design. We highlight the limitations of existing approaches and introduce a novel fused architecture that performs spatial and temporal attention within a single layer. The key to our method is a sparse attention pattern, where tokens attend to others in the same frame, at the same timestamp, or from the same viewpoint. In the second part, we extend existing 3D reconstruction algorithms by introducing a Gaussian head, a camera token replacement algorithm, and additional dynamic layers and training. Overall, we establish a new state of the art for 4D generation, improving both visual quality and reconstruction capability.

我们提出了首个能够利用前馈架构在每个时间步计算视频帧与三维高斯粒子的四维时空网格的框架。该架构主要由两个核心组件组成：**四维视频模型**与**四维重建模型**。在第一部分中，我们分析了当前的四维视频扩散架构，这些架构通常在双流设计中顺序或并行地执行空间与时间注意力机制。我们指出了现有方法的局限性，并提出了一种全新的融合式架构，在单一层内同时执行空间与时间注意力。我们方法的关键在于一种稀疏注意力模式，其中 token 仅关注同一帧、同一时间戳或来自相同视角的其他 token。在第二部分中，我们扩展了现有的三维重建算法，引入了**高斯头（Gaussian Head）**、**相机 token 替换算法**以及额外的动态层与训练机制。总体而言，我们的方法在四维生成任务中建立了新的最优水平，同时提升了视觉质量与重建能力。
