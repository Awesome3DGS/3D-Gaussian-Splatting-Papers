### MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors

Autonomous driving systems rely heavily on multimodal perception data to understand complex environments. However, the long-tailed distribution of real-world data hinders generalization, especially for rare but safety-critical vehicle categories. To address this challenge, we propose MultiEditor, a dual-branch latent diffusion framework designed to edit images and LiDAR point clouds in driving scenarios jointly. At the core of our approach is introducing 3D Gaussian Splatting (3DGS) as a structural and appearance prior for target objects. Leveraging this prior, we design a multi-level appearance control mechanism--comprising pixel-level pasting, semantic-level guidance, and multi-branch refinement--to achieve high-fidelity reconstruction across modalities. We further propose a depth-guided deformable cross-modality condition module that adaptively enables mutual guidance between modalities using 3DGS-rendered depth, significantly enhancing cross-modality consistency. Extensive experiments demonstrate that MultiEditor achieves superior performance in visual and geometric fidelity, editing controllability, and cross-modality consistency. Furthermore, generating rare-category vehicle data with MultiEditor substantially enhances the detection accuracy of perception models on underrepresented classes.

自动驾驶系统在理解复杂环境时高度依赖多模态感知数据。然而，真实世界数据的长尾分布会阻碍模型的泛化能力，尤其是对于罕见但安全关键的车辆类别。为应对这一挑战，我们提出了 MultiEditor，这是一种双分支潜空间扩散框架，旨在在驾驶场景中联合编辑图像和激光雷达点云。我们方法的核心是引入三维高斯溅射（3DGS）作为目标物体的结构与外观先验。在此先验的基础上，我们设计了一个多级外观控制机制——包括像素级粘贴、语义级引导以及多分支细化——以在多模态间实现高保真重建。此外，我们提出了一种深度引导的可变形跨模态条件模块，该模块利用 3DGS 渲染的深度自适应地实现模态间的相互引导，从而显著提升跨模态一致性。大量实验结果表明，MultiEditor 在视觉与几何保真度、编辑可控性以及跨模态一致性方面均取得了优异表现。此外，利用 MultiEditor 生成罕见类别车辆数据能够显著提升感知模型在少样本类别上的检测精度。
