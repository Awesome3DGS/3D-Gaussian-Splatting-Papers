### GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond

As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.

随着多模态语言模型的发展，它们在三维场景理解中的应用正迅速成为前沿方向，推动了三维视觉语言模型（3D Vision-Language Models, VLMs）的发展。现有方法对目标检测器的依赖较强，导致处理瓶颈并限制了分类体系的灵活性。为克服这些局限，我们提出了一种面向场景的三维高斯投影（Gaussian Splatting）场景的三维视觉语言模型，该模型采用具备语言感知与任务感知的场景表示。我们的方法通过将语言与每个高斯基元关联，将丰富的语言特征直接嵌入三维场景表示中，实现了模态的早期对齐。为处理由此产生的稠密表示，我们引入了双稀疏化器（dual sparsifier），通过任务引导和位置引导两条路径将其提炼为紧凑的、与任务相关的标记，生成稀疏的、任务感知的全局与局部场景标记。值得注意的是，我们提出了首个基于高斯投影的视觉语言模型，利用由标准 RGB 图像生成的逼真三维表示，展现出强大的泛化能力：在域外设置中，其性能相较此前的三维视觉语言模型提升了五倍。
