### 3D Gaussian Splatting as Markov Chain Monte Carlo

While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which does not always generalize and may lead to poor-quality renderings. In addition, for real-world scenes, they rely on a good initial point cloud to perform well. In this work, we rethink 3D Gaussians as random samples drawn from an underlying probability distribution describing the physical representation of the scene -- in other words, Markov Chain Monte Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates are strikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update. As with MCMC, samples are nothing but past visit locations, adding new Gaussians under our framework can simply be realized without heuristics as placing Gaussians at existing Gaussian locations. To encourage using fewer Gaussians for efficiency, we introduce an L1-regularizer on the Gaussians. On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization.

尽管3D高斯飞溅最近在神经渲染领域变得流行，但当前方法依赖于精心设计的克隆和分割策略来放置高斯点，这并不总是泛化的，可能导致渲染质量较差。此外，对于现实世界的场景，它们依赖于一个良好的初始点云以表现良好。在这项工作中，我们重新思考3D高斯点作为从描述场景物理表示的潜在概率分布中抽取的随机样本——换句话说，是马尔可夫链蒙特卡洛（MCMC）样本。在这种观点下，我们展示3D高斯的更新与随机朗之万梯度下降（SGLD）的更新惊人地相似。就像MCMC一样，样本仅是过去的访问位置，根据我们的框架，添加新的高斯点可以简单地实现，无需启发式，只需将高斯点放置在现有高斯位置上。为了鼓励使用更少的高斯点以提高效率，我们引入了一个对高斯点的L1正则化器。在各种标准评估场景上，我们展示了我们的方法提供了改进的渲染质量、对高斯点数量的简易控制以及对初始化的鲁棒性。
