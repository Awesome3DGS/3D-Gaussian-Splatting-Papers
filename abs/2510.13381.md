#### Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering

Dynamic scene rendering and reconstruction play a crucial role in computer vision and augmented reality. Recent methods based on 3D Gaussian Splatting (3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban scenes they require both camera and LiDAR data, ground-truth 3D segmentations and motion data in the form of tracklets or pre-defined object templates such as SMPL. In this work, we explore whether a combination of 2D object agnostic priors in the form of depth and point tracking coupled with a signed distance function (SDF) representation for dynamic objects can be used to relax some of these requirements. We present a novel approach that integrates Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust object representation by harnessing the strengths of both methods. Our unified optimization framework enhances the geometric accuracy of 3D Gaussian splatting and improves deformation modeling within the SDF, resulting in a more adaptable and precise representation. We demonstrate that our method achieves state-of-the-art performance in rendering metrics even without LiDAR data on urban scenes. When incorporating LiDAR, our approach improved further in reconstructing and generating novel views across diverse object categories, without ground-truth 3D motion annotation. Additionally, our method enables various scene editing tasks, including scene decomposition, and scene composition.

动态场景的渲染与重建在计算机视觉和增强现实中具有关键作用。近期基于三维高斯溅射（3D Gaussian Splatting, 3DGS）的方法实现了对动态城市场景的高精度建模，但对于城市环境，它们通常依赖于相机与LiDAR数据、三维分割的真实标签以及形式为tracklets或预定义对象模板（如SMPL）的运动信息。本文探讨了是否可以结合以深度和点追踪为形式的无类别二维先验信息，以及针对动态物体的有符号距离函数（Signed Distance Function, SDF）表示，以放宽这些数据要求。我们提出了一种新颖方法，将SDF与3DGS相结合，利用两者的优势构建更鲁棒的物体表示。我们的统一优化框架不仅提升了3DGS的几何精度，还增强了SDF内的变形建模能力，从而实现更灵活、精确的表示方式。实验表明，即便在没有LiDAR数据的情况下，我们的方法在渲染指标上也达到了当前最先进的性能；而在引入LiDAR数据后，我们的方法在无需真实3D运动标注的前提下，进一步提升了对多种物体类别的新视图重建与生成能力。此外，我们的方法还支持多种场景编辑任务，包括场景分解与组合。
