### EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation

This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalker's potential for real-time multimedia applications.

本文提出了EGSTalker，一种基于三维高斯泼溅（3D Gaussian Splatting, 3DGS）的实时音频驱动拟人头像生成框架。EGSTalker旨在提升生成速度与视觉保真度，仅需3至5分钟的训练视频即可合成高质量面部动画。该框架包含两个关键阶段：静态高斯初始化和音频驱动变形。在第一阶段，采用多分辨率哈希三平面（multi-resolution hash triplane）和Kolmogorov-Arnold网络（Kolmogorov-Arnold Network, KAN）提取空间特征，并构建紧凑的三维高斯表示。在第二阶段，我们提出高效空间-音频注意力（Efficient Spatial-Audio Attention, ESAA）模块，用于融合音频与空间线索，同时由KAN预测相应的高斯变形。大量实验表明，EGSTalker在渲染质量和唇形同步准确性方面可媲美现有最先进方法，同时在推理速度上显著超越它们。上述结果凸显了EGSTalker在实时多媒体应用中的潜力。
