### In-2-4D: Inbetweening from Two Single-View Images to 4D Generation

We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components.

我们提出了一个新问题 In-2-4D，旨在在极简输入设定下实现4D（即三维 + 运动）生成式补间建模。具体而言，该任务从仅有的两张单视图图像出发，图像分别捕捉了同一物体在两个不同运动状态下的瞬间目标是生成并重建该物体的连续运动过程，形成完整的 4D 表达。
我们首先利用视频插帧模型预测中间运动过程，但当帧间差异较大时，运动解释容易出现歧义。为克服这一问题，我们采用分层策略，首先识别出与输入状态视觉上接近且具有显著运动的关键帧，然后在它们之间生成平滑的片段。
对于每个片段，我们使用 Gaussian Splatting 构建其关键帧的三维表示，并利用片段内的时间帧指导运动，通过变形场将其转化为动态高斯。为提升时间一致性并优化三维运动表达，我们将多视角扩散模型的自注意机制拓展至时间维度，同时施加刚体变换正则化以抑制不合理形变。
最终，我们通过边界变形场插值与优化，将各个独立生成的三维运动片段融合，确保整体运动连续流畅、无闪烁。
在大量定性、定量实验及用户研究中，我们验证了本方法及其各组成部分的有效性。
