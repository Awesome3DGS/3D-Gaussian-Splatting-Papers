### DCHM: Depth-Consistent Human Modeling for Multiview Detection

Multiview pedestrian detection typically involves two stages: human modeling and pedestrian localization. Human modeling represents pedestrians in 3D space by fusing multiview information, making its quality crucial for detection accuracy. However, existing methods often introduce noise and have low precision. While some approaches reduce noise by fitting on costly multiview 3D annotations, they often struggle to generalize across diverse scenes. To eliminate reliance on human-labeled annotations and accurately model humans, we propose Depth-Consistent Human Modeling (DCHM), a framework designed for consistent depth estimation and multiview fusion in global coordinates. Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization. Extensive validations demonstrate that our method significantly reduces noise during human modeling, outperforming previous state-of-the-art baselines. Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians and perform multiview segmentation in such a challenging setting.

多视角行人检测通常包括两个阶段：行人建模和行人定位。行人建模通过融合多视角信息在三维空间中表示行人，其质量对检测精度至关重要。然而，现有方法常引入噪声且精度较低。虽然一些方法通过拟合代价高昂的多视角三维标注来降低噪声，但往往难以在多样化场景中具备良好的泛化能力。为消除对人工标注的依赖并精确建模行人，我们提出了深度一致性行人建模（Depth-Consistent Human Modeling, DCHM），该框架旨在实现全局坐标系下的一致深度估计与多视角融合。具体而言，我们提出的基于超像素级高斯点渲染（superpixel-wise Gaussian Splatting）的处理流程，在稀视角、大规模及拥挤场景中实现了多视角深度一致性，生成用于行人定位的精确点云。大量验证结果表明，该方法在行人建模过程中显著降低了噪声，性能优于以往的最新基线方法。此外，据我们所知，DCHM 是首个在如此具有挑战性的环境中同时实现行人重建与多视角分割的方法。
