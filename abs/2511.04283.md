### FastGS: Training 3D Gaussian Splatting in 100 Seconds

The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32× training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45× acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7× training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping.

当前主流的三维高斯投影（3D Gaussian Splatting, 3DGS）加速方法在训练过程中未能有效调控高斯数量，导致冗余的计算时间开销。本文提出了FastGS，一种新颖、简洁且通用的加速框架，能够基于多视角一致性全面评估每个高斯的重要性，从而高效解决训练时间与渲染质量之间的权衡问题。我们创新性地设计了基于多视角一致性的密化与剪枝策略，摒弃了传统的预算机制。我们在Mip-NeRF 360、Tanks & Temples以及Deep Blending等数据集上进行了大量实验，结果表明：与最先进方法相比，FastGS在训练速度方面显著提升，在Mip-NeRF 360数据集上相较于DashGaussian实现了3.32倍的加速，同时保持了可比的渲染质量；在Deep Blending数据集上相较于原始3DGS实现了15.45倍的加速。此外，我们还展示了FastGS在多种任务中的强泛化能力，在动态场景重建、表面重建、稀疏视图重建、大规模重建和同时定位与建图（SLAM）等任务中实现了2至7倍的训练加速效果。
