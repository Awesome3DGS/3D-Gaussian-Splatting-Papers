### IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion

Reconstructing complete and interactive 3D scenes remains a fundamental challenge in computer vision and robotics, particularly due to persistent object occlusions and limited sensor coverage. Multiview observations from a single scene scan often fail to capture the full structural details. Existing approaches typically rely on multi stage pipelines, such as segmentation, background completion, and inpainting or require per-object dense scanning, both of which are error-prone, and not easily scalable. We propose IGFuse, a novel framework that reconstructs interactive Gaussian scene by fusing observations from multiple scans, where natural object rearrangement between captures reveal previously occluded regions. Our method constructs segmentation aware Gaussian fields and enforces bi-directional photometric and semantic consistency across scans. To handle spatial misalignments, we introduce a pseudo-intermediate scene state for unified alignment, alongside collaborative co-pruning strategies to refine geometry. IGFuse enables high fidelity rendering and object level scene manipulation without dense observations or complex pipelines. Extensive experiments validate the framework's strong generalization to novel scene configurations, demonstrating its effectiveness for real world 3D reconstruction and real-to-simulation transfer.

重建完整且可交互的三维场景仍然是计算机视觉和机器人学中的一项基础性挑战，主要原因在于持续存在的物体遮挡和有限的传感器覆盖。单次场景扫描的多视角观测往往难以捕获完整的结构细节。现有方法通常依赖多阶段流水线，如分割、背景补全和修复，或要求对每个物体进行密集扫描，这两种方案都容易出错且难以扩展。我们提出了 IGFuse，一种新颖的框架，通过融合多次扫描的观测来重建可交互的高斯场景，其中自然的物体重排揭示了先前被遮挡的区域。我们的方法构建了具备分割感知的高斯场，并在多次扫描之间施加双向光度和语义一致性约束。为处理空间错位，我们引入了伪中间场景状态以实现统一对齐，并结合协同联合裁剪策略来优化几何。IGFuse 在无需密集观测或复杂流水线的情况下，实现了高保真渲染和物体级场景操控。大量实验验证了该框架在新场景配置上的强泛化能力，展示了其在真实三维重建和真实到仿真迁移中的有效性。
