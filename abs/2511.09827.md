### AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting

We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.

我们提出了一种利用**三维高斯溅射（3D Gaussian Splatting，3DGS）**在三维场景中进行人体动画的新型框架。3DGS 作为一种神经场景表示，近年来在新视角合成任务中取得了最先进的照片级真实效果，但在**人体—场景动画与交互**方面仍有待深入探索。
不同于现有以**网格或点云**作为底层三维表示的动画流程，我们的方法首次将 **3DGS 作为三维表示**引入到场景中人体动画的问题中。通过将人体和场景统一表示为**高斯基元**，该方法能够实现人体与三维场景交互时在**几何上一致的自由视角渲染**。
我们的关键洞见在于，可以将**渲染过程与运动合成过程解耦**，并在无需成对的人体—场景数据的情况下，独立地解决这两个子问题。方法的核心是一个**与高斯对齐的运动模块**，该模块在不显式依赖场景几何的情况下进行运动合成，利用**基于不透明度的线索**以及**投影后的高斯结构**来引导人体的放置与姿态对齐。
为确保交互的自然性，我们进一步提出了一种**人体—场景高斯细化优化策略**，以约束真实的接触关系与行走导航。我们在 **Scannet++** 和 **SuperSplat** 库中的场景上，以及由**稀疏和稠密多视角人体采集**重建的虚拟角色上对该方法进行了评估。
最后，我们展示了该框架支持的一系列新颖应用，例如在**经过编辑的单目 RGB 视频**中引入新的动画人体，并进行**几何一致的自由视角渲染**，从而体现了 **3DGS 在基于单目视频的人体动画任务中的独特优势**。
