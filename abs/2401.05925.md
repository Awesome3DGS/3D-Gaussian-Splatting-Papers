### CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians

We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method firstly optimizes Gaussian points' position, convariance and color attributes under the supervision of RGB images. After Gaussian Locating, we distill multi-scale DINO features extracted from images through unprojection to each Gaussian, which is then incorporated with spatial features from the fast point features processing network, i.e. RandLA-Net. Then the shallow decoding MLP is applied to the multi-scale fused features to obtain compact segmentation. Experimental results show that our model can perform high-quality zero-shot scene segmentation, as our model outperforms other segmentation methods on both semantic and panoptic segmentation task, meanwhile consumes approximately only 10% segmenting time compared to NeRF-based segmentation.

我们提出了一种紧凑而快速的分割3D高斯方法（CoSSegGaussians），用于在仅有RGB图像输入的情况下实现紧凑且一致的3D场景分割和快速渲染。之前基于NeRF的3D分割方法依赖于隐式或体素神经场景表示和射线行走体积渲染，这些方法耗时较长。最近的3D高斯喷溅显著提高了渲染速度，但现有基于高斯的分割方法（例如：高斯分组）尤其在零样本分割中未能提供紧凑的分割掩码，这主要是由于在遇到不一致的2D机器生成标签时，直接为每个高斯分配可学习参数缺乏鲁棒性和紧凑性。我们的方法旨在通过映射每个高斯点的融合空间和语义意义特征，并使用浅层解码网络，迅速实现紧凑且可靠的零样本场景分割。具体来说，我们的方法首先在RGB图像的监督下优化高斯点的位置、协方差和颜色属性。在定位高斯后，我们通过投影将从图像中提取的多尺度DINO特征提炼到每个高斯点上，然后与来自快速点特征处理网络（即RandLA-Net）的空间特征结合。接着，对融合的多尺度特征应用浅层解码MLP，以获得紧凑的分割。实验结果表明，我们的模型能够执行高质量的零样本场景分割，我们的模型在语义和全景分割任务上均优于其他分割方法，同时大约只需NeRF基分割的10%分割时间。
