### ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes

Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the object's appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the object's placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing.

高斯泼溅（Gaussian Splatting, GS）支持沉浸式渲染，但逼真的三维物体-场景合成仍然面临诸多挑战。GS 辐射场中烘焙的外观与阴影信息在物体与场景合成时会导致视觉不一致。要解决该问题，需同时实现可重光照的物体重建与场景光照估计。对于可重光照的物体重建，现有基于高斯的逆向渲染方法多依赖光线追踪，效率较低。为此，我们提出了表面八面体探针（Surface Octahedral Probes, SOPs），用于存储光照与遮挡信息，并通过插值实现高效的三维查询，从而避免高成本的光线追踪。SOPs 可将重建效率提升至少两倍，并支持高斯场景中的实时阴影计算。
对于光照估计，现有基于高斯的逆渲染方法难以建模复杂光传输，容易在复杂场景中失败，而基于学习的方法常常从单张图像预测光照，受视角影响较大。我们观察到，三维物体-场景合成的核心关注在于物体外观及其附近的阴影表现。因此，我们将全局场景光照估计这一难题简化为物体摆放位置处的环境光估计。具体而言，我们在目标位置采集 360 度的重建辐射场，并微调扩散模型以完成光照预测。
基于以上技术，我们提出了 ComGS —— 一个新颖的三维物体-场景合成框架。该方法在约 28 帧每秒（FPS）的速率下实现高质量、实时渲染，能够生成光影生动、视觉协调的合成结果，且编辑时间仅需 36 秒。
