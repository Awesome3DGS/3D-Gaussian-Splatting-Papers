### MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting

Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks.

表面重建在计算机视觉和图形学中已被广泛研究。然而，当输入视角极其稀疏时，现有的表面重建方法难以恢复准确的场景几何。为了解决这一问题，我们提出了 **MeshSplat**，一种基于高斯点绘（Gaussian Splatting）的可泛化稀疏视角表面重建框架。我们的核心思想是利用 **2DGS** 作为桥梁，将新视角合成与学习到的几何先验相连接，并将这些先验迁移以实现表面重建。具体来说，我们引入了一个前馈网络，用于预测逐视角像素对齐的 2DGS，使网络能够合成新视角图像，从而消除了对直接三维真实监督的需求。为提高 2DGS 的位置与方向预测的准确性，我们提出了一种 **加权Chamfer距离损失** 来正则化深度图，特别是在输入视角的重叠区域，同时引入一个法向预测网络，将 2DGS 的方向与单目法向估计器预测的法向向量对齐。大量实验验证了所提改进的有效性，结果表明，我们的方法在可泛化的稀疏视角网格重建任务中实现了当前最优的性能。
