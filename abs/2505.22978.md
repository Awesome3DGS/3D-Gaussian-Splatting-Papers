### Pose-free 3D Gaussian splatting via shape-ray estimation

While generalizable 3D Gaussian splatting enables efficient, high-quality rendering of unseen scenes, it heavily depends on precise camera poses for accurate geometry. In real-world scenarios, obtaining accurate poses is challenging, leading to noisy pose estimates and geometric misalignments. To address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting framework that overcomes these ambiguities by joint shape and camera rays estimation. Instead of relying on explicit 3D transformations, SHARE builds a pose-aware canonical volume representation that seamlessly integrates multi-view information, reducing misalignment caused by inaccurate pose estimates. Additionally, anchor-aligned Gaussian prediction enhances scene reconstruction by refining local geometry around coarse anchors, allowing for more precise Gaussian placement. Extensive experiments on diverse real-world datasets show that our method achieves robust performance in pose-free generalizable Gaussian splatting.

尽管具备泛化能力的三维高斯投影（3D Gaussian Splatting）能够高效地对未见场景进行高质量渲染，但其高度依赖于精确的相机位姿以确保几何准确性。在现实应用中，获取精确的相机位姿具有挑战性，往往会出现噪声估计与几何错位的问题。
为应对这一问题，我们提出了 SHARE，一种无需位姿信息的前馈式高斯投影框架，通过联合估计物体形状与相机光线，有效缓解由位姿不确定性带来的模糊与对齐误差。SHARE 不依赖显式的三维变换，而是构建了一个具备位姿感知能力的标准体积表示，能够自然融合多视角信息，从而减少由不准位姿估计引发的几何错位。
此外，我们还引入了锚点对齐高斯预测机制，通过围绕粗略锚点优化局部几何，实现更精确的高斯放置，进一步提升场景重建质量。
在多个真实世界数据集上的广泛实验证明，SHARE 在无需位姿信息的泛化高斯投影任务中展现出强健的性能与出色的渲染效果。
