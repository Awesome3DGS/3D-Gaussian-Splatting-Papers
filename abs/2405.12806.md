### MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video

Single-view clothed human reconstruction holds a central position in virtual reality applications, especially in contexts involving intricate human motions. It presents notable challenges in achieving realistic clothing deformation. Current methodologies often overlook the influence of motion on surface deformation, resulting in surfaces lacking the constraints imposed by global motion. To overcome these limitations, we introduce an innovative framework, Motion-Based 3D Clothed Humans Synthesis (MOSS), which employs kinematic information to achieve motion-aware Gaussian split on the human surface. Our framework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS) and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher distribution to propagate global motion across the body surface. The density and rotation factors of this distribution explicitly control the Gaussians, thereby enhancing the realism of the reconstructed surface. Additionally, to address local occlusions in single-view, based on KGAS, UID identifies significant surfaces, and geometric reconstruction is performed to compensate for these deformations. Experimental results demonstrate that MOSS achieves state-of-the-art visual quality in 3D clothed human synthesis from monocular videos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94% and 16.75% in LPIPS* respectively.

单视图穿着人体重建在虚拟现实应用中占据核心地位，尤其是在涉及复杂人体运动的情境中。它在实现真实服装变形方面呈现出显著的挑战。当前的方法常常忽视运动对表面变形的影响，导致重建的表面缺乏全局运动所施加的约束。为了克服这些限制，我们引入了一个创新的框架，基于运动的3D穿着人体合成（MOSS），该框架利用运动学信息实现对人体表面的运动感知高斯分裂。我们的框架包括两个模块：运动学高斯定位喷溅（KGAS）和表面变形探测器（UID）。KGAS采用矩阵-费舍尔分布来传播全身表面的全局运动。这种分布的密度和旋转因素显式控制高斯，从而增强重建表面的真实性。此外，为了应对单视图中的局部遮挡，基于KGAS，UID识别重要表面，并执行几何重建以补偿这些变形。实验结果表明，MOSS在从单眼视频合成3D穿着人形方面实现了最先进的视觉质量。特别地，我们分别在Human NeRF和高斯喷溅的LPIPS*指标上提高了33.94%和16.75%。
