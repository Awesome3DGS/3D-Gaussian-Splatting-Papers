### DrivingScene: A Multi-Task Online Feed-Forward 3D Gaussian Splatting Method for Dynamic Driving Scenes

Real-time, high-fidelity reconstruction of dynamic driving scenes is challenged by complex dynamics and sparse views, with prior methods struggling to balance quality and efficiency. We propose DrivingScene, an online, feed-forward framework that reconstructs 4D dynamic scenes from only two consecutive surround-view images. Our key innovation is a lightweight residual flow network that predicts the non-rigid motion of dynamic objects per camera on top of a learned static scene prior, explicitly modeling dynamics via scene flow. We also introduce a coarse-to-fine training paradigm that circumvents the instabilities common to end-to-end approaches. Experiments on nuScenes dataset show our image-only method simultaneously generates high-quality depth, scene flow, and 3D Gaussian point clouds online, significantly outperforming state-of-the-art methods in both dynamic reconstruction and novel view synthesis.

在动态驾驶场景中实现实时、高保真的重建面临复杂动态变化和视角稀疏性的挑战，现有方法在质量与效率之间往往难以兼顾。为此，我们提出了 DrivingScene，一个基于在线前馈机制的框架，仅利用两帧连续的环视图像即可重建四维动态场景。我们的核心创新在于引入一个轻量级残差流网络，结合静态场景先验，在每个摄像头视角下预测动态物体的非刚性运动，通过场景流显式建模动态变化。此外，我们还提出了粗到细的训练范式，有效规避了端到端方法常见的不稳定性问题。在 nuScenes 数据集上的实验表明，我们的图像驱动方法能够同时在线生成高质量的深度图、场景流和三维高斯点云，在动态重建与新视角合成方面均显著优于现有最先进方法。
