### DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines

Differentiable rendering is a technique used in an important emerging class of visual computing applications that involves representing a 3D scene as a model that is trained from 2D images using gradient descent. Recent works (e.g. 3D Gaussian Splatting) use a rasterization pipeline to enable rendering high quality photo-realistic imagery at high speeds from these learned 3D models. These methods have been demonstrated to be very promising, providing state-of-art quality for many important tasks. However, training a model to represent a scene is still a time-consuming task even when using powerful GPUs. In this work, we observe that the gradient computation phase during training is a significant bottleneck on GPUs due to the large number of atomic operations that need to be processed. These atomic operations overwhelm atomic units in the L2 partitions causing stalls. To address this challenge, we leverage the observations that during the gradient computation: (1) for most warps, all threads atomically update the same memory locations; and (2) warps generate varying amounts of atomic traffic (since some threads may be inactive). We propose DISTWAR, a software-approach to accelerate atomic operations based on two key ideas: First, we enable warp-level reduction of threads at the SM sub-cores using registers to leverage the locality in intra-warp atomic updates. Second, we distribute the atomic computation between the warp-level reduction at the SM and the L2 atomic units to increase the throughput of atomic computation. Warps with many threads performing atomic updates to the same memory locations are scheduled at the SM, and the rest using L2 atomic units. We implement DISTWAR using existing warp-level primitives. We evaluate DISTWAR on widely used raster-based differentiable rendering workloads. We demonstrate significant speedups of 2.44x on average (up to 5.7x).

可微渲染是一种在视觉计算应用中日益重要的技术，它通过使用梯度下降算法从2D图像中训练得到的模型来表示3D场景。最近的研究（例如3D高斯喷溅）使用光栅化管道来实现从这些学习到的3D模型中以高速渲染高质量的照片级真实图像。这些方法已经被证明非常有前景，为许多重要任务提供了最先进的质量。然而，即使使用强大的GPU，训练一个模型来表示一个场景仍然是一个耗时的任务。在这项工作中，我们观察到在训练过程中的梯度计算阶段是GPU上的一个重要瓶颈，因为需要处理大量的原子操作。这些原子操作压倒了L2分区中的原子单位，导致停滞。为了应对这一挑战，我们利用以下观察结果：（1）对于大多数warp，所有线程都会原子性地更新相同的内存位置；（2）warp生成不同数量的原子流量（因为有些线程可能不活跃）。我们提出了DISTWAR，一种基于两个关键思想的软件方法来加速原子操作：首先，我们利用寄存器在SM子核心内启用warp级别的线程缩减，以利用内部warp原子更新的局部性。其次，我们将原子计算在SM的warp级别缩减和L2原子单位之间分配，以提高原子计算的吞吐量。在SM上调度对相同内存位置进行大量原子更新的warp，而其他则使用L2原子单位。我们使用现有的warp级别原语实现了DISTWAR。我们在广泛使用的基于光栅的可微渲染工作负载上评估了DISTWAR。我们展示了平均2.44倍的显著加速（高达5.7倍）。
