### 4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar

3D reconstruction and novel view synthesis are critical for validating autonomous driving systems and training advanced perception models. Recent self-supervised methods have gained significant attention due to their cost-effectiveness and enhanced generalization in scenarios where annotated bounding boxes are unavailable. However, existing approaches, which often rely on frequency-domain decoupling or optical flow, struggle to accurately reconstruct dynamic objects due to imprecise motion estimation and weak temporal consistency, resulting in incomplete or distorted representations of dynamic scene elements. To address these challenges, we propose 4DRadar-GS, a 4D Radar-augmented self-supervised 3D reconstruction framework tailored for dynamic driving scenes. Specifically, we first present a 4D Radar-assisted Gaussian initialization scheme that leverages 4D Radar's velocity and spatial information to segment dynamic objects and recover monocular depth scale, generating accurate Gaussian point representations. In addition, we propose a Velocity-guided PointTrack (VGPT) model, which is jointly trained with the reconstruction pipeline under scene flow supervision, to track fine-grained dynamic trajectories and construct temporally consistent representations. Evaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art performance in dynamic driving scene 3D reconstruction.

三维重建与新视角合成在自动驾驶系统验证和高级感知模型训练中具有关键作用。近年来，自监督方法因其高性价比和在无标注边界框场景中的优越泛化能力而备受关注。然而，现有方法多依赖频域解耦或光流估计，在处理动态目标时仍存在运动估计不精确、时间一致性弱等问题，导致动态场景要素的重建结果不完整或发生畸变。为应对这些挑战，我们提出了**4DRadar-GS**——一种针对动态驾驶场景的**基于4D雷达增强的自监督三维重建框架**。具体而言，我们首先提出了**4D雷达辅助的高斯初始化方案（4D Radar-assisted Gaussian Initialization）**，利用4D雷达的速度与空间信息对动态目标进行分割并恢复单目深度尺度，从而生成精确的高斯点表示。此外，我们还提出了**速度引导的点追踪模型（Velocity-guided PointTrack, VGPT）**，该模型在场景流监督下与重建管线联合训练，以跟踪细粒度的动态轨迹并构建时间一致的场景表示。在OmniHD-Scenes数据集上的实验结果表明，4DRadar-GS在动态驾驶场景三维重建任务中取得了当前最优的性能。
