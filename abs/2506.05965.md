### Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments

Current Simultaneous Localization and Mapping (SLAM) methods based on Neural Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static 3D scenes but struggle with tracking and reconstruction in dynamic environments, such as real-world scenes with moving elements. Existing NeRF-based SLAM approaches addressing dynamic challenges typically rely on RGB-D inputs, with few methods accommodating pure RGB input. To overcome these limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS) SLAM method for dynamic scenes using monocular RGB input. To address dynamic interference, we fuse optical flow masks and depth masks through a probabilistic model to obtain a fused dynamic mask. With only a single network iteration, this can constrain tracking scales and refine rendered geometry. Based on the fused dynamic mask, we designed a novel motion loss to constrain the pose estimation network for tracking. In mapping, we use the rendering loss of dynamic pixels, color, and depth to eliminate transient interference and occlusion caused by dynamic objects. Experimental results demonstrate that Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic environments, outperforming or matching existing RGB-D methods.

当前基于神经辐射场（Neural Radiance Fields, NeRF）或三维高斯泼溅（3D Gaussian Splatting, 3DGS）的同步定位与建图（Simultaneous Localization and Mapping, SLAM）方法在重建静态三维场景方面表现优异，但在包含运动元素的真实动态环境中，仍面临跟踪与重建的困难。现有面向动态场景的 NeRF-based SLAM 方法多数依赖 RGB-D 输入，能够处理纯 RGB 输入的方案仍较为稀缺。
为突破上述限制，我们提出 Dy3DGS-SLAM，这是首个面向动态场景、仅使用单目 RGB 输入的三维高斯泼溅 SLAM 方法。为应对动态干扰，我们通过概率模型融合光流掩码与深度掩码，获得融合动态掩码。在仅进行一次网络迭代的条件下，该掩码即可用于约束跟踪尺度并优化重建几何。同时，基于该融合动态掩码，我们设计了一种新颖的运动损失函数，用于约束位姿估计网络以实现鲁棒跟踪。
在建图阶段，我们利用动态像素的渲染损失、颜色和深度信息，有效剔除动态物体带来的瞬时干扰与遮挡。实验结果表明，Dy3DGS-SLAM 在动态环境中实现了领先的跟踪与渲染性能，超越或可比现有的 RGB-D 方法。
