### PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos

We present PAD3R, a method for reconstructing deformable 3D objects from casually captured, unposed monocular videos. Unlike existing approaches, PAD3R handles long video sequences featuring substantial object deformation, large-scale camera movement, and limited view coverage that typically challenge conventional systems. At its core, our approach trains a personalized, object-centric pose estimator, supervised by a pre-trained image-to-3D model. This guides the optimization of deformable 3D Gaussian representation. The optimization is further regularized by long-term 2D point tracking over the entire input video. By combining generative priors and differentiable rendering, PAD3R reconstructs high-fidelity, articulated 3D representations of objects in a category-agnostic way. Extensive qualitative and quantitative results show that PAD3R is robust and generalizes well across challenging scenarios, highlighting its potential for dynamic scene understanding and 3D content creation.

我们提出了 **PAD3R**，一种从随手拍摄、无姿态标注的单目视频中重建可变形三维物体的方法。与现有方法不同，PAD3R 能够处理包含显著物体形变、大范围相机运动以及有限视角覆盖的长视频序列，这些因素通常会对传统系统造成严重挑战。该方法的核心是训练一个个性化、以物体为中心的姿态估计器，并通过预训练的图像到三维模型（image-to-3D model）进行监督，从而引导可变形三维高斯表示（deformable 3D Gaussian representation）的优化过程。优化过程中还结合了整个输入视频的长期二维点跟踪（long-term 2D point tracking）进行正则化。通过结合生成先验与可微渲染（differentiable rendering），PAD3R 能够以类别无关的方式重建高保真、具关节结构的三维物体表示。大量定性与定量实验结果表明，PAD3R 在复杂场景中表现出极高的鲁棒性与泛化能力，展现了其在动态场景理解与三维内容创作中的巨大潜力。
