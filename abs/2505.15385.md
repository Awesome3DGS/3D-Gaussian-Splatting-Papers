### EVA: Expressive Virtual Avatars from Multi-view Videos

With recent advancements in neural rendering and motion capture algorithms, remarkable progress has been made in photorealistic human avatar modeling, unlocking immense potential for applications in virtual reality, augmented reality, remote communication, and industries such as gaming, film, and medicine. However, existing methods fail to provide complete, faithful, and expressive control over human avatars due to their entangled representation of facial expressions and body movements. In this work, we introduce Expressive Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive human avatar framework that achieves high-fidelity, lifelike renderings in real time while enabling independent control of facial expressions, body movements, and hand gestures. Specifically, our approach designs the human avatar as a two-layer model: an expressive template geometry layer and a 3D Gaussian appearance layer. First, we present an expressive template tracking algorithm that leverages coarse-to-fine optimization to accurately recover body motions, facial expressions, and non-rigid deformation parameters from multi-view videos. Next, we propose a novel decoupled 3D Gaussian appearance model designed to effectively disentangle body and facial appearance. Unlike unified Gaussian estimation approaches, our method employs two specialized and independent modules to model the body and face separately. Experimental results demonstrate that EVA surpasses state-of-the-art methods in terms of rendering quality and expressiveness, validating its effectiveness in creating full-body avatars. This work represents a significant advancement towards fully drivable digital human models, enabling the creation of lifelike digital avatars that faithfully replicate human geometry and appearance.

随着神经渲染与动作捕捉算法的持续进展，真实感人类虚拟头像建模取得了显著突破，为虚拟现实、增强现实、远程通信，以及游戏、影视、医疗等多个行业带来了巨大的应用潜力。然而，现有方法由于面部表情与身体动作的表示纠缠，尚无法实现对人类头像的完整、真实且富有表现力的控制。为此，我们提出了 EVA（Expressive Virtual Avatars），一个具有人物特异性、可完全控制且富有表现力的人类虚拟头像建模框架，能够在实时渲染下实现高度真实的视觉表现，同时支持对面部表情、身体动作与手势的独立控制。具体而言，我们将虚拟人头像建模为一个双层结构模型：由表情模板几何层与三维高斯外观层组成。首先，我们提出了一种表情模板跟踪算法，采用由粗到细的优化策略，从多视角视频中精准恢复身体动作、面部表情及非刚性变形参数。接着，我们设计了一种解耦的三维高斯外观建模方法，有效实现身体与面部外观特征的分离。区别于统一高斯估计的方法，我们的方法采用两个独立模块，分别对身体与面部进行建模。实验结果表明，EVA 在渲染质量与表现力方面均优于现有先进方法，验证了其在全身虚拟人建模中的有效性。该工作朝着完全可驱动的数字人模型迈出了关键一步，使得构建能够真实还原人体几何与外观的虚拟头像成为可能。
