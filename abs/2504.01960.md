### Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis

Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have achieved impressive results in real-time 3D reconstruction and novel view synthesis. However, these methods struggle in large-scale, unconstrained environments where sparse and uneven input coverage, transient occlusions, appearance variability, and inconsistent camera settings lead to degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a multi-view diffusion model to address these limitations. By generating pseudo-observations conditioned on multi-view inputs, our method transforms under-constrained 3D reconstruction problems into well-posed ones, enabling robust optimization even with sparse data. GS-Diff further integrates several enhancements, including appearance embedding, monocular depth priors, dynamic object modeling, anisotropy regularization, and advanced rasterization techniques, to tackle geometric and photometric challenges in real-world settings. Experiments on four benchmarks demonstrate that GS-Diff consistently outperforms state-of-the-art baselines by significant margins.

近年来，三维高斯喷洒（3D Gaussian Splatting, 3DGS）与神经辐射场（Neural Radiance Fields, NeRF）在实时三维重建与新视角合成方面取得了令人瞩目的成果。然而，在大规模、非受控环境下，这些方法仍面临诸多挑战，如输入视角稀疏且分布不均、短暂遮挡、外观变化显著，以及相机设置不一致等问题，导致渲染质量严重下降。
为应对这些限制，本文提出 GS-Diff，一种结合多视图扩散模型的全新 3DGS 框架。GS-Diff 通过在多视图条件下生成伪观测图像（pseudo-observations），将原本欠约束的三维重建问题转化为良设问题，即便在数据稀疏的情况下也能实现稳健优化。
此外，GS-Diff 融合了多项增强机制，以应对真实场景中的几何与光度挑战，包括：外观嵌入（appearance embedding）、单目深度先验、动态物体建模、各向异性正则化（anisotropy regularization）以及先进的光栅化技术。
在四个基准数据集上的实验结果表明，GS-Diff 在各项指标上均显著优于当前最先进方法，展现出一致且卓越的性能提升。
