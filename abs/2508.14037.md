### Distilled-3DGS:Distilled 3D Gaussian Splatting

3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view synthesis (NVS). However, it suffers from a significant drawback: achieving high-fidelity rendering typically necessitates a large number of 3D Gaussians, resulting in substantial memory consumption and storage requirements. To address this challenge, we propose the first knowledge distillation framework for 3DGS, featuring various teacher models, including vanilla 3DGS, noise-augmented variants, and dropout-regularized versions. The outputs of these teachers are aggregated to guide the optimization of a lightweight student model. To distill the hidden geometric structure, we propose a structural similarity loss to boost the consistency of spatial geometric distributions between the student and teacher model. Through comprehensive quantitative and qualitative evaluations across diverse datasets, the proposed Distilled-3DGS, a simple yet effective framework without bells and whistles, achieves promising rendering results in both rendering quality and storage efficiency compared to state-of-the-art methods.

三维高斯喷溅（3DGS）在新视角合成（NVS）中展现了卓越的效果。然而，它存在一个显著缺点：要实现高保真渲染通常需要大量的三维高斯，从而导致巨大的内存占用和存储需求。为解决这一问题，我们提出了首个用于 3DGS 的知识蒸馏框架，该框架包含多种教师模型，包括原始 3DGS、噪声增强变体和 Dropout 正则化版本。教师模型的输出被聚合，用以引导轻量级学生模型的优化。为了提炼隐藏的几何结构，我们提出了一种结构相似性损失，以增强学生模型与教师模型在空间几何分布上的一致性。通过在多种数据集上的全面定量与定性评估，所提出的 Distilled-3DGS 框架尽管简单但高效，在渲染质量和存储效率方面均优于现有最先进方法，取得了令人满意的渲染结果。
