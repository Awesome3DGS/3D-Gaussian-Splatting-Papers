### E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting

Novel view synthesis techniques predominantly utilize RGB cameras, inheriting their limitations such as the need for sufficient lighting, susceptibility to motion blur, and restricted dynamic range. In contrast, event cameras are significantly more resilient to these limitations but have been less explored in this domain, particularly in large-scale settings. Current methodologies primarily focus on front-facing or object-oriented (360-degree view) scenarios. For the first time, we introduce 3D Gaussians for event-based novel view synthesis. Our method reconstructs large and unbounded scenes with high visual quality. We contribute the first real and synthetic event datasets tailored for this setting. Our method demonstrates superior novel view synthesis and consistently outperforms the baseline EventNeRF by a margin of 11-25% in PSNR (dB) while being orders of magnitude faster in reconstruction and rendering.

新型视图合成技术主要依赖于RGB相机，继承了其诸如需要充足光照、易受运动模糊影响以及动态范围受限等局限性。相比之下，事件相机在这些限制下表现得更加稳健，但在这一领域，尤其是在大规模设置中，仍然探索较少。目前的方法主要集中在前向或面向物体（360度视图）场景。首次，我们将3D高斯溅射应用于基于事件的视图合成。我们的方法能够重建大型且无限制的场景，并提供高视觉质量。我们贡献了首个专为此设置设计的真实和合成事件数据集。我们的算法在新视图合成方面表现优异，在PSNR（分贝）上比基线方法EventNeRF提高了11-25%，同时在重建和渲染速度上快了几个数量级。
