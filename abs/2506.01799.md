### WorldExplorer: Towards Generating Fully Navigable 3D Scenes

Generating 3D worlds from text is a highly anticipated goal in computer vision. Existing works are limited by the degree of exploration they allow inside of a scene, i.e., produce streched-out and noisy artifacts when moving beyond central or panoramic perspectives. To this end, we propose WorldExplorer, a novel method based on autoregressive video trajectory generation, which builds fully navigable 3D scenes with consistent visual quality across a wide range of viewpoints. We initialize our scenes by creating multi-view consistent images corresponding to a 360 degree panorama. Then, we expand it by leveraging video diffusion models in an iterative scene generation pipeline. Concretely, we generate multiple videos along short, pre-defined trajectories, that explore the scene in depth, including motion around objects. Our novel scene memory conditions each video on the most relevant prior views, while a collision-detection mechanism prevents degenerate results, like moving into objects. Finally, we fuse all generated views into a unified 3D representation via 3D Gaussian Splatting optimization. Compared to prior approaches, WorldExplorer produces high-quality scenes that remain stable under large camera motion, enabling for the first time realistic and unrestricted exploration. We believe this marks a significant step toward generating immersive and truly explorable virtual 3D environments.

从文本生成三维世界是计算机视觉领域备受期待的目标。然而，现有方法在场景可探索性方面存在显著限制 —— 一旦视角超出中心或全景视角范围，生成结果便会出现拉伸或噪声伪影等问题。为此，我们提出WorldExplorer，一种基于自回归视频轨迹生成的新方法，能够构建在广泛视角下视觉一致且可自由导航的三维场景。
我们的方法首先通过生成多视角一致的图像，构建对应于360度全景的初始场景；随后，借助视频扩散模型，采用迭代式的场景生成流程对其进行扩展。具体而言，我们沿着预定义的短路径生成多个视频片段，以探索场景的深度结构，包括围绕物体的运动视角。我们设计的新型场景记忆机制能够为每段视频提供与之最相关的前视图作为条件，同时配备碰撞检测机制以防止出现如相机穿入物体等失真结果。
最终，所有生成的视图将通过三维高斯点渲染（3D Gaussian Splatting）优化过程融合为统一的三维表示。与现有方法相比，WorldExplorer 能够生成在大范围相机运动下依然保持稳定高质量的场景，首次实现了逼真且不受限制的三维空间自由探索。
我们相信，该方法标志着向沉浸式、真实可探索的虚拟三维环境生成迈出了重要一步。
