### Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model

In this paper, we propose Scene Splatter, a momentum-based paradigm for video diffusion to generate generic scenes from single image. Existing methods, which employ video generation models to synthesize novel views, suffer from limited video length and scene inconsistency, leading to artifacts and distortions during further reconstruction. To address this issue, we construct noisy samples from original features as momentum to enhance video details and maintain scene consistency. However, for latent features with the perception field that spans both known and unknown regions, such latent-level momentum restricts the generative ability of video diffusion in unknown regions. Therefore, we further introduce the aforementioned consistent video as a pixel-level momentum to a directly generated video without momentum for better recovery of unseen regions. Our cascaded momentum enables video diffusion models to generate both high-fidelity and consistent novel views. We further finetune the global Gaussian representations with enhanced frames and render new frames for momentum update in the next step. In this manner, we can iteratively recover a 3D scene, avoiding the limitation of video length. Extensive experiments demonstrate the generalization capability and superior performance of our method in high-fidelity and consistent scene generation.

本文提出了 Scene Splatter，一种基于动量的视频扩散生成新范式，用于从单张图像生成通用场景视频。现有方法多采用视频生成模型合成新视角，但普遍存在视频时长受限与场景不一致的问题，进而在后续三维重建中引发伪影与失真。
为解决这一问题，我们从原始特征中构建带噪样本，作为动量信号，用于增强视频细节并保持场景一致性。然而，当潜在特征的感知范围覆盖已知与未知区域时，此类潜在层级的动量机制会在未知区域限制视频扩散模型的生成能力。
因此，我们进一步引入上述一致性视频作为像素级动量，辅以一条不含动量的直接生成路径，以更好地还原不可见区域。通过这种方式，我们的级联动量机制使得视频扩散模型能够生成同时具备高保真度与场景一致性的新视角视频。
在此基础上，我们还对全局三维高斯表示进行微调，并利用增强帧渲染出新的帧图像，用于下一轮的动量更新。通过这种迭代过程，我们可以逐步恢复完整的三维场景，避免传统方法受限于视频长度的瓶颈。
大量实验证明，我们的方法在高保真、场景一致性和泛化能力方面均显著优于现有方法。
