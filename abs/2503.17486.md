### ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes

3D Gaussian Splatting (3DGS) has made significant strides in novel view synthesis but is limited by the substantial number of Gaussian primitives required, posing challenges for deployment on lightweight devices. Recent methods address this issue by compressing the storage size of densified Gaussians, yet fail to preserve rendering quality and efficiency. To overcome these limitations, we propose ProtoGS to learn Gaussian prototypes to represent Gaussian primitives, significantly reducing the total Gaussian amount without sacrificing visual quality. Our method directly uses Gaussian prototypes to enable efficient rendering and leverage the resulting reconstruction loss to guide prototype learning. To further optimize memory efficiency during training, we incorporate structure-from-motion (SfM) points as anchor points to group Gaussian primitives. Gaussian prototypes are derived within each group by clustering of K-means, and both the anchor points and the prototypes are optimized jointly. Our experiments on real-world and synthetic datasets prove that we outperform existing methods, achieving a substantial reduction in the number of Gaussians, and enabling high rendering speed while maintaining or even enhancing rendering fidelity.

3D Gaussian Splatting（3DGS）在新视角合成任务中取得了显著进展，但其依赖大量高斯基元，限制了在轻量级设备上的部署。近期方法尝试通过压缩稠密高斯的存储大小来缓解该问题，但未能在保持渲染质量和效率方面取得良好平衡。为克服这些局限，我们提出了 ProtoGS，通过学习高斯原型来表征高斯基元，在不牺牲视觉质量的前提下显著减少所需高斯数量。我们的方法直接利用高斯原型进行高效渲染，并通过重建误差引导原型的学习。为了进一步优化训练过程中的内存效率，我们引入结构光束法（SfM）点作为锚点，对高斯基元进行分组，并在每组内通过 K-means 聚类得到高斯原型，同时联合优化锚点位置与原型参数。在真实和合成数据集上的实验结果表明，我们的方法优于现有方法，在显著减少高斯数量的同时，实现了更高的渲染速度，并保持甚至提升了渲染保真度。
