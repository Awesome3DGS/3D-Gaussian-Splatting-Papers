### Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction

3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.

三维高斯溅射（3DGS）已成为高质量新视角合成的领先方法，众多变体不断拓展其在三维和四维场景重建中的应用范围。尽管取得了显著成功，3DGS 的表达能力仍受限于使用三维高斯核对局部变化的建模能力。近期一些研究尝试通过引入每个高斯基元的纹理等增强结构来提升其表达力，但这些方法主要针对的是使用较少高斯基元进行的稠密新视角合成，在更一般性的重建场景中表现往往不佳。本文旨在在更广泛的任务范围内，包括新视角合成、几何重建与动态重建，并同时涵盖稀疏和稠密输入设置，全面超越当前最先进的 3DGS 变体。为此，我们提出了 Neural Texture Splatting（NTS）。其核心思想是在全局神经场中引入混合结构（由 tri-plane 与神经解码器构成），为每个基元预测其局部外观与几何属性。借助这种跨基元共享的全局表示，我们显著减少了模型规模，并实现了高效的全局信息交互，从而展现出良好的任务泛化能力。此外，我们对局部纹理场的神经建模还引入了具备表达力的视角与时间依赖效果，这是现有方法普遍忽略的关键要素。大量实验证明，Neural Texture Splatting 在多个基准任务上持续提升模型性能，达成当前最优水平。
