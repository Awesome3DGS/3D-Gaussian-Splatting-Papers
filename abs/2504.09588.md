### TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting

Recent advancements in Generalizable Gaussian Splatting have enabled robust 3D reconstruction from sparse input views by utilizing feed-forward Gaussian Splatting models, achieving superior cross-scene generalization. However, while many methods focus on geometric consistency, they often neglect the potential of text-driven guidance to enhance semantic understanding, which is crucial for accurately reconstructing fine-grained details in complex scenes. To address this limitation, we propose TextSplat--the first text-driven Generalizable Gaussian Splatting framework. By employing a text-guided fusion of diverse semantic cues, our framework learns robust cross-modal feature representations that improve the alignment of geometric and semantic information, producing high-fidelity 3D reconstructions. Specifically, our framework employs three parallel modules to obtain complementary representations: the Diffusion Prior Depth Estimator for accurate depth information, the Semantic Aware Segmentation Network for detailed semantic information, and the Multi-View Interaction Network for refined cross-view features. Then, in the Text-Guided Semantic Fusion Module, these representations are integrated via the text-guided and attention-based feature aggregation mechanism, resulting in enhanced 3D Gaussian parameters enriched with detailed semantic cues. Experimental results on various benchmark datasets demonstrate improved performance compared to existing methods across multiple evaluation metrics, validating the effectiveness of our framework.

近年来，通用型高斯投影（Generalizable Gaussian Splatting）取得了显著进展，借助前馈式高斯投影模型，在稀疏视角输入下实现了鲁棒的三维重建，并展现出优越的跨场景泛化能力。然而，尽管许多方法侧重于几何一致性，它们往往忽略了文本引导在增强语义理解方面的潜力，而语义理解对于复杂场景中细粒度细节的准确重建至关重要。
为了解决这一限制，我们提出了 TextSplat ——首个文本驱动的通用型高斯投影框架。该框架通过融合多种语义线索的文本引导方式，学习鲁棒的跨模态特征表示，从而提升几何信息与语义信息之间的对齐效果，生成高保真的三维重建结果。
在框架设计上，我们引入了三个并行模块以获取互补特征表示：Diffusion Prior Depth Estimator 用于提供准确的深度信息，Semantic Aware Segmentation Network 捕捉细致的语义信息，Multi-View Interaction Network 则整合多视角特征。随后，在 Text-Guided Semantic Fusion Module 中，这些特征通过文本引导的注意力机制进行融合，从而生成富含语义细节的三维高斯参数。
在多个基准数据集上的实验结果表明，与现有方法相比，我们的方法在多个评估指标上均取得了更优表现，验证了所提出框架的有效性。
