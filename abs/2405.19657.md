### Uncertainty-guided Optimal Transport in Depth Supervised Sparse-View 3D Gaussian

3D Gaussian splatting has demonstrated impressive performance in real-time novel view synthesis. However, achieving successful reconstruction from RGB images generally requires multiple input views captured under static conditions. To address the challenge of sparse input views, previous approaches have incorporated depth supervision into the training of 3D Gaussians to mitigate overfitting, using dense predictions from pretrained depth networks as pseudo-ground truth. Nevertheless, depth predictions from monocular depth estimation models inherently exhibit significant uncertainty in specific areas. Relying solely on pixel-wise L2 loss may inadvertently incorporate detrimental noise from these uncertain areas. In this work, we introduce a novel method to supervise the depth distribution of 3D Gaussians, utilizing depth priors with integrated uncertainty estimates. To address these localized errors in depth predictions, we integrate a patch-wise optimal transport strategy to complement traditional L2 loss in depth supervision. Extensive experiments conducted on the LLFF, DTU, and Blender datasets demonstrate that our approach, UGOT, achieves superior novel view synthesis and consistently outperforms state-of-the-art methods.

三维高斯溅射在实时新视角合成中展示了令人印象深刻的性能。然而，从RGB图像成功重建通常需要在静态条件下捕获的多个输入视图。为了应对稀疏输入视图的挑战，先前的方法引入了深度监督到3D高斯的训练中，以减轻过拟合，使用预训练深度网络的密集预测作为伪真实值。尽管如此，来自单目深度估计模型的深度预测本质上在特定区域显示出显著的不确定性。仅依赖逐像素L2损失可能会无意中引入这些不确定区域的有害噪声。在这项工作中，我们引入了一种新的方法来监督3D高斯的深度分布，利用具有集成不确定性估计的深度先验。为了解决深度预测中这些局部错误，我们整合了一种基于块的最优传输策略，以补充传统的L2损失进行深度监督。在LLFF、DTU和Blender数据集上进行的广泛实验表明，我们的方法UGOT实现了优越的新视角合成，并一致超越了最先进的方法。
