### Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling

Rendering dynamic scenes from monocular videos is a crucial yet challenging task. The recent deformable Gaussian Splatting has emerged as a robust solution to represent real-world dynamic scenes. However, it often leads to heavily redundant Gaussians, attempting to fit every training view at various time steps, leading to slower rendering speeds. Additionally, the attributes of Gaussians in static areas are time-invariant, making it unnecessary to model every Gaussian, which can cause jittering in static regions. In practice, the primary bottleneck in rendering speed for dynamic scenes is the number of Gaussians. In response, we introduce Efficient Dynamic Gaussian Splatting (EDGS), which represents dynamic scenes via sparse time-variant attribute modeling. Our approach formulates dynamic scenes using a sparse anchor-grid representation, with the motion flow of dense Gaussians calculated via a classical kernel representation. Furthermore, we propose an unsupervised strategy to efficiently filter out anchors corresponding to static areas. Only anchors associated with deformable objects are input into MLPs to query time-variant attributes. Experiments on two real-world datasets demonstrate that our EDGS significantly improves the rendering speed with superior rendering quality compared to previous state-of-the-art methods.

从单目视频渲染动态场景是一个关键但具有挑战性的任务。近年来，可变形高斯散点 (Deformable Gaussian Splatting) 已成为表示真实世界动态场景的强大解决方案。然而，该方法往往会引入大量冗余高斯基元，试图在不同时间步拟合所有训练视图，导致渲染速度变慢。此外，静态区域的高斯属性本质上是时间不变的，但现有方法仍对其进行建模，容易导致静态区域的抖动 (jittering)。在实际应用中，动态场景渲染速度的主要瓶颈是高斯基元的数量。
为此，我们提出高效动态高斯散点 (Efficient Dynamic Gaussian Splatting, EDGS)，通过稀疏的时间变化属性建模来表示动态场景。我们的方法采用稀疏锚点网格 (sparse anchor-grid) 表示动态场景，并利用经典核表示 (classical kernel representation) 计算稠密高斯的运动流 (motion flow)。此外，我们提出了一种无监督策略 (unsupervised strategy) 来高效过滤静态区域的锚点，仅对可变形物体关联的锚点输入 MLP 以查询时间变化属性。
在两个真实世界数据集上的实验表明，EDGS 在显著提升渲染速度的同时，渲染质量优于当前最先进方法。
