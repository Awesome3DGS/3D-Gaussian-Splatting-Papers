### MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion

Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.


网格模型在众多城市中日益普及，然而缺乏逼真的纹理限制了其在虚拟城市导航和自动驾驶中的应用。为了解决这一问题，本文提出了 MeSS（基于网格的场景合成）方法，用城市网格模型作为几何先验来生成高质量、风格一致的室外场景。尽管图像和视频扩散模型可以利用空间布局（如深度图或高清地图）作为控制条件来生成街景视角，但它们并不能直接用于三维场景生成。视频扩散模型擅长合成一致的视角序列以描绘场景，但往往难以严格遵循预设的相机路径或与渲染的控制视频准确对齐。相比之下，图像扩散模型虽然无法保证跨视角的一致性，但在结合 ControlNet 时能够生成更符合几何约束的结果。基于这一洞察，我们的方法通过改进跨视角一致性来增强图像扩散模型。其流程包括三个关键阶段：首先，利用级联外扩绘制 ControlNet 生成几何一致的稀疏视角；其次，通过名为 AGInpaint 的组件传播更密集的中间视角；第三，借助 GCAlign 模块全局消除视觉不一致性（如曝光差异）。在生成的同时，我们还通过在网格表面初始化高斯球来重建一个三维高斯溅射（3DGS）场景。实验表明，我们的方法在几何对齐和生成质量上均优于现有方法。完成合成后，该场景还可以通过重光照和风格迁移技术呈现多样化的风格。
