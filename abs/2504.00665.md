### Monocular and Generalizable Gaussian Talking Head Animation

In this work, we introduce Monocular and Generalizable Gaussian Talking Head Animation (MGGTalk), which requires monocular datasets and generalizes to unseen identities without personalized re-training. Compared with previous 3D Gaussian Splatting (3DGS) methods that requires elusive multi-view datasets or tedious personalized learning/inference, MGGtalk enables more practical and broader applications. However, in the absence of multi-view and personalized training data, the incompleteness of geometric and appearance information poses a significant challenge. To address these challenges, MGGTalk explores depth information to enhance geometric and facial symmetry characteristics to supplement both geometric and appearance features. Initially, based on the pixel-wise geometric information obtained from depth estimation, we incorporate symmetry operations and point cloud filtering techniques to ensure a complete and precise position parameter for 3DGS. Subsequently, we adopt a two-stage strategy with symmetric priors for predicting the remaining 3DGS parameters. We begin by predicting Gaussian parameters for the visible facial regions of the source image. These parameters are subsequently utilized to improve the prediction of Gaussian parameters for the non-visible regions. Extensive experiments demonstrate that MGGTalk surpasses previous state-of-the-art methods, achieving superior performance across various metrics.

在本工作中，我们提出了 MGGTalk（Monocular and Generalizable Gaussian Talking Head Animation），一种无需多视图数据、可泛化至未见身份且无需个性化再训练的高斯说话人动画方法。相较于以往依赖难以获取的多视图数据或繁琐个性化训练/推理的三维高斯喷洒（3D Gaussian Splatting, 3DGS）方法，MGGTalk 在实际性与适用范围上具有更大优势。
然而，在缺乏多视图与个性化训练数据的条件下，几何与外观信息的不完整性成为主要挑战。为应对此问题，MGGTalk 利用深度信息增强几何结构与面部对称性特征，从而在几何与外观层面进行有效补全。
具体而言，首先我们基于深度估计获得的像素级几何信息，引入对称操作与点云过滤技术，以获得完整、精确的 3DGS 位置参数。随后，我们采用结合对称先验的两阶段策略预测其余 3DGS 参数：第一阶段预测源图像中可见面部区域的高斯参数，第二阶段则基于第一阶段结果进一步完善不可见区域的参数预测。
大量实验表明，MGGTalk 在多个评价指标上均优于现有最先进方法，展现出更优异的性能。
