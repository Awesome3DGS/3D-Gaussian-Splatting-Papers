### Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos

Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed DualGS, for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers' fingertips.

体积视频代表了视觉媒体的革命性进步，使用户能够自由探索沉浸式虚拟体验，缩小了数字世界与现实世界之间的差距。然而，现有工作流程中大量手动干预以稳定网格序列以及生成过于庞大的资产，阻碍了其广泛应用。在本文中，我们提出了一种基于高斯的新方法，称为DualGS，用于复杂人类表演的实时高保真播放，并具有出色的压缩比。DualGS 的核心思想是使用对应的皮肤和关节高斯分别表示运动和外观。这样的显式解耦可以显著减少运动冗余并增强时间一致性。我们首先初始化 DualGS，并在第一帧将皮肤高斯锚定在关节高斯上。随后，我们采用由粗到细的训练策略对逐帧人类表演进行建模。该策略包括整体运动预测的粗略对齐阶段，以及用于鲁棒跟踪和高保真渲染的精细优化阶段。
为了将体积视频无缝集成到 VR 环境中，我们通过熵编码高效压缩运动，并结合持久码本使用编解码器压缩外观。我们的方法实现了高达 120 倍的压缩比，每帧仅需大约 350KB 的存储空间。我们通过在 VR 头显上提供逼真的自由视角体验，展示了我们的表示法的有效性，让用户能够沉浸式观看音乐家的表演，感受到演奏者指尖音符的节奏。
