### 3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction

The accurate reconstruction of dynamic street scenes is critical for applications in autonomous driving, augmented reality, and virtual reality. Traditional methods relying on dense point clouds and triangular meshes struggle with moving objects, occlusions, and real-time processing constraints, limiting their effectiveness in complex urban environments. While multi-view stereo and neural radiance fields have advanced 3D reconstruction, they face challenges in computational efficiency and handling scene dynamics. This paper proposes a novel 3D Gaussian point distribution method for dynamic street scene reconstruction. Our approach introduces an adaptive transparency mechanism that eliminates moving objects while preserving high-fidelity static scene details. Additionally, iterative refinement of Gaussian point distribution enhances geometric accuracy and texture representation. We integrate directional encoding with spatial position optimization to optimize storage and rendering efficiency, reducing redundancy while maintaining scene integrity. Experimental results demonstrate that our method achieves high reconstruction quality, improved rendering performance, and adaptability in large-scale dynamic environments. These contributions establish a robust framework for real-time, high-precision 3D reconstruction, advancing the practicality of dynamic scene modeling across multiple applications.

动态街景的准确重建对于自动驾驶、增强现实和虚拟现实等应用至关重要。传统方法依赖于密集点云和三角网格，但在处理运动物体、遮挡和实时处理限制时存在困难，这限制了它们在复杂城市环境中的有效性。尽管多视角立体视觉和神经辐射场（Neural Radiance Fields）在三维重建方面取得了进展，但它们在计算效率和处理场景动态方面仍面临挑战。本文提出了一种用于动态街景重建的全新三维高斯点分布方法。我们的方法引入了一种自适应透明度机制，可以消除运动物体，同时保留高保真的静态场景细节。此外，迭代优化高斯点分布提高了几何精度和纹理表现。我们结合方向编码与空间位置优化，以优化存储和渲染效率，减少冗余的同时保持场景完整性。实验结果表明，我们的方法在大规模动态环境中实现了高质量的重建、更好的渲染性能以及较强的适应性。这些贡献为实时高精度三维重建建立了一个稳健的框架，推动了动态场景建模在多个应用中的实际可行性。
