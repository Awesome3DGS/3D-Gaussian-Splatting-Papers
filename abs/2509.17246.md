### SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views

We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training and inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs. A masked attention mechanism is introduced to efficiently estimate target poses during training, while a reprojection loss enforces pixel-aligned Gaussian primitives, providing stronger geometric constraints. We further demonstrate the compatibility of our training framework with different reconstruction architectures, resulting in two model variants. Remarkably, despite the absence of pose supervision, our method achieves state-of-the-art performance in both in-domain and out-of-domain novel view synthesis, even under extreme viewpoint changes and limited image overlap, and surpasses recent methods that rely on geometric supervision for relative pose estimation. By eliminating dependence on ground-truth poses, our method offers the scalability to leverage larger and more diverse datasets.

我们提出了 **SPFSplatV2**，一种高效的前馈式三维高斯溅射（3D Gaussian Splatting）框架，能够从稀疏多视图图像中进行重建，在训练与推理阶段均无需使用真实位姿监督。该方法采用共享特征提取骨干网络，可在无姿态输入的情况下同时预测规范空间中的三维高斯基元与相机位姿。我们引入了**掩码注意力机制（masked attention mechanism）**以高效估计训练阶段的目标位姿，并通过**重投影损失（reprojection loss）**实现像素级对齐的高斯基元，从而提供更强的几何约束。此外，我们展示了该训练框架与不同重建架构的兼容性，构建了两个模型变体。值得注意的是，即便在无姿态监督的条件下，本方法在域内与跨域的新视角合成任务中均达到了当前最优性能，尤其在极端视角变化与图像重叠有限的情况下，依然显著优于依赖几何监督进行相对姿态估计的最新方法。通过消除对真实位姿的依赖，SPFSplatV2 具备了利用更大规模与更多样化数据集进行扩展的能力。
