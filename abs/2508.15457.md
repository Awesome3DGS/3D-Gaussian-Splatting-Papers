### Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework

3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time performance in novel view synthesis, yet its effectiveness relies heavily on dense multi-view inputs with precisely known camera poses, which are rarely available in real-world scenarios. When input views become extremely sparse, the Structure-from-Motion (SfM) method that 3DGS depends on for initialization fails to accurately reconstruct the 3D geometric structures of scenes, resulting in degraded rendering quality. In this paper, we propose a novel SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs 3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we propose a dense stereo module to progressively estimates camera pose information and reconstructs a global dense point cloud for initialization. To address the inherent problem of information scarcity in extremely sparse-view settings, we propose a coherent view interpolation module that interpolates camera poses based on training view pairs and generates viewpoint-consistent content as additional supervision signals for training. Furthermore, we introduce multi-scale Laplacian consistent regularization and adaptive spatial-aware multi-scale geometry regularization to enhance the quality of geometrical structures and rendered content. Experiments show that our method significantly outperforms other state-of-the-art 3DGS-based approaches, achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view conditions (using only 2 training views). The images synthesized by our method exhibit minimal distortion while preserving rich high-frequency details, resulting in superior visual quality compared to existing techniques.

三维高斯溅射（3DGS）在新视角合成中展现了卓越的实时性能，但其有效性严重依赖于密集多视角输入及精确已知的相机位姿，而这些在真实场景中往往难以获得。当输入视角极度稀疏时，3DGS 依赖的结构自运动（SfM）初始化方法无法准确重建场景的三维几何结构，导致渲染质量下降。为了解决这一问题，本文提出了一种无需 SfM 的基于 3DGS 的新方法，可在极稀疏视角输入下联合估计相机位姿并重建三维场景。具体而言，我们以稠密立体模块取代 SfM，逐步估计相机位姿信息并重建用于初始化的全局稠密点云。针对极稀疏视角下信息不足的固有问题，我们提出了一种一致视角插值模块，该模块基于训练视角对进行相机位姿插值，并生成视角一致的内容作为额外监督信号。此外，我们引入了多尺度拉普拉斯一致性正则化与自适应空间感知多尺度几何正则化，以提升几何结构与渲染内容的质量。实验结果表明，在极稀疏视角条件下（仅使用 2 个训练视角），我们的方法显著优于其他最先进的基于 3DGS 的方法，在 PSNR 上实现了 2.75dB 的显著提升。合成图像几乎无畸变，同时保留了丰富的高频细节，在视觉质量上明显优于现有技术。
