### DIMO: Diverse 3D Motion Generation for Arbitrary Objects

We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation.

我们提出了 **DIMO**，一种仅从单张图像即可为任意物体生成多样化三维运动的生成式方法。该方法的核心思想是利用成熟视频模型中蕴含的丰富先验知识，提取通用的运动模式，并将其嵌入到一个共享的低维潜在空间中。具体而言，我们首先为同一物体生成多段具有不同运动形态的视频；随后将每种运动嵌入为一个潜在向量，并训练一个共享的运动解码器，以学习由结构化且紧凑的运动表示——即**神经关键点轨迹**——所刻画的运动分布。接着，利用这些关键点来驱动规范化的三维高斯基元，并进行融合以建模物体的几何结构与外观。在推理阶段，借助学习到的潜在空间，模型能够在单次前向传播中高效采样多样化的三维运动，并支持多种有趣的应用场景，包括**三维运动插值**以及**语言引导的运动生成**。
