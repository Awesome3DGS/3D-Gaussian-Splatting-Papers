### MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting

Recent advances in dynamic scene reconstruction have significantly benefited from 3D Gaussian Splatting, yet existing methods show inconsistent performance across diverse scenes, indicating no single approach effectively handles all dynamic challenges. To overcome these limitations, we propose Mixture of Experts for Dynamic Gaussian Splatting (MoE-GS), a unified framework integrating multiple specialized experts via a novel Volume-aware Pixel Router. Our router adaptively blends expert outputs by projecting volumetric Gaussian-level weights into pixel space through differentiable weight splatting, ensuring spatially and temporally coherent results. Although MoE-GS improves rendering quality, the increased model capacity and reduced FPS are inherent to the MoE architecture. To mitigate this, we explore two complementary directions: (1) single-pass multi-expert rendering and gate-aware Gaussian pruning, which improve efficiency within the MoE framework, and (2) a distillation strategy that transfers MoE performance to individual experts, enabling lightweight deployment without architectural changes. To the best of our knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts techniques into dynamic Gaussian splatting. Extensive experiments on the N3V and Technicolor datasets demonstrate that MoE-GS consistently outperforms state-of-the-art methods with improved efficiency.

近年来，动态场景重建在三维高斯投影（3D Gaussian Splatting）技术的发展下取得了显著进展，但现有方法在不同场景中的表现仍不稳定，表明尚无一种方法能有效应对所有动态挑战。为克服这一局限，本文提出了一种面向动态高斯投影的专家混合方法（Mixture of Experts for Dynamic Gaussian Splatting，简称MoE-GS），该方法通过引入新颖的体素感知像素路由器（Volume-aware Pixel Router）实现多个专业子模型的统一融合。该路由器通过可微分的权重投影机制将体素级高斯权重映射到像素空间，从而自适应地融合各专家输出，确保空间和时间上的一致性。尽管MoE-GS提升了渲染质量，但由于其架构本身的特点，模型容量增大和帧率下降也随之而来。为缓解这一问题，我们探索了两个互补方向：(1) 单次多专家渲染与感知门控的高斯剪枝机制，以提升MoE架构下的运行效率；(2) 蒸馏策略，将MoE的表现迁移至单一专家模型，从而在不更改模型架构的前提下实现轻量化部署。据我们所知，MoE-GS是首个将专家混合机制引入动态高斯投影的工作。大量在N3V和Technicolor数据集上的实验结果表明，MoE-GS在保持高效性的同时，始终优于当前最先进方法。
