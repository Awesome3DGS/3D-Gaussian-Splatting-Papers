### Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content

3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key requirement for immersive applications. However, the extension of 3DGS to dynamic scenes remains limitations on the substantial data volume of dense Gaussians and the prolonged training time required for each frame. This paper presents Scale-GS, a scalable Gaussian Splatting framework designed for efficient training in streaming tasks. Specifically, Gaussian spheres are hierarchically organized by scale within an anchor-based structure. Coarser-level Gaussians represent the low-resolution structure of the scene, while finer-level Gaussians, responsible for detailed high-fidelity rendering, are selectively activated by the coarser-level Gaussians. To further reduce computational overhead, we introduce a hybrid deformation and spawning strategy that models motion of inter-frame through Gaussian deformation and triggers Gaussian spawning to characterize wide-range motion. Additionally, a bidirectional adaptive masking mechanism enhances training efficiency by removing static regions and prioritizing informative viewpoints. Extensive experiments demonstrate that Scale-GS achieves superior visual quality while significantly reducing training time compared to state-of-the-art methods.

三维高斯点绘（3DGS）支持高保真的实时渲染，这是沉浸式应用的关键需求。然而，将 3DGS 扩展至动态场景仍然面临限制：一方面是密集高斯带来的庞大数据量，另一方面是每帧所需的长时间训练。本文提出了 Scale-GS，这是一种面向流式任务的可扩展高斯点绘框架，旨在实现高效训练。具体而言，高斯球在基于锚点的结构中按照尺度进行分层组织。粗层高斯表示场景的低分辨率结构，而细层高斯负责高保真渲染细节，并由粗层高斯选择性激活。为进一步降低计算开销，我们引入了一种混合变形与生成策略，通过高斯变形来建模帧间运动，并在大范围运动时触发高斯生成。同时，双向自适应掩码机制通过剔除静态区域并优先考虑信息量较高的视角，从而提升训练效率。大量实验证明，Scale-GS在显著缩短训练时间的同时，依然能实现优于现有方法的视觉质量。
