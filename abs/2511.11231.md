### 3D Gaussian and Diffusion-Based Gaze Redirection

High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.

高保真的视线重定向对于生成增强数据、提升视线估计器的泛化能力至关重要。以 GazeGaussian 为代表的 3D Gaussian Splatting（3DGS）模型已达到当前最先进水平，但在渲染细微且连续的视线变化时仍面临困难。本文提出了 DiT-Gaze，一个通过结合扩散 Transformer（Diffusion Transformer，DiT）、跨视线角度的弱监督以及正交约束损失来增强三维视线重定向模型的框架。DiT 能够实现更高保真的图像合成，而我们基于合成生成的中间视线角度的弱监督策略，则在训练过程中提供了平滑的视线方向流形。正交约束损失从数学上强制视线、头部姿态与表情等内部表示之间的解耦。大量实验表明，DiT-Gaze 在感知质量和重定向精度两方面均达到了新的最先进水平，将当前最优的视线误差降低了 4.1%，达到 6.353 度，为构建合成训练数据提供了一种更优的方法。我们的代码和模型将向研究社区公开，以便进行基准评测。
