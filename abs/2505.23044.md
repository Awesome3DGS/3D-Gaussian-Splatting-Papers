### SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images

A major breakthrough in 3D reconstruction is the feedforward paradigm to generate pixel-wise 3D points or Gaussian primitives from sparse, unposed images. To further incorporate semantics while avoiding the significant memory and storage costs of high-dimensional semantic features, existing methods extend this paradigm by associating each primitive with a compressed semantic feature vector. However, these methods have two major limitations: (a) the naively compressed feature compromises expressiveness, affecting the model's ability to capture fine-grained semantics, and (b) the pixel-wise primitive prediction introduces redundancy in overlapping areas, causing unnecessary memory overhead. To this end, we introduce SpatialSplat, a feedforward framework that produces redundancy-aware Gaussians and capitalizes on a dual-field semantic representation. Particularly, with the insight that primitives within the same instance exhibit high semantic consistency, we decompose the semantic representation into a coarse feature field that encodes uncompressed semantics with minimal primitives, and a fine-grained yet low-dimensional feature field that captures detailed inter-instance relationships. Moreover, we propose a selective Gaussian mechanism, which retains only essential Gaussians in the scene, effectively eliminating redundant primitives. Our proposed Spatialsplat learns accurate semantic information and detailed instances prior with more compact 3D Gaussians, making semantic 3D reconstruction more applicable. We conduct extensive experiments to evaluate our method, demonstrating a remarkable 60% reduction in scene representation parameters while achieving superior performance over state-of-the-art methods.

三维重建领域的一项重要突破是采用前馈式范式，从稀疏、无位姿图像中直接生成逐像素的三维点或高斯基元。为进一步引入语义信息，同时避免高维语义特征所带来的显著内存与存储开销，现有方法通常为每个基元关联一个压缩的语义特征向量。然而，这些方法存在两个主要局限：（a）特征压缩方式过于简单，导致表达能力受限，影响模型对细粒度语义的捕捉能力；（b）逐像素预测基元会在重叠区域引入冗余，造成不必要的内存负担。
为解决上述问题，我们提出了 SpatialSplat，一个生成冗余感知高斯基元的前馈式框架，并结合了双场语义表示。基于这样一个观察：同一实例内部的基元往往具有高度语义一致性，我们将语义表示解耦为两个部分：一个粗粒度特征场，使用极少量基元编码未经压缩的全局语义信息；以及一个细粒度但低维的特征场，捕捉实例间的局部语义关系。
此外，我们提出了选择性高斯机制，保留场景中最具代表性的基元，有效剔除冗余高斯点，从而提升表达效率。
SpatialSplat 能够以更紧凑的三维高斯形式学习准确的语义信息与丰富的实例先验，使语义三维重建更具实用性。我们在多个数据集上进行了大量实验，结果表明该方法在场景表示参数上减少了约 60%，同时在性能上优于当前最先进的方法。
